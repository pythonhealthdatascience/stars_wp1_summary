[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STARS Work Package 1",
    "section": "",
    "text": "Overview\nThis book describes the findings from work package 1 of the project STARS: Sharing Tools and Artefacts for Reproducible Simulations in healthcare.\nUse the sidebar to navigate through:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "STARS Work Package 1",
    "section": "",
    "text": "Introduction - background on STARS, pilot work, and the aim of work package 1\nMethods - summarises the methods which are described in detail in our protocol\nResults - describes the results of the reproductions and evaluations, and reflections from the process\nDiscussion - considers and applies findings",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "STARS Work Package 1",
    "section": "Funding",
    "text": "Funding\nThis work is supported by the Medical Research Council [grant number MR/Z503915/1].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "STARS Work Package 1",
    "section": "Licence",
    "text": "Licence\nThis book is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "pages/background.html",
    "href": "pages/background.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 General context\nBaker (2016) - survey of 1576 researchers across disciplines, finding:\nDifferences between reproduction and replication\nKorbmacher et al. (2023) - great discussion of the replication crisis\nOn the protocol page, several reproducibility studies are mentioned, which were used to help inform the methods in our reproducibility assessments.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#general-context",
    "href": "pages/background.html#general-context",
    "title": "2  Introduction",
    "section": "",
    "text": "“More than 70% of researchers have tried and failed to reproduce another scientist’s experiments”\n“More than half have failed to reproduce their own experiments”\n52% “agree that there is a significant ‘crisis’ of reproducibility”\n“Less than 31% think that failure to reproduce published results means that the result is probably wrong, and most say that they still trust the published literature”\n\n\n\nReproduction - using their code\nReplication - writing your own code",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#pilot-work",
    "href": "pages/background.html#pilot-work",
    "title": "2  Introduction",
    "section": "2.2 Pilot work",
    "text": "2.2 Pilot work\n\n2.2.1 Review of healthcare simulation sharing practices\nMonks and Harper (2023) explored how discrete-event simulation (DES) models used in a health context (e.g. health services, health economics) were shared and whether this sharing adhered to best practice. The full study can be viewed at:\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\nIn summary, they identified 564 papers describing a DES model from a health context published from 2019 to 2022. Of these, only 8.3% (47/564) had available model code (either sharing the code themselves, or citing an openly available model). Looking by year, this rose from 4.0% for studies published in 2019, to 9.0% for 2022.\nFurther findings:\n\nMore likely that code was shared if model was:\n\nCreated using free and open source software (FOSS) (28.7%, 29/101)\nA COVID-19 model (24.6%, 17/69)\n\nOf the papers that did share a model:\n\nMost were written in a programming language (66%, 31/47), the rest in a commercial off the shelf Visual Interactive Modelling (VIM) software\nThese were evaluated in a best practice audit, with the results as follows: “In general, computer models and artefacts were published without a DOI (n = 7); rarely included ORCIDs for authors (n = 6); rarely included an open licence (n = 21); were mostly supported by a README file (n = 28); rarely included documentation detailing how to run the model (n = 15); provided no form of formal or informal dependency management (n = 21); did not include any evidence of model testing (n = 3); were almost all downloadable (n = 38); and rarely executable via a cloud-based platform (n = 10).” (Monks and Harper (2023))\n\nFew studies used a reporting guideline (12.8%, 72/564) - mostly using:\n\nOne of the International Society for Pharmacoeconomics and Outcomes Research (ISPOR) publications (n=37), or\nThe Strengthening the Reporting of Empirical Simulation Studies (STRESS) guidelines from Monks et al. (2019) (n=22)\n\n\nThe review concludes that “there are many (simple) best practices the community can adopt, such as the use of trusted archives, and documentation, to improve its sharing”. (Monks and Harper (2023))\n\n\n2.2.2 Pilot STARS framework\nMonks, Harper, and Mustafee (2024) introduces a pilot framework for sharing DES models called STARS: Sharing Tools and Artefacts for Reusable Simulations. Note that this “reusable” is changed into “reproducible” for the STARS project as we build on this work in the current project.\nThe pilot STARS framework consists of essential components (minimum to make models “long-term, citable, functional, appropriately licenced”) and optional components (enhance model “accessibility, understanding, and maintainability”). (Monks, Harper, and Mustafee (2024))\nThe essential components are:\n\nOpen license\nDependency management\nFOSS model\nMinimum documentation\nOpen Researcher and Contributor IDs (ORCID)\nCitation information\nRemote code repository\nOpen science archive\n\nThe optional components are:\n\nEnhanced documentation\nDocumentation hosting\nOnline coding environment\nModel interface\nWeb app hosting\n\nThis is summarised in the diagram below…\n\n\n\nSTARS framework overview\n\n\nThis was supported by example implementations in Python:\n\nExample 1 - stars-treat-sim\nExample 2 - stars-streamlit-example and stars-simpy-example-docs - with web app and hosted docs\nExample 3 - stars-ciw-example - with web app and hosted docs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#stars-project",
    "href": "pages/background.html#stars-project",
    "title": "2  Introduction",
    "section": "2.3 STARS project",
    "text": "2.3 STARS project\nThe MRC-funded STARS project builds on this pilot work. As stated in the funding application, the objective of this project is “to improve the quality and quantity of shared discrete-event simulation models, tools and other research artefacts:\n\nIdentify barriers, and good practices for sharing simulation models;\nDevelop a new framework for sharing computer models applicable the most common free and open-source languages;\nTest the framework in both retrospective and prospective case studies;\nDevelop online interactive training materials;\nTransfer knowledge of our STARS framework to health data science researchers;\nEnsure sustainability of materials;\nSupport our partner archival journals adopt open science principles and our findings;”\n\nThis objectives will be achieved through four work packages:\nWork package 1: Reproducibility of computational results\n\nAssess the computational reproducibility of six published DES models created in Python and R.\nEvaluate the publication, code and associated artefacts against reporting guidelines, best practice for code sharing, and criteria from journal artefact badges.\n\nWork package 2: R and Python framework for sharing DES models\n\nImprove the pilot framework (e.g. extend baesd on barriers and enablers to reproduction observed in work package 1, and making it relevant to R models)\nProvide time-saving measures for researchers (e.g. automated support for STRESS, use of large language models (LLM) to support creating of summaries, automated testing of models, continuous integration tools)\n\nWork package 3: Prospective and retrospective application of the framework\n\nApply STARS framework within two case studies (one prospective and one retrospective)\n\nWork package 4: Training\n\nDevelop online interactive training materials to support researchers in using the STARS framework\n\nFrom this point onwards, this site/book summarises the findings from work package 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#references",
    "href": "pages/background.html#references",
    "title": "2  Introduction",
    "section": "2.4 References",
    "text": "2.4 References\n\n\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nKorbmacher, Max, Flavio Azevedo, Charlotte R. Pennington, Helena Hartmann, Madeleine Pownall, Kathleen Schmidt, Mahmoud Elsherif, et al. 2023. “The Replication Crisis Has Led to Positive Structural, Procedural, and Community Changes.” Communications Psychology 1 (1): 3. https://doi.org/10.1038/s44271-023-00003-2.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html",
    "href": "pages/protocol.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Protocol summary\nFor this work, six published healthcare DES models were selected. These were models with publicly available code under an open licence (either already available or add on request from the STARS team). For each model, the follow stages of work were conducted:\nStage 1: Reproduction - assessing the computational reproducibility of each study\nStage 2: Evaluation - evaluating the publication, code and associated artefacts against sharing and reporting standards\nStage 3: Report and research compendium - summary report and organised repository\nFor each study, a quarto site was produced which shared the results from the reproduction and evaluation and the summary report. Throughout the work, a detailed logbook was kept to keep track of timings and to record work on each stage, such as detailing troubleshooting steps during the reproduction, or uncertainities discussed with another STARS team member during the evaluation.\nSummary diagram\nThis process is captured in the diagram below:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#protocol-summary",
    "href": "pages/protocol.html#protocol-summary",
    "title": "3  Methods",
    "section": "",
    "text": "Informed authors about study and, if not available, asked if they would be happy to add an open licence to their code\nSet up repository for reproduction with the article and code\nRead the article and defined the scope of the reproduction, archiving the scope (and repository) on Zenodo before proceeding\nLooked over the model code, created a suitable environment with the software and packages required, and then ran the model. For each study, with any issues faced in running the model, troubleshooting was performed such as modifying or writing code. If troubleshooting was exhaused and there were still issues or discrepancies in the results, the original study authors were informed and provided the opportunity to advice on the reason for this (although with no pressure or requirement to do so)\nFor each item in the scope, a decision was made as to whether this had been successfully reproduced or not. This was a subjective decision which allowed some expected deviation due to model stochasticity (for example, lack of seed control).\nThis is timed (including time to produce each item in the scope), and limited to a maximum of 40 horus\n\n\n\nThe publication was evaluated against reporting guidelines for DES:\n\nMonks et al. (2019) - STRESS-DES: Strengthening The Reporting of Empirical Simulation Studies (Discrete-Event Simulation) (Version 1.0).\nZhang, Lhachimi, and Rogowski (2020) - The generic reporting checklist for healthcare-related discrete event simulation studies derived from the the International Society for Pharmacoeconomics and Outcomes Research Society for Medical Decision Making (ISPOR-SDM) Modeling Good Research Practices Task Force reports.\n\nThe model code and associated artefacts (e.g. the GitHub repository shared by the authors) was evaluated against:\n\nThe criteria of badges related to reproducibility from various organisations and journals - namely:\n\nNational Information Standards Organisation (NISO)(NISO Reproducibility Badging and Definitions Working Group (2021))\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\nCenter for Open Science (COS) (Blohowiak et al. (2023))\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\nRecommendations from the pilot STARS framework for the sharing of code and associated materials from discrete-event simulation models (Monks, Harper, and Mustafee (2024)).\n\nThis is timed\n\n\n\nWrote a report summarising the computational reproducibility assessment and evaluation\nRestructed the reposuitory into a “research compendium”, which essentially consisted of organising the repository to ensure it is easy and clear for someone else to re-run. Steps included:\n\nAdding run times to the model notebooks\nWrite a README for the reproduction folder\nMoving data, methods and outputs into seperate folders\nCreating tests which check if a user can get the same results from the model as we did during the reproduction\nA Dockerfile and Docker image published on the GitHub container registry\n\nA second researcher from the STARS team then attempted to use the repository and confirm whether they were able to reproduce the results of the first researcher\nFinally, the repository was archived on Zenodo, and the authors were informed.\n\n\n\n\n\n\n\nWorkflow for STARS work package 1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#inspiration",
    "href": "pages/protocol.html#inspiration",
    "title": "3  Methods",
    "section": "3.2 Inspiration",
    "text": "3.2 Inspiration\nAs cited in Heather et al. (2024), the protocol was informed by several prior studies assessing computational reproducibility:\n\nKrafczyk et al. (2021): Assessed the reproducibility of seven articles published in the Journal of Computational Physics.\nB. D. K. Wood, Müller, and Brown (2018) and B. Wood et al. (2018): Assessed the reproducibility of 109 published impact evaluations in low- and middle-income countries. Conducted in association with the replication programme of the International Initiative for Impact Evaluation (3ie).\nSchwander et al. (2021): Assessed the reproducibility of four health economic obesity models.\nLaurinavichyute, Yadav, and Vasishth (2022): Assessed the reproducibility of 118 articles published in the Journal of Memory and Language.\nKonkol, Kray, and Pfeiffer (2019): Assessed the reproducibility of 41 geoscientific articles from Copernicus Publications and the Journal of Statistical Software.\n\nIt was also informed by:\n\nAyllón et al. (2021): Article with recommendations on keeping modelling notebooks to support completion of TRACE (TRAnsparent and Comprehensive model Evaluation) documents.\nBerkeley Initiative for Transparency in the Social Sciences (2022): Guidelines on conducting reproductions of published social science research.\nMcManus, Turner, and Sach (2019): Article proposing several possible definitions for success in reproducing or replicating models in health economics.\nMarwick, Boettiger, and Mullen (2018): Article recommending how to structure data analytical work as research compendiums using the R package structure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#major-deviations-from-the-protocol",
    "href": "pages/protocol.html#major-deviations-from-the-protocol",
    "title": "3  Methods",
    "section": "3.3 Major deviations from the protocol",
    "text": "3.3 Major deviations from the protocol\nThere was one major deviation from the protocol…\n\n\n\n\n\n\n\nDeviation\nDescription and reason for the change\n\n\n\n\nExpanding from 6 to 8 studies\nWhilst partway through the 6th study, we reflected on what we had found so far, and felt it would be beneficial to do a few more studies. The primary motivation for this was to try and include a selection of studies that reflect the range of studies in the literature. So far, we had a few studies where the model code/run times were very large or very small, and we felt it beneficial to try and include some more “medium-sized” studies, as these are more typical of the literature. We also thought it would be good to include an older study (for example, about 10 years ago, if we can find one), as all those included so far were within the last few years, but working with more outdated code will be/can be a problem people encounter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#minor-deviations-from-the-protocol",
    "href": "pages/protocol.html#minor-deviations-from-the-protocol",
    "title": "3  Methods",
    "section": "3.4 Minor deviations from the protocol",
    "text": "3.4 Minor deviations from the protocol\nThere were some minor deviations from the protocol, which are explained below…\n\n\n\n\n\n\n\nDeviation\nDescription and reason for the change\n\n\n\n\nUsing the latest software packages\nIn the protocol, I had planned that - if no versions were provided we select a version of the software and each package that is closest to but still prior to the date of the code archive or paper publication. I kept to this for the Python models (easily set using a conda/mamba environment). However, I had great difficulties attempting to do this in R, and could not successfully backdate both. As such, I used the latest versions of R and each package for those studies\n\n\nUsing percentage difference in results to help decided reproduction success\nThis is not particularly deviation, as I did explore this, but I ultimately found it very unhelpful, as the percentage difference could be greatly impacted by scale (for example, 0.1 vs 0.2 would appear much greater than 3 vs 4, but the actual meaning of these differences might be similar (e.g. both might be considered a small difference) depending on the scale used and what is being compared - whilst in another context with a different scale, 0.1 vs 0.2 might actually reflect a huge difference!).\n\n\nMoving onto evaluation stage before receive author response regarding reproduction discrepancy or before getting consensus on reproduction\nIn the protocol, we required that authors are contacted if there are any remaining difficulties in running the code or items in the scope that were not reproduced. The authors were given a total of four weeks to respond if they chose to. We had implied that we must wait for this time to pass before continuing to the evaluation stage (since the three stages were presented as being completed one after another). The rationale for this was that the timings for the reproduction would be influenced by whether the evaluation had been completed or not, and vice versa. However, given the many possible influences on the timings, this was considered negligble.We also required consensus on whether items had been succesfully reproduced or not before moving on. In some cases, this was not done, as other team members were not available (e.g. busy or on annual leave) and so could not yet give a second opinion, and so I progressed with the evaluation and got consensus on reproductions afterwards, once they were available.\n\n\nOrganisation of the repository for the research compendium\nIn the protocol, we had planned that seperate folders were created for data, methods and outputs. This was generally followed but, if an alternative structure seemed more suitable. For example, if the original study already divided items well, but perhaps with different folder names or with multiple scripts folders or so on, we might have used that original structure, as it still served the purpose of being clear and easy to re-run, whilst reducing the number of differences compared with the original study.\n\n\nRetrospective archiving\nFor Johnson et al. 2021, I forgot to create a release to archive this repository after defining scope and before proceeding with reproduction. However, I was easily able to resolve this by setting the release to a prior commit, choosing the commit from that timepoint (between scope and reproduction), so it was as if it had been done at the time\n\n\nIdentification of papers\nAlthough X  papers were identified from the review by Monks and Harper (2023), X were identified from additional searches (informal, not systematic)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#timings",
    "href": "pages/protocol.html#timings",
    "title": "3  Methods",
    "section": "3.5 Timings",
    "text": "3.5 Timings\nAs per the protocol, the reproduction and evaluation stages were timed. Although this was conducted carefully and thoroughly, it will not be perfect, and we recognise some of the potential sources of variation in timings between studies, such as:\n\nWhether we consistently included amendments to the quarto site and repository and time spent on GitHub commits etc. within the timings for the reproduction.\nFor the first R study (Huang et al. (2019)), I initially tried to create an environment with R and package versions prior to the article publication date, although had great difficulties with this and ended up using the latest versions. This contributed to the set-up time during this reproduction, but on later R models (Kim et al. (2021) and Johnson et al. (2021)), based on that experience, I did not attempt to backdate them when getting started.\nAny estimated times (for example, if I were partway through working but someone in the office came to talk to me and I forgot to note the time of that, I might estimate if that were about five or ten minutes of conversation, and set the time accordingly).\nTimings from consensus discussions regarding uncertainities in the evaluation or reproduction (as these might be longer if done in person rather than over email - or vice versa - and I sometimes spent longer on sorting/tidying these for some studies than others, which I would have included in the time)\nWhether subjectively feel that need to add random seeds during reproduction stage, if results vary considerably between each run, and so a certain seed could get a much more similar result than another\n\nAt an estimate, this uncertainty between study timings would lead me to conclude that the timings are approximately correct, give or take up to about four hours. However, this is just an estimate, and it is worth noting that Krafczyk et al. (2021), who also conducted computational reproducibility assessments in a different context, estimated that human error introduced a maximum of 8 hours ambiguity in the timings, due to the “non-precise nature of starting and stopping the watch consistently”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#references",
    "href": "pages/protocol.html#references",
    "title": "3  Methods",
    "section": "3.6 References",
    "text": "3.6 References\n\n\n\n\nAssociation for Computing Machinery (ACM). 2020. “Artifact Review and Badging Version 1.1.” ACM. https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nAssociation for Psychological Science (APS). 2023. “Psychological Science Submission Guidelines.” APS. https://www.psychologicalscience.org/publications/psychological_science/ps-submissions.\n\n\nAyllón, Daniel, Steven F. Railsback, Cara Gallagher, Jacqueline Augusiak, Hans Baveco, Uta Berger, Sandrine Charles, et al. 2021. “Keeping Modelling Notebooks with TRACE: Good for You and Good for Environmental Research and Management Support.” Environmental Modelling & Software 136 (February): 104932. https://doi.org/10.1016/j.envsoft.2020.104932.\n\n\nBerkeley Initiative for Transparency in the Social Sciences. 2022. “Guide for Advancing Computational Reproducibility in the Social Sciences.” https://bitss.github.io/ACRE/.\n\n\nBlohowiak, Ben B., Johanna Cohoon, Lee de-Wit, Eric Eich, Frank J. Farach, Fred Hasselman, Alex O. Holcombe, Macartan Humphreys, Melissa Lewis, and Brian A. Nosek. 2023. “Badges to Acknowledge Open Practices.” https://osf.io/tvyxz/.\n\n\nHardwicke, Tom E., and Simine Vazire. 2023. “Transparency Is Now the Default at Psychological Science.” Psychological Science 0 (0). https://doi.org/https://doi.org/10.1177/09567976231221573.\n\n\nHeather, Amy, Thomas Monks, Alison Harper, Navonil Mustafee, and Andrew Mayne. 2024. “Protocol for Assessing the Computational Reproducibility of Discrete-Event Simulation Models on STARS,” June. https://zenodo.org/records/12179846.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nInstitute of Electrical and Electronics Engineers (IEEE). n.d. “About Content in IEEE Xplore.” IEEE Explore. Accessed May 20, 2024. https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-content.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nKonkol, Markus, Christian Kray, and Max Pfeiffer. 2019. “Computational Reproducibility in Geoscientific Papers: Insights from a Series of Studies with Geoscientists and a Reproduction Study.” International Journal of Geographical Information Science 33 (2): 408–29. https://doi.org/10.1080/13658816.2018.1508687.\n\n\nKrafczyk, M. S., A. Shi, A. Bhaskar, D. Marinov, and V. Stodden. 2021. “Learning from Reproducing Computational Results: Introducing Three Principles and the Reproduction Package.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 379 (2197): 20200069. https://doi.org/10.1098/rsta.2020.0069.\n\n\nLaurinavichyute, Anna, Himanshu Yadav, and Shravan Vasishth. 2022. “Share the Code, Not Just the Data: A Case Study of the Reproducibility of Articles Published in the Journal of Memory and Language Under the Open Data Policy.” Journal of Memory and Language 125 (August): 104332. https://doi.org/10.1016/j.jml.2022.104332.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using R (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\nMcManus, Emma, David Turner, and Tracey Sach. 2019. “Can You Repeat That? Exploring the Definition of a Successful Model Replication in Health Economics.” PharmacoEconomics 37 (11): 1371–81. https://doi.org/10.1007/s40273-019-00836-y.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.\n\n\nNISO Reproducibility Badging and Definitions Working Group. 2021. “Reproducibility Badging and Definitions.” https://doi.org/10.3789/niso-rp-31-2021.\n\n\nSchwander, Björn, Mark Nuijten, Silvia Evers, and Mickaël Hiligsmann. 2021. “Replication of Published Health Economic Obesity Models: Assessment of Facilitators, Hurdles and Reproduction Success.” Pharmacoeconomics 39 (4): 433–46. https://doi.org/10.1007/s40273-021-01008-7.\n\n\nWood, Benjamin D. K., Rui Müller, and Annette N. Brown. 2018. “Push Button Replication: Is Impact Evaluation Evidence for International Development Verifiable?” PLOS ONE 13 (12): e0209416. https://doi.org/10.1371/journal.pone.0209416.\n\n\nWood, Benjamin, Annette Brown, Eric Djimeu, Maria Vasquez, Semi Yoon, and Jane Burke. 2018. “Replication Protocol for Push Button Replication (PBR).” OSF, January. https://doi.org/https://doi.org/10.17605/OSF.IO/YFBR8.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html",
    "href": "pages/reproduction.html",
    "title": "4  Reproduction",
    "section": "",
    "text": "4.1 Studies\nShoaib and Ramamohan (2021): Uses python (salabim) to model primary health centres (PHCs) in India. The model has four patient types: outpatients, inpatients, childbirth cases and antenatal care patients. Four model configurations are developed based on observed PHC practices or government-mandated operational guidelines. The paper explores different operational patterns for scenarios where very high utilisation was observed, to explore what might help reduce utilisation of these resources. Note: The article was as Shoaib and Ramamohan (2022), but we used the green open access pre-print Shoaib and Ramamohan (2021). Link to reproduction.\nHuang et al. (2019): Uses R (simmer) to model an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway. The paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR. Link to reproduction.\nLim et al. (2020): Uses python (NumPy and pandas) to model the transmission of COVID-19 in a laboratory. It examines the proportion of staff infected in scenarios varying the: number of shifts per day; number of staff per shift; overall staff pool; shift patterns; secondary attack rate of the virus; introduction of protective measures (social distancing and personal protective equipment). Link to reproduction.\nKim et al. (2021): Adapts a previously developed R (Rcpp, expm, msm, foreach, iterators, doParallel) model for abdominal aortic aneurysm (AAA) screening of men in England. The model is adapted/used to explore different approaches to resuming screening and surgical repair for AAA, as these services were paused or substantially reduced during COVID-19 due to concerns about virus transmission. Link to reproduction.\nAnagnostou et al. (2022): This paper includes two models - we have focussed just on the dynamiC Hospital wARd Management (CHARM) model. CHARM uses python (SimPy) to model intensive care units (ICU) in the COVID-19 pandemic (as well as subsequent stays in a recovery bed). It includes three types of admission to the ICU (emergency, elective or COVID-19). COVID-19 patients are kept seperate, and if they run out of capacity due to a surge in COVID-19 admissions, additional capacity can be pooled from the elective and emergency capacity. Link to reproduction.\nJohnson et al. (2021): TBC\nHernandez et al. (2015): This study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense counter-measures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput. They use python for most of the analysis (with SimPy for the simulation component), but R to produce the plots and tables for the paper. Link to reproduction.\nWood et al. (2021): This study uses discrete-event simulation (R (base R, dplyr, tidyr)) to explore the deaths and life years lost under different triage strategies for an intensive care unit, relative to a baseline strategy. The unit is modelled with 20 beds (varied from 10 to 200 in sensitivity analyses). Three different triage strategies are explored, under three different projected demand trajectories. Link to reproduction.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#scope",
    "href": "pages/reproduction.html#scope",
    "title": "4  Reproduction",
    "section": "4.2 Scope",
    "text": "4.2 Scope\n\n\n\n\n\n\n\n\n\nStudy\nScope\nSuccess\nTime\n\n\n\n\nShoaib and Ramamohan 2022\n17 items:• 1 table• 9 figures• 7 in-text results\n16 out of 17 (94%)\n28h 14m\n\n\nHuang et al. 2019\n8 items:• 5 figures• 3 in-text results\n3 out of 8 (37.5%)\n24h 10m\n\n\nLim et al. 2020\n9 items:• 5 tables• 4 figures\n9 out of 9 (100%)\n12h 27m\n\n\nKim et al. 2021\n10 items:• 3 tables• 6 figures• 1 in-text result\n10 out of 10 (100%)\n14h 42m\n\n\nAnagnostou et al. 2022\n1 item:• 1 figure\n1 out of 1 (100%)\n2h 11m\n\n\nJohnson et al. 2021\n5 items:• 1 table• 4 figures\n\n\n\n\nHernandez et al. 2015\n8 items:• 6 figures• 2 tables\n1 out of 8 (12.5%)\n17h 56m\n\n\nWood et al. 2021\n5 items:• 4 figures• 1 table\n5 out of 5 (100%)\n3h 50m",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#time-to-completion",
    "href": "pages/reproduction.html#time-to-completion",
    "title": "4  Reproduction",
    "section": "4.3 Time to completion",
    "text": "4.3 Time to completion\n\nNon-interactive figure:\n\n\n\n\n\n\n\n\n\nInteractive figure:\n\n\n                                                \n\n\n\n\n\n\n\n\nReproduction reflections\n\n\n\n\n\nTODO: Where possible, reflect on what I think to be the primary reason that didn’t manage to fully reproduce despite troubleshooting, for the studies where this was the case\n\n\n\n\n\n\n\n\n\nTiming reflections\n\n\n\n\n\nTODO: Reflect on these timings - what I think were the primary reason for things to be quicker or slower - and what I think was relevant/important",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#references",
    "href": "pages/reproduction.html#references",
    "title": "4  Reproduction",
    "section": "4.4 References",
    "text": "4.4 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\n———. 2022. “Simulation Modeling and Analysis of Primary Health Center Operations.” SIMULATION 98 (3): 183–208. https://doi.org/10.1177/00375497211030931.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html",
    "href": "pages/reflections.html",
    "title": "5  Reflections from reproductions",
    "section": "",
    "text": "5.1 Environment",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#environment",
    "href": "pages/reflections.html#environment",
    "title": "5  Reflections from reproductions",
    "section": "",
    "text": "List required packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n✅\n✅\n\n🟡\n❌\n\n\n\nShoaib and Ramamohan (2021): Not met. This became a time-consuming issue as it took a while to identify a dependency that was needed for the code to work (greenlet) (based on reading the documentation for salabim), and a while longer to realise I had installed another package when the package I needed was in base (statistics).\nHuang et al. (2019): Not met. However, this was fairly easily resolved based on imports to .R script, and then on extra imports suggested by RStudio when I tried and failed to run the script.\nLim et al. (2020): Partially met. The only packages needed (numpy and pandas) are mentioned in the paper (although only listed as imports in the script).\nKim et al. (2021): Fully met. Provides commands to install packages required at the start of scripts, which I could then easily base renv on automatically (as it detects them).\nAnagnostou et al. (2022): Fully met. Provides requirements.txt\nJohnson et al. (2021): \nHernandez et al. (2015): Partially met. Some (but not all) of the required packages were listed in the paper. Of particular note, this depended on having a local package myutils/, which was another GitHub repository from the author. This was not mentioned anywhere, and so required to notice this was needed.\nWood et al. (2021): Not met. However, easily resolved based on imports to .R script.\nReflections: \n\nThe import statements can be sufficient in indicating all the packages required but this is not always the case if there are “hidden”/unmentioned dependencies that don’t get imported\nThere are various options for listing the packages (e.g. comprehensive import statements, installation lines in the script, environment files).\nIdeally mention this in repository, not just the paper.\nIf there are local dependencies (e.g. other GitHub repositories), make sure to (a) mention and link to these repositories, so it is clear they are also required, and (b) include licence/s in those repositories also, so they can be used.\nThis was a common issue.\n\n\n\n\n\n\n\n\n\n\nProvide versions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n🟡\n🟡\n\n🟡\n🟡\n\n\n\nShoaib and Ramamohan (2021): Not met. I had to backdate the package versions as the model didn’t work with the latest.\nHuang et al. (2019): Not met. I initially tried to create an environment with R and package versions that were prior to the article publication date. However, I had great difficulties implementing this with R, and never managed to successfully do this. This was related to:\n\nThe difficulty of switching between R versions\nProblems in finding available/a source for specific package versions for specific versions or R\n\nLim et al. (2020): Partially met. Provides major Python version, but chose minor and the package versions based on article publication date.\nKim et al. (2021): Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.\nAnagnostou et al. (2022): Partially met (depending on how strict you are being). The Python version was stated in the paper, and the SimPy version was stated in the complementary app repository (although neither were mentioned in the model repository itself).\nJohnson et al. (2021): \nHernandez et al. (2015): Partially. Versions of Python, R and some (but not all) packages given in the paper. Some versions weren’t very specific (e.g. Python 2.7 v.s. something specific like 2.7.12)\nWood et al. (2021): Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.\nReflections: \n\nModels will sometimes work with the latest versions of packages, but likewise, you will sometimes need to backdate as it no longer works with the latest\nFor Python, it was very easy to “backdate” the python and package versions. However, I found this very difficult to in R, and ended up always using the latest versions.\nVersions are sometimes provided elsewhere (e.g. in paper, in other repositories), but would be handy to be in model repository itself.\nHandy to provide specific versions too, particularly when there can be reasonably large changes between minor versions.\nThis was a very common issue.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#structure-of-code-and-scripts",
    "href": "pages/reflections.html#structure-of-code-and-scripts",
    "title": "5  Reflections from reproductions",
    "section": "5.2 Structure of code and scripts",
    "text": "5.2 Structure of code and scripts\n\n\n\n\n\n\nModel is provided in a “runnable” format\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n✅\n✅\n✅\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Fully met. Provided as a single .py file which ran model with function main().\nHuang et al. (2019): Not met. The model code was provided within the code for a web application, but the paper was not focused on this application, and instead on specific model scenarios. I had to extract the model code and transform it into a format that was “runnable” as an R script/notebook.\nLim et al. (2020): Fully met. Provided as a single .py file which ran the model with a for loop.\nKim et al. (2021): Fully met. Has seperate .R scripts for each scenario which ran the model by calling functions from elsewhere in repository.\nAnagnostou et al. (2022): Fully met. Can run model from command line.\nJohnson et al. (2021): \nHernandez et al. (2015): Fully met. The model (python code) can be run from main.py.\nWood et al. (2021): Fully met. Provided as a single .R file which ran the model with a for loop.\nReflections: \n\nIf you are presenting the results of a model, then provide the code for that model in a “runnable” format.\nThis was an uncommon issue.\n\n\n\n\n\n\n\n\n\n\n\nModel is designed to be run programmatically (i.e. can run model with different parameters without needing to change the model code)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n✅\n✅\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The model is set up as classes and run using a function. However, it is not designed to allow any variation in inputs. Everything uses default inputs, and it designed in such a way that - if you wish to vary model parameters - you need to directly change these in the script itself.\nHuang et al. (2019): Fully met. Model was set up as a function, with many of the required parameters already set as “changeable” inputs to that function.\nLim et al. (2020): Fully met. The model is created from a series of functions and run with a for loop that iterates through different parameters. As such, the model is able to be run programmatically (within that for loop, which varied e.g. staff per shift and so on and re-ran the model).\nKim et al. (2021): Fully met. Each scenario is an R script which states different parameters and then calls functions to run model.\nAnagnostou et al. (2022): Fully met. Change inputs in input .csv files.\nJohnson et al. (2021): \nHernandez et al. (2015): Fully met. Model created from classes, which accept some inputs and can run the model.\nWood et al. (2021): Fully met. Changes inputs to run all scenarios from a single .R file.\nReflections: \n\nDesign model so that you can re-run it with different parameters without needing to make changes to the model code itself.\n\nThis allows you to run multiple versions of the model with the same script.\nIt also reduces the likelihood of missing errors (e.g. if miss changing an input parameter somewhere, or input the wrong parameters and don’t realise).\n\nThis was an uncommon issue.\n\n\n\n\n\n\n\n\n\n\nDon’t hard code parameters that you will want to change for scenario analyses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n🟡\n❌\n✅\nN/A\n\n🟡\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. Although some parameters sat “outside” of the model within the main() function (and hence were more “changeable”, even if not “changeable” inputs to that function, but changed directly in script). However, many of the other parameters were hard-coded within the model itself. It took time to spot where these were and correctly adjust them to be modifiable inputs.\nHuang et al. (2019): Partially met. Pretty much all of the parameters that we wanted to change were not hard coded and were instead inputs to the model function simulate_nav(). However, I did need to add an exclusive_use scenario which conditionally changed ir_resources, but that is the only exception. I also add ed_triage as a changeable input but didn’t end up needing that to reproduce any results (was just part of troubleshooting). I also\nLim et al. (2020): Not met. Some parameters were not hard coded within the model, but lots of them were not.\nKim et al. (2021): Fully met. All model parameters could be varied from “outside” the model code itself, as they were provided as changeable inputs to the model.\nAnagnostou et al. (2022): N/A as no scenarios.\nJohnson et al. (2021): \nHernandez et al. (2015): Partially met. Did not hard code, runs, population, generations, and percent pre-screened. However, did hard code other parameters like bi-objective v.s tri-objective model and bounding. Also, it was pretty tricky to change percent pre-screened, as it assumed you provided a .txt file for each %.\nWood et al. (2021): Fully met. All model parameters for the scenarios/sensitivity analysis could be varied from “outside” the model code itself.\nReflections: \n\nIt can be quite difficult to change parameters that are hard coded into the model. Ideally, all the parameters that a user might want to change should be easily changeable and not hard coded.\nThis is a somewhat common issue.\nThere is overlap between this and whether the code for scenarios is provided (as typically, the code for scenario is conditionally changing parameter values, although this can be facilitated by not hard coding the parameters, so you call need to change the values from “outside” the model code, rather than making changes to the model functions themselves). Hence, have included as two seperate reflections.\nImportant to note that we evaluate this in the context of reproduction - and have not checked for hard-coded parameters outside the specified scenario analyses, but that someone may wish to alter if reusing the model for a different analysis/context/purpose.\n\n\n\n\n\n\n\n\n\n\nUse relative file paths\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n\n\n\n\n\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): \nHuang et al. (2019): \nLim et al. (2020): \nKim et al. (2021): \nAnagnostou et al. (2022): \nJohnson et al. (2021): \nHernandez et al. (2015): Fully met. Creates folder in current working directory based on date/time to store results.\nWood et al. (2021): Fully met. Although I then changed things a bit as reorganised repository and prefer not to work with setwd(), these were set up in such a way that it would be really easy to correct file path, just by setting working directory at start of script.\nReflections: \n\n\n\n\n\n\n\n\n\nAvoid large amounts of code duplication\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n❌\n✅\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The model often contained very similar blocks of code before or after warm-up. This has the potential to introduce mistakes - with a suspected (although unconfirmed) mistake being that the lower boundary for the doctor consultation times in configuration 1 differed before and after warm-up.\nHuang et al. (2019): Fully met.\nLim et al. (2020): Fully met.\nKim et al. (2021): Not met. There was alot of duplication when running each scenario (e.g. repeated calls to Eventsandcosts, and repeatedly defining the same parameters). This meant, if changing a parameter that you want to be consistent between all the scripts (e.g. number of persons), you had to change each of the scripts one by one.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): \nHernandez et al. (2015): Fully met.\nWood et al. (2021): Fully met.\nReflections:  Large amounts of code duplication are non-ideal as they can:\n\nMake code less readable\nMake it trickier to change universal parameters\nIncrease the likelihood of introducing mistakes\n\n\n\n\n\n\n\n\n\n\nInclude sufficient comments in the code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n✅\n🟡\n🟡\n\n🟡\n❌\n\n\n\nShoaib and Ramamohan (2021) and Huang et al. (2019): Not met. Would have benefitted from more comments, as it took some time to ensure I have correctly understood code, particularly if they used lots of abbreviations.\nLim et al. (2020): Fully met. There were lots of comments in the code (including doc-string-style comments at the start of functions) that aided understanding of how it worked.\nKim et al. (2021): Partially met. Didn’t have any particular issues in working out the code. There are sufficient comments in the scenario scripts and at the start of the model scripts, although within the model scripts, there were sometimes quite dense sections of code that would likely benefit from some additional comments.\nAnagnostou et al. (2022): Partially met. Didn’t have to delve into the code much, so can’t speak from experience as to whether the comments were sufficient. From looking through the model code, several scripts have lots of comments and docstrings for each function, but some do not.\nJohnson et al. (2021): \nHernandez et al. (2015): Partially met. There are some comments and doc-strings, but not comprehensively.\nWood et al. (2021): Not met. Very few comments, so for the small bit of the code that I did delve into, took a bit of working out what different variables referred to.\nReflections: \n\nWith increasing code complexity, the inclusion of sufficient comments becomes increasingly important, as it can otherwise be quite time consuming to figure out how to fix and change sections of code\nDefine abbreviations used within the code\nGood to have consistent comments and docstrings throughout (i.e. on all scripts, on not just some of them)",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#run-time-and-memory-usage",
    "href": "pages/reflections.html#run-time-and-memory-usage",
    "title": "5  Reflections from reproductions",
    "section": "5.3 Run time and memory usage",
    "text": "5.3 Run time and memory usage\n\n\n\n\n\n\nQuicker models are easier to work with\n\n\n\n\n\nI have not evaluated like as a criteria, as a long run time is not inherently a bad thing. However, I definitely found that the run time of models had a big impact on how easy it was to reproduce results as longer run times meant it was tricky (or even impossible) to run in the first place, or tricky to re-run.\nThe studies where I made adjustments were:\n\nShoaib and Ramamohan (2021): Add parallel processing and ran fewer replications\nHuang et al. (2019): No changes made.\nLim et al. (2020): Add parallel processing\nKim et al. (2021): Reduced number of people in simulation, and switched from serial to the provided parallel option.\nAnagnostou et al. (2022): Model was super quick which made it really easy to run and re-run each time\nJohnson et al. (2021): \nHernandez et al. (2015): Add parallel processing, did not run one of the scenarios (it was very long, and hadn’t managed to reproduce other parts of same figure regardless), and experimented with reducing parameters for evolutionary algorithm (but, in the end, ran with full parameters, though lower were helpful while working through and troubleshooting).\nWood et al. (2021): No changes made, but unlike other reproduction, didn’t try to run at smaller amounts - just set it to run as-is over the weekend.\n\nFor Kim et al. (2021), an error appears to have been introduced with the aoorta diameter thresholds by switching between nested and unnested lists, which I’m anticipating was unresolved due to the long run times of the model meaning they weren’t all run in sequence at the end.\nReflections: \n\nReduce model run time if possible as it makes it easier to work with, and facilitates doing full re-runs of all scenarios (which can be important with code changes, etc).\n\nRelatedly, it is good practice to re-run all scripts before finishing up, as then you can spot any errors like the one mentioned for Kim et al. (2021)\n\nCommon issue (to varying degrees - i.e. taking 20 minutes, up to taking several hours).\n\n\n\n\n\n\n\n\n\n\nFor slow models, state the expected run time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n❌\n❌\nN/A\n\n🟡\n🟡\n\n\n\nShoaib and Ramamohan (2021): Fully met. Run time stated in paper (but not repository).\nHuang et al. (2019): Not met.\nLim et al. (2020): Not met.\nKim et al. (2021): Not met. A prior paper describing the model development mentions the run time, but not the current paper or repository, so this is easily missed.\nAnagnostou et al. (2022): Not applicable. Very quick! Seconds! So not particularly relevant - although, you could argue, potentially still important if there were some error that made it look like the model were running continuously (e.g. stuck in a loop) - although this is relatively unlikely.\nJohnson et al. (2021): \nHernandez et al. (2015): Partially met. Some of the run times are mentioned in the paper, but not all, although this did help indicate that we would anticipate other s scenarios to similarly take hours to run.\nWood et al. (2021): Partially met. In the paper, they state that it takes less than five minutes for each scenario, but this feels like half the picture, given the total run time was 48 hours.\nReflections: \n\nFor long models with no statement, it can take a while to realise that it’s not an error in the code or anything, but actually just a long run time! And hard to know how long to expect, and whether it is without the capacities of your machine and so on.\nIdeally include statement of run time in repository as well as paper.\nIdeally include run time of all components of analysis (e.g. all scenarios).\nCommon issue.\n\n\n\n\n\n\n\n\n\n\nFor computationally expensive models, state memory usage and provide alternatives for lower spec machines\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\nN/A\nN/A\nN/A\n❌\nN/A\n\n\nN/A\n\n\n\nShoaib and Ramamohan (2021), Huang et al. (2019), and Lim et al. (2020): Not applicable. Didn’t find it to be too computationally expensive for my machine.\nKim et al. (2021): Not met. Unable to run on my machine (serial took too long to run (would have to leave laptop on for many many hours which isn’t feasible), and parallel was too computationally expensive and crashed the machine (with the original number of people)). This is not mentioned in the repository or paper, but only referred to in a prior publication. Would’ve been handy if it included suggestions like reducing number of people and so on (which is what I had to do to feasibly run it).\nAnagnostou et al. (2022): Not applicable. Runs in seconds.\nJohnson et al. (2021): \nHernandez et al. (2015): \nWood et al. (2021): Not applicable. As stated in their prior paper, the model is constrained by processing time, not computer memory.\nReflections: \n\nSome models are so computationally expensive that it may be simply impossible to run it a feasible length of time without a high powered machine.\nIf a model is computationally expensive, it would be good to provide suggested alternatives that allow it to be run on lower spec machines\nNot a common problem - only relevant to computationally expensive models",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#parameters-scenarios-and-outputs",
    "href": "pages/reflections.html#parameters-scenarios-and-outputs",
    "title": "5  Reflections from reproductions",
    "section": "5.4 Parameters, scenarios and outputs",
    "text": "5.4 Parameters, scenarios and outputs\n\n\n\n\n\n\nProvide code for all scenarios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\nN/A\n\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. There were several instances where it took quite a while to understand how and where to modify the code in order to run scenarios (e.g. no arrivals, transferring admin work, reducing doctor intervention in deliveries).\nHuang et al. (2019): Not met. Set up a notebook to programmatically run the model scenarios. It took alot of work to modify and write code that could run the scenarios, and I often made mistakes in my interpretation for the implementation of scenarios, which could be avoided if code for those scenarios was provided.\nLim et al. (2020): Not met. Several parameters or scenarios were not incorporated in the code, and had to be added (e.g. with conditional logic to skip or change code run, removing hard-coding, adding parameters to existing).\nKim et al. (2021): Not met. Took alot of work to change model from for loop to function, to set all parameters as inputs (some were hard coded), and add conditional logic of scenarios when required.\nAnagnostou et al. (2022): Not applicable. No scenarios.\nJohnson et al. (2021): \nHernandez et al. (2015): Not met. Took a while to figure out how to implement scenarios.\nWood et al. (2021): Fully met.\nReflections: \n\nCommon issue\nTime consuming and tricky to resolve\n\n\n\n\n\n\n\n\n\n\nAll the required outputs are calculated/provided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\n✅\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. Had to add some outputs and calculations (e.g. proportion of childbirth cases referred, standard deviation)\nHuang et al. (2019): Not met. It has a complicated output (standardised density of patient in queue) that I was never certain on whether I correctly calculated. Although it outputs the columns required to calculate it, due its complexity, I feel this was not met, as it feels like a whole new output in its own right (and not just something simple like a mean).\nLim et al. (2020): Not met. The model script provided was only set up to provide results from days 7, 14 and 21. The figures require daily results, so I needed to modify the code to output that.\nKim et al. (2021): Not met. Had to write code to find aorta sizes of people with AAA-related deaths.\nAnagnostou et al. (2022): Fully met. Although worth noting this only had one scenario/version of model and one output to reproduce.\nJohnson et al. (2021): \nHernandez et al. (2015): Fully met.\nWood et al. (2021): Fully met.\nReflections: \n\nCalculate and provide all the outputs required\nAppreicate this can be a bit “ambiguous” (e.g. if its just plotting a mean or simple calculation, then didn’t consider that here) (however, combined with other criteria, we do want them to provide code to calculate outputs, so we would want them to provide that anyway)\n\n\n\n\n\n\n\n\n\n\nInclude correct parameters in the script (even if just for one scenario)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n🟡\n❌\n🟡\n✅\n✅\n\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Partially met. Script is set with parameters for base configuration 1, with the exception of number of replications.\nHuang et al. (2019): Not met. The baseline model in the script did not match the baseline model (or any scenario) in the paper, so had to modify parameters.\nLim et al. (2020): Partially met. The included parameters were corrected, but the baseline scenario included varying staff strength to 2, and the provided code only varied 4 and 6. I had to add some code that enabled it to run with staff strength 2 (as there were an error that occured if you tried to set that).\nKim et al. (2021): Fully met.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): \nHernandez et al. (2015): Not met. As agreed with the author, this is likely the primary reason for the discrepancy in these results - they are very close, and we see similar patterns, but not reproduced. Unfortunately, several parameters were wrong, and although we changed those we spotted, we anticipate there could be others we hadn’t spotted that might explain the remaining discrepancies.\nWood et al. (2021): Fully met.\nReflections: \n\nAt least provide a script that can run the baseline model as in the paper (even if not providing the scenarios)\nThis can introduce difficulties - when some parameters are wrong, you rely on the paper to check which parameters are correct or not, but if the paper doesn’t mention every single parameter (which is reasonably likely, as this includes those not varied by scenarios), then you aren’t able to be sure that the model you are running is correct.\nThis can make a really big difference, and be likely cause of managing to reproduce everything v.s. nothing, if it impacts all aspects of the results.\n\n\n\n\n\n\n\n\n\n\nProvide all the required parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n✅\n✅\n\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Some parameters that could not be calculated were not provided - ie. what consultation boundaries to use when mean length of doctor consultation was 2.5 minutes\nHuang et al. (2019): Not met. In this case, patient arrivals and resource numbers were listed in the paper, and there were several discprepancies between this and the provided code. However, for many of the model parameters like length of appointment, these were not mentioned in the paper, and so it was not possible to confirm whether or not those were correct. Hence, marked as not met, as the presence of discrepenancies for several other parameters puts these into doubt.\nLim et al. (2020): Not met. For Figure 5, had to guess the value for staff_per_shift.\nKim et al. (2021): Fully met.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): \nHernandez et al. (2015): Not met. The results have a large impact by the bounding set, but this was not mentioned in the paper or repository, and required me looking at the numbers in results and GitHub commit history to estimate the appropriate bounds to use.\nWood et al. (2021): Fully met.\nReflections: \n\nProvide all required parameters\n\n\n\n\n\n\n\n\n\n\nIf not provided in the script, then clearly present all parameters in the paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\nN/A\nN/A\n\n🟡\nN/A\n\n\n\nShoaib and Ramamohan (2021): Not met. Although there was a scenario table, this did not include all the parameters I would need to change. It was more challenging to identify parameters that were only described in the body of the article. There were also some discrepancies in parameters between the main text of the article, and the tables and figures. Some scenarios were quite ambiguous/unclear from their description in the text, and I initially misunderstood the required parameters for the scenarios.\nHuang et al. (2019): Not met. As described above, paper didn’t adequately describe all parameters.\nLim et al. (2020): Partially met. Nearly all parameters are in the paper table, and others are described in the article. However, didn’t provide information for the staff_per_shift for Figure 5.\nKim et al. (2021) and Anagnostou et al. (2022): Not applicable. All provided.\nJohnson et al. (2021): \nHernandez et al. (2015): Most parameters are relatively easily identified from the text or figure legends (though would be easier if provided in a table or similar). Parameter for bounding was not provided in paper.\nWood et al. (2021): Not applicable. All provided.\nReflections: \n\nProvide parameters in a table (including for each scenario), as it can be difficult/ambiguous to interpret them from the text, and hard to spot them too.\nBe sure to mention every parameter that gets changed (e.g. for Lim et al. (2020), as there wasn’t a default staff_per_shift across all scenarios, but not stated for the scenario, had to guess it).\n\n\n\n\n\n\n\n\n\n\nIf will need to process parameters, provide required calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\nN/A\nN/A\nN/A\n\nN/A\nN/A\n\n\n\nShoaib and Ramamohan (2021): Not met. It was unclear how to estimate inter-arrival time.\nHuang et al. (2019): Fully met. The calculations for inter-arrival times were provided in the code, and the inputs to the code were the number of arrivals, as reported in the paper, and so making it easy to compare those parameters and check if numbers were correct or not.\nLim et al. (2020): Not applicable. The parameter not provided is not one that you would calculate.\nKim et al. (2021) and Anagnostou et al. (2022): Not applicable. All provided.\nJohnson et al. (2021): \nHernandez et al. (2015): Not applicable.\nWood et al. (2021): Not applicable. All provided.\nReflections: \n\nIf you are going to be mentioning the “pre-processed” values at all, then its important to include the calculation (ideally in the code, as that is the clearest demonstration of exactly what you did)",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#output-format",
    "href": "pages/reflections.html#output-format",
    "title": "5  Reflections from reproductions",
    "section": "5.5 Output format",
    "text": "5.5 Output format\n\n\n\n\n\n\nSaves output to a file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n❌\n❌\n✅\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Fully met. Outputs to .xlsx files\nHuang et al. (2019), Lim et al. (2020), and Kim et al. (2021): Not met. Outputs to dataframe/s.\nAnagnostou et al. (2022): Outputs to OUT_STATS.csv. Note: Although not needed for the reproduction itself, when I tried to amend the name and location of the csv file output the model for use in tests, this was very tricky to do as it was hard coded into the scripts and I found difficult to amend due to how the model is run and set up.\nJohnson et al. (2021): \nHernandez et al. (2015): Fully met. Outputs to .txt files.\nWood et al. (2021): Fully met. Outputs to .csv files.\nReflections: \n\nCommon issue\nParticularly important if model run time is even slightly long (even just minutes long, but even more so as becomes many minutes / hours), so don’t always have to re-run it each time to get results\nSet up this in such a way that it is easy to change the name and location of the output file.\n\n\n\n\n\n\n\n\n\n\nUnderstandable output tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n❌\n✅\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. There were two alternative results spreadsheets with some duplicate metrics but sometimes differing results between them, which made it a bit confusing to work out what to use.\nHuang et al. (2019), Lim et al. (2020), and Anagnostou et al. (2022): Fully met. Didn’t experience issues interpreting the contents of the output table/s.\nKim et al. (2021): Not met. It took me a little while to work out what surgery columns I needed, and to realise I needed to combine two of them. This required looking at what inputs genreated this, and referring to a input data dictionary.\nJohnson et al. (2021): \nHernandez et al. (2015): Fully met. Straightforward with key information provided.\nWood et al. (2021): Fully met. I didn’t need to work with the output tables, but from looking at them now, they make sense.\nReflections: \n\nDon’t provide alternative results for the same metrics\nMake it clear what each colum/category in the results table means, if it might not be immediately clear.\n\n\n\n\n\n\n\n\n\n\nAvoid large file sizes if possible\n\n\n\n\n\nI have not evaluated like as a criteria, as a large file size is not inherently a bad thing, and might be difficult to avoid. However, when files are very large, this can make things trickier, such as with requiring compression and use of GitHub Large File Storage (LFS) for tracking, which has limits on the free tier.\nRegarding file sizes in each study:\n\nShoaib and Ramamohan (2021): Not relevant (results files &lt;35 kB)\nHuang et al. (2019): Provided code didn’t save results to file. When I saved to file, these were large, so I compressed to .csv.gz, which made them small enough that GitHub was still happy (26 MB).\nLim et al. (2020): Provided code didn’t save results to file. When I saved to file, these were small, so not relevant (results files &lt;60 kB)\nKim et al. (2021): Provided code didn’t save results to file. When I saved to file, these were small, so not relevant (results files &lt;1 kB)\nAnagnostou et al. (2022): Not relevant (results file 34 kB)\nJohnson et al. (2021): \nHernandez et al. (2015): Not relevant (aggregate results files &lt;10 kB).\nWood et al. (2021): Aggregate results files are small (327 kB), but raw results files are very large (2.38 GB), and even when compressed to .csv.gz (128 MB) require using of GitHub LFS\n\nReflections: \n\nFor most studies, this was not relevant, with outputs relatively small\nI only really found this to be an issue when files exceeded GitHub threshold. GitHub give warning over 50 MB, blocks files over 100 MB, requiring you to use GitHub LFS. Recommends repositories ideally &lt;1 GB and for sure &lt; 5GB. GitHub LFS has limits on storage and bandwith use (1GB of each).\nReducing file size isn’t the only solution. In cases where you have large files, a good option can be storing it elsewhere and then pulling from there into your workflow - for example, storing in zenodo and fetching using pooch.\n\n\n\n\n\n\n\n\n\n\nDo not output excessive numbers of files\n\n\n\n\n\nI have not evaluated like as a criteria, as a large number of files is not inherently a bad thing, and might be difficult to avoid.\nRegarding number of files:\n\nShoaib and Ramamohan (2021): Not relevant.\nHuang et al. (2019): Not relevant.\nLim et al. (2020): Not relevant.\nKim et al. (2021): Not relevant.\nAnagnostou et al. (2022): Not relevant.\nJohnson et al. (2021): \nHernandez et al. (2015): The default behaviour of the script was to output lots of files from each round (so you could easily have 90, 100, 200+ files), which were then not used in analysis (as it just depended on an aggregate results file). Although these individual files might be useful during quality control, as a default behaviour of the script, it could easily make the repository quite busy/littered with files.\nWood et al. (2021): Not relevant.\n\nReflections: \n\nOutputting hundreds of files - particularly if they are not then used normally in analysis - can easily make repository cluttered/busy",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#seeds",
    "href": "pages/reflections.html#seeds",
    "title": "5  Reflections from reproductions",
    "section": "5.6 Seeds",
    "text": "5.6 Seeds\n\n\n\n\n\n\nUse seeds to control stochasticity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n✅\n✅\n\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The lack of seeds wasn’t actually a barrier to the reproduction though due to the replication number. I later add seeds so my results could be reproduced, and found that the ease of setting seeds with salabim was a greater facilitator to the work. I only had to change one or two lines of code to then get consistent results between runs (unlike other simulation software like SimPy where you have to consider the use of seeds by different sampling functions). Moreover, by default, salabim would have set a seed (although overridden by original authors to enable them to run replications).\nHuang et al. (2019): Not met. It would have been beneficial to include seeds, as there was a fair amount of variability, so with seeds I could then I could be sure that my results do not differ from the original simply due to randomness.\nLim et al. (2020): Not met. The results obtained looked very similar to the original article, with minimal differences that I felt to be within the expected variation from the model stochasticity. However, if seeds had been present, we would have been able to say with certainty. I did not feel I needed to add seeds during the reproduction to get the same results.\nKim et al. (2021): Fully met. Included a seed, although I don’t get identical results as I had to reduce number of people in simulation.\nAnagnostou et al. (2022): Fully met. The authors included a random seed so the results I got were identical to the original (so no need for any subjectivity in deciding whether its similar enough, as I could perfectly reproduce).\nJohnson et al. (2021): \nHernandez et al. (2015): Fully met. This ensured consistent results between runs of the script, which was really helpful.\nWood et al. (2021): Fully met. Sets seed based on replication number.\nReflections: \n\nDepending on your model and the outputs/type of output you are looking at, the lack of seeds can have varying impacts on the appearance of your results, and can make the subjective judgement of whether results are consistent harder (if discrepancies could be attributed to not having consistent seeds or not).\nIt can be really quite simple to include seeds.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#code-to-produce-article-results",
    "href": "pages/reflections.html#code-to-produce-article-results",
    "title": "5  Reflections from reproductions",
    "section": "5.7 Code to produce article results",
    "text": "5.7 Code to produce article results\n\n\n\n\n\n\nProvide code to process results into tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\nN/A\n🟡\n❌\nN/A\n\n❌\n✅\n\n\n\n\nShoaib and Ramamohan (2021): Not met.\nHuang et al. (2019): Not applicable. No tables in scope.\nLim et al. (2020): Partially met. It outputs the results in a similar structure to the paper (like a section of a table). However, it doesn’t have the full code to produce a table outright, for any of the tables, so additional processing still required.\nKim et al. (2021): Not met. Had to write code to generate tables, which included correctly implementing calculation of excess e.g. deaths, scaling to population size, and identify which columns provide the operation outcomes.\nAnagnostou et al. (2022): Not applicable. No tables in scope.\nJohnson et al. (2021): \nHernandez et al. (2015): Not met.\nWood et al. (2021): Fully met.\nReflections: \n\nIt can take a bit of time to do this processing, so very handy for it to be provided.\nCommon issue.\n\n\n\n\n\n\n\n\n\n\nProvide code to process results into figures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\n❌\n\n🟡\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met.\nHuang et al. (2019): Not met. Had to write code from scratch. For one of the figures, it would have been handy if informed that plot was produced by a simmer function (as didn’t initially realise this). It also took a bit of time for me to work out how to transform the figure axes as this was not mentioned in the paper (and no code was provided for these). It was also unclear and a bit tricky to work out how to standardise the density in the figures (since it is only described in the text and no formula/calculations are provided there or in the code). \nLim et al. (2020), Kim et al. (2021) and Anagnostou et al. (2022): Not met. However, the simplicity and repetition of the figures was handy.\nJohnson et al. (2021): \nHernandez et al. (2015): Partially met. Provides a few example ggplots, but these are not all the plots, nor exactly matching article, nor including any of the pre-processing required before the plots, and so could only serve as a starting point (though that was still really handy).\nWood et al. (2021): Fully met. Figures match article, with one minor exception that I had to add smoothing to the lines on one of the figures.\nReflections: \n\nIt can take a bit of time to do this processing, particularly if the figure involves any transformations (and less so if the figure is simple), so very handy for it to be provided.\nCommon issue.\n\n\n\n\n\n\n\n\n\n\nProvide code to calculate in-text results\n\n\n\n\n\nBy “in-text results”, I am referred to results that are mentioned in the text but not included in/cannot be deduced from any of the tables or figures.\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\nN/A\n❌\nN/A\nN/A\nN/A\nN/A\n\n\n\nShoaib and Ramamohan (2021), Huang et al. (2019), Kim et al. (2021): Not met.\nLim et al. (2020), Anagnostou et al. (2022), Johnson et al. (2021), Hernandez et al. (2015), Wood et al. (2021): Not applicable (no in-text results).\nReflections:\n\nProvide code to calculate in-text results\nCommon issue",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#documentation",
    "href": "pages/reflections.html#documentation",
    "title": "5  Reflections from reproductions",
    "section": "5.8 Documentation",
    "text": "5.8 Documentation\n\n\n\n\n\n\nInclude instructions on how to run the model/script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n🟡\n✅\n\n❌\n❌\n\n\n\nShoaib and Ramamohan (2021): Not met. No instructions, although is just a single script that you run.\nHuang et al. (2019): Not met. Not provided in runnable form but, regardless, no instructions for running it as it is provided (as a web application - i.e. no info on how to get that running).\nLim et al. (2020): Not met. No instructions, although is just a single script that you run.\nKim et al. (2021): Partially met. README tells you which folder has the scripts you need, although nothing further. Although all you need to do is run them.\nAnagnostou et al. (2022): Fully met. Clear README with instructions on how to run the model was really helpful.\nJohnson et al. (2021): \nHernandez et al. (2015): Not met.\nWood et al. (2021): Not met. No instructions, although it was fairly self explanatory (single script master.R to run, then processing scripts named after items in article e.g. fig7.R).\nReflections: \n\nEven if as simple as running a script, include instructions on how to do so\nCommon issue",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#other",
    "href": "pages/reflections.html#other",
    "title": "5  Reflections from reproductions",
    "section": "5.9 Other",
    "text": "5.9 Other\n\nGrid lines. Include tick marks/grid lines on figures, so it is easier to read across and judge whether a result is above or below a certain Y value.\nData dictionaries. Anagnostou et al. (2022): Included data dictionary for input parameters. Although I didn’t need this, this would have been great if I needed to change the input parameters at all.\nUnsupported versions. Hernandez et al. (2015): Due to the age of the work:\n\nSome packages were no longer available on Conda and had to be installed from PyPI\nThe version of python was no longer supported which meant:\n\nNot supported by Jupyter Lab and Jupyter Notebook (so no .ipynb files, or Jupyter Lab on Docker)\nNot supported by VSCode (so had to use “tricks” to run it, involving using a pre-release version of Python on VSCode)\nHad to create Docker image from scratch (i.e. couldn’t start from e.g. miniconda3)\n\n\nHowever, this is a slightly unavoidable problem unless you continue to maintain your code (which is ideal but not always feasible in a research environment). Realistically, if reusing this code for a new purpose, you would upgrade it to supported versions.\nOriginal results files. Hernandez et al. (2015): Included some original results files, which was invaluable in identifying some of the parameters in the code that needed to be fixed.\nClasses. Hernandez et al. (2015): Structured code into classes, which was nice to work with/amend.\nReflections:\n\nInclude grid lines\nInclude data dictionaries\nIf feasible, maintain code to avoid dependency on unsupported old versions of packages and languages\nIdeally, include copies of your results, so can (a) compare against, (b) use to run analysis on, and (c) use in troubleshooting",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#references",
    "href": "pages/reflections.html#references",
    "title": "5  Reflections from reproductions",
    "section": "5.10 References",
    "text": "5.10 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html",
    "href": "pages/repo_evaluation.html",
    "title": "6  Evaluation of the repository",
    "section": "",
    "text": "6.1 Summary\nUnique badge criteria:\nBadges:\nEssential components of STARS framework:\nOptional components of STARS framework:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#summary",
    "href": "pages/repo_evaluation.html#summary",
    "title": "6  Evaluation of the repository",
    "section": "",
    "text": "Reflections\n\n\n\n\n\nFour studies met 25% of criteria, and ranged from 12.5% to 100% reproduced.\nTwo studies met 33% of criteria, and these were 80% and 100% reproduced.\nThe remaining two studies were fully reproduced and met 41.7% and 83.3% of criteria.\nHowever, I think it is more meaningful to actually look at what criteria were and were not met.\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nNot certain how meaningful these numbers are, as we have imbalanced numbers of different types of badge, and meeting certain popular criteria will weight what is met v.s. not.\nFeel that looking at the criteria met is a bit more meaningful? And then specific examples of how that translates into badges - e.g.\n\nNone meeting ACM “Artifacts Evaluated - Functional” as requires xyz and these are commonly not met.\nFor several, they meet three badges, but those three badges have one criteria: reproducing results.\n\nAlso, it’s important to remember here that the criteria used for these were based on what could be found about each badge online, but we likely differences in our procedure (e.g. allowed troubleshooting for execution and reproduction, not under tight time pressure to complete). Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals (and, in fact, know likely not for many).\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nSimilar to journal criteria (unsurprisingly, as looking at similar things) - most studies meet very few and have wide range of reproduction success, from 12.5% to 100%. Three met more, and these were 80% to 100% reproduced.\nI think, if we were to draw anything from this, it would be to reflect on exactly what criteria were and were not met, and why/how that impacted reproduction, in any way (either success or time).\nNote: Just considers those fully met, in plot\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nThis highlights how Huang meets the most criteria, but is only partially reproduced - but I think it is most interesting to consider why this is.\nNote: Just considers those fully met, in plot",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#journal-badges",
    "href": "pages/repo_evaluation.html#journal-badges",
    "title": "6  Evaluation of the repository",
    "section": "6.2 Journal badges",
    "text": "6.2 Journal badges\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\n\nIn this section and below, the criteria for each study are marked as either being fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A).\nUnique criteria:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nCriteria related to how artefacts are shared\n\n\n\n\n\n\n\n\n\n\n\nStored in a permanent archive that is publicly and openly accessible\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nHas a persistent identifier\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nIncludes an open licence\n❌\n✅\n❌\n✅\n✅\n✅\n❌\n❌\n\n\n\nCriteria related to what artefacts are shared\n\n\n\n\n\n\n\n\n\n\n\nArtefacts are relevant to and contribute to the article’s results\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nComplete set of materials shared (as would be needed to fully reproduce article)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\nCriteria related to the structure and documentation of the artefacts\n\n\n\n\n\n\n\n\n\n\n\nArtefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n❌\n❌\n❌\n✅\n✅\n❌\n✅\n❌\n\n\n\nArtefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nArtefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nArtefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nCriteria related to running and reproducing results\n\n\n\n\n\n\n\n\n\n\n\nScripts can be successfully executed\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nIndependent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nReproduced within approximately one hour (excluding compute time)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nStored in a permanent archive that is publicly and openly accessible: Fulfillment doesn’t impact reproduction as I was able to get everything needed from the remote code repository (GitHub or GitLab). However, if these had been deleted from GitHub, it would have become invaluable.\nHas a persistent identifier: Fulfillment doesn’t impact reproduction.\nIncludes an open licence: This had a big impact on our ability to complete reproductions, as we had to ask authors to add an open licence to their work, to enable us to use it. Gladly, all authors we contacted kindly add these on request. However, it’s worth noting that this was a relatively common issue, and one of the most important, since it completely prevents reuse if excluded.\nArtefacts are relevant to and contribute to the article’s results: All met (if not met, this would be a massive hindrance).\nComplete set of materials shared (as would be needed to fully reproduce article): This had a really big impact on the reproduction. The main reason for longer times in reproduction was (a) code for scenarios not provided, and (b) code to process results into figures and tables not provided.\nArtefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community): In cases where this was not met, this was often related to hard-coding of parameters, or set up of code in a way that - alike hard-coding - made it much harder to reuse the model (in this context, to reuse them between different scenarios) - or quite a busy/cluttered repository which was confusing to navigate.\nArtefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions): Only three studies had any documentation - READMEs for Kim et al. (2021), Anagnostou et al. (2022) and Johnson et al. (2021) - but this criteria also required “package versions” which were uncommon. However, focusing on the READMEs, in each of these cases it was great to have these, guiding on how to run the scripts, or on what each folder/file in the repository is.\nArtefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose): In Anagnostou et al. (2022), they include a file CHARM_INFO.md alongside their README which walks through the input parameters for the model. I didn’t need to change any of these for the reproduction, but would imagine this is to be very helpful if someone were to reuse the model.\nArtefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript: Whilst this was true for Anagnostou et al. (2022), it should be noted that this was a very simple example, just requiring to run one script which quickly reproduces everything! I had been a bit uncertain on it, since the README doesn’t explicitly say how to make the figure, but it does provide instructions that lead you to regenerate the exact model results from the paper, and so I feel that it does provide instructions to reproduce results sufficiently (although would be more complete to include instructions for figure too - so if it weren’t a yes/no decision for badges, I would’ve said this was partially met). Ideally, studies would clearly outline how to reproduce results in full.\nScripts can be successfully executed: This is true, though I did allow troubleshooting, which sometimes took a long while. Hence, the importance of e.g. environments and scripts being provided in a runnable format (both covered on the reflections page), since these are the hurdles to successfully executing scripts.\nIndependent party regenerated results using the authors research artefacts: On the reproduction page, I reflected (where possible) on what I thought the primary reasons were, for cases where I didn’t manage to reproduce results despite troubleshooting.\nReproduced within approximately one hour (excluding compute time): In this study, this was pretty much impossible, since I followed a protocol of first setting up, reading the article, and so on. It is worth noting however that there were two studies that were quite quick to run, which I reflect about on the reproduction page.\n\n\n\nBadges:\nThe badges are grouped into three categories:\n\n“Open objects” badges: These badges relate to research artefacts being made openly available.\n“Object review” badges: These badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n“Reproduced” badges: These badges relate to an independent party regenerating the reuslts of the article using the author objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\n“Open objects” badges\n\n\n\n\n\n\n\n\n\n\n\nNISO “Open Research Objects (ORO)”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier• Includes an open licence\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nNISO “Open Research Objects - All (ORO-A)”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier• Includes an open licence• Complete set of materials shared (as would be needed to fully reproduce article)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nACM “Artifacts Available”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nCOS “Open Code”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier• Includes an open licence• Complete set of materials shared (as would be needed to fully reproduce article)• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nIEEE “Code Available”• Complete set of materials shared (as would be needed to fully reproduce article)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n“Object review” badges\n\n\n\n\n\n\n\n\n\n\n\nACM “Artifacts Evaluated - Functional”• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)• Artefacts are relevant to and contribute to the article’s results• Complete set of materials shared (as would be needed to fully reproduce article)• Scripts can be successfully executed\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nACM “Artifacts Evaluated - Reusable”• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)• Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)• Artefacts are relevant to and contribute to the article’s results• Complete set of materials shared (as would be needed to fully reproduce article)• Scripts can be successfully executed• Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nIEEE “Code Reviewed”• Complete set of materials shared (as would be needed to fully reproduce article)• Scripts can be successfully executed\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n“Reproduced” badges\n\n\n\n\n\n\n\n\n\n\n\nNISO “Results Reproduced (ROR-R)”• Independent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nACM “Results Reproduced”• Independent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nIEEE “Code Reproducible”• Independent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nPsychological Science “Computational Reproducibility”• Independent party regenerated results using the authors research artefacts• Reproduced within approximately one hour (excluding compute time)• Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)• Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n\n                                                \n\n\nOnly one study had permanent archive (with persistent identifier), hence one being awarded NISO “Open Research Objects (ORO)” and ACM “Artifacts Available”. This was required by two other badges as well as sufficient documentation and/or complete set of materials. Since the one permanently archived study didn’t meet these criteria, none were awarded the two badges: NISO “Open Research Objects - All (ORO-A)” or COS “Open Code”.\nA complete set of materials was required by IEEE “Code Available” and IEEE “Code Reviewed” - but this was only met by one study, as studies commonly did not include code for scenarios or creation of figures and tables. It was also required by ACM “Artifacts Evaluated - Functional and Reusable” badges, but since that one study didn’t meet their documentation requirements, none were awarded those badges.\nThree badges had one criteria: reproduction of results - and hence, several studies received these (NISO “Results Reproduced (ROR-R)”, ACM “Results Reproduced”, IEEE “Code Reproducible”). However, it’s worth noting that we allowed troubleshooting (since that was how we approached the reproductions), and that some of these studies might not have received these badges, if they have any requirements that exclude troubleshooting (which is likely).\nThe final badge Psychological Science “Computational Reproducibility” had several criteria, but of importance, one of those was to complete the reproduction within an hour. This was somewhat impossible in our procedures, since we read the article and set-up etc beforehand, and so none were awarded this badge.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#stars-framework",
    "href": "pages/repo_evaluation.html#stars-framework",
    "title": "6  Evaluation of the repository",
    "section": "6.3 STARS framework",
    "text": "6.3 STARS framework\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nEssential components\n\n\n\n\n\n\n\n\n\n\n\nOpen licenceFree and open-source software (FOSS) licence (e.g. MIT, GNU Public Licence (GPL))\n❌\n✅\n❌\n✅\n✅\n✅\n❌\n❌\n\n\n\nDependency managementSpecify software libraries, version numbers and sources (e.g. dependency management tools like virtualenv, conda, poetry)\n❌\n❌\n❌\n🟡\n✅\n🟡\n❌\n❌\n\n\n\nFOSS modelCoded in FOSS language (e.g. R, Julia, Python)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nMinimum documentationMinimal instructions (e.g. in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments\n❌\n❌\n❌\n✅\n✅\n🟡\n❌\n❌\n\n\n\nORCIDORCID for each study author\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nCitation informationInstructions on how to cite the research artefact (e.g. CITATION.cff file)\n❌\n❌\n❌\n❌\n✅\n✅\n❌\n❌\n\n\n\nRemote code repositoryCode available in a remote code repository (e.g. GitHub, GitLab, BitBucket)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nOpen science archiveCode stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g. Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net))\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nOptional components\n\n\n\n\n\n\n\n\n\n\n\nEnhanced documentationOpen and high quality documentation on how the model is implemented and works (e.g. via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:• Plain english summary of project and model• Clarifying licence• Citation instructions• Contribution instructions• Model installation instructions• Structured code walk through of model• Documentation of modelling cycle using TRACE• Annotated simulation reporting guidelines• Clear description of model validation including its intended purpose\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nDocumentation hostingHost documentation (e.g. with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nOnline coding environmentProvide an online environment where users can run and change code (e.g. BinderHub, Google Colaboratory, Deepnote)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nModel interfaceProvide web application interface to the model so it is accessible to less technical simulation users\n❌\n✅\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nWeb app hostingHost web app online (e.g. Streamlit Community Cloud, ShinyApps hosting)\n❌\n✅\n❌\n❌\n🟡\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n\n                                                \n\n\nThese topics were covered in the badge criteria reflections: open licence, minimum documentation, and open science archive. \nDependency management: This was pretty uncommon, and often took some troubleshooting at the start, to figure out which packages were needed, and certain versions.\nFOSS model: All met as requirement of our reproduction.\nORCID and citation information: Doesn’t impact reproduction in this case - but we do go to these from having found an article. In all cases, I emailed the authors, which requires finding contact information (generally via paper, sometimes from googling them to find new emails).\nRemote code repository: All met, most common way to share code.\nEnhanced documentation: Only three studies had any documentation, and neither met these extensive requirements. I anticipate - if any had met this - it would’ve made the reproduction very quick and easy!\nDocumentation hosting: Not applicable, given only basic documentation.\nOnline coding environment: None provided. I always intended to run on my own machine, so this might not have had much bearing in my case if provided, but would moreso for people who perhaps didn’t have Python or R installed, and hopefully would have bypassed environment troubleshooting issues.\nModel interface: Two studies had applications, although in both cases, these weren’t “outcomes” in scope of reproduction, nor did they produce them.\nWeb app hosting: This was quite important. Both apps had been hosted, but one was hosted with a site that is no longer operational. In both cases, the app wasn’t in “scope” although I did still view it and look into it for one as it was hosted and so could very easily - but for the other, I didn’t view it, as I didn’t go through the steps of running it locally, since it wasn’t the focus.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#timings",
    "href": "pages/repo_evaluation.html#timings",
    "title": "6  Evaluation of the repository",
    "section": "6.4 Timings",
    "text": "6.4 Timings\n\nShoaib and Ramamohan (2021) - 30m\nHuang et al. (2019) - 17m\nLim et al. (2020) - 18m\nKim et al. (2021) - 18m\nAnagnostou et al. (2022) - 19m\nJohnson et al. (2021) - 20m\nHernandez et al. (2015) - 13m\nWood et al. (2021) - 14m\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nNo particular comments, don’t think we learn much from the timings here.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#badge-sources",
    "href": "pages/repo_evaluation.html#badge-sources",
    "title": "6  Evaluation of the repository",
    "section": "6.5 Badge sources",
    "text": "6.5 Badge sources\nNational Information Standards Organisation (NISO) (NISO Reproducibility Badging and Definitions Working Group (2021))\n\n“Open Research Objects (ORO)”\n“Open Research Objects - All (ORO-A)”\n“Results Reproduced (ROR-R)”\n\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\n\n“Artifacts Available”\n“Artifacts Evaluated - Functional”\n“Artifacts Evaluated - Resuable”\n“Results Reproduced”\n\nCenter for Open Science (COS) (Blohowiak et al. (2023))\n\n“Open Code”\n\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\n\n“Code Available”\n“Code Reviewed”\n“Code Reproducible”\n\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\n“Computational Reproducibility”",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#references",
    "href": "pages/repo_evaluation.html#references",
    "title": "6  Evaluation of the repository",
    "section": "6.6 References",
    "text": "6.6 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nAssociation for Computing Machinery (ACM). 2020. “Artifact Review and Badging Version 1.1.” ACM. https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nAssociation for Psychological Science (APS). 2023. “Psychological Science Submission Guidelines.” APS. https://www.psychologicalscience.org/publications/psychological_science/ps-submissions.\n\n\nBlohowiak, Ben B., Johanna Cohoon, Lee de-Wit, Eric Eich, Frank J. Farach, Fred Hasselman, Alex O. Holcombe, Macartan Humphreys, Melissa Lewis, and Brian A. Nosek. 2023. “Badges to Acknowledge Open Practices.” https://osf.io/tvyxz/.\n\n\nHardwicke, Tom E., and Simine Vazire. 2023. “Transparency Is Now the Default at Psychological Science.” Psychological Science 0 (0). https://doi.org/https://doi.org/10.1177/09567976231221573.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nInstitute of Electrical and Electronics Engineers (IEEE). n.d. “About Content in IEEE Xplore.” IEEE Explore. Accessed May 20, 2024. https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-content.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.\n\n\nNISO Reproducibility Badging and Definitions Working Group. 2021. “Reproducibility Badging and Definitions.” https://doi.org/10.3789/niso-rp-31-2021.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html",
    "href": "pages/paper_evaluation.html",
    "title": "7  Evaluation of the article",
    "section": "",
    "text": "7.1 Summary\nSTRESS-DES:\nDES checklist derived from ISPOR-SDM:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#summary",
    "href": "pages/paper_evaluation.html#summary",
    "title": "7  Evaluation of the article",
    "section": "",
    "text": "Reflections\n\n\n\n\n\nOf the applicable criteria, all studies fully met at least 60% (and many others still partially met).\nI don’t think we learn much in relation to items reproduced, and that it is more interesting to consider which criteria were met and whether this aided the reproduction or not.\nNote. This plots the proportion of applicable criteria that were fully met.\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nThe proportion of applicable criteria met was lower for this checklist.\nHowever, I again think it’s most relevant to consider which criteria were met, rather than draw conclusions from this chart.\nNote. This plots the proportion of applicable criteria that were fully met.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#stress-des",
    "href": "pages/paper_evaluation.html#stress-des",
    "title": "7  Evaluation of the article",
    "section": "7.2 STRESS-DES",
    "text": "7.2 STRESS-DES\n\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\nIn this section and below, the criteria for each study are marked as either being fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nObjectives\n\n\n\n\n\n\n\n\n\n\n\n1.1 Purpose of the modelExplain the background and objectives for the model\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n1.2 Model outputsDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n🟡\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n1.3 Experimentation aimsIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis – Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments – Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation – (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\nLogic\n\n\n\n\n\n\n\n\n\n\n\n2.1 Base model overview diagramDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.2 Base model logicGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.3 Scenario logicGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\n2.4 AlgorithmsProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e. scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n✅\n🟡\n✅\n🟡\n✅\n✅\n✅\n✅\n\n\n\n2.5.1 Components - entitiesGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.5.2 Components - activitiesDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.5.3 Components - resourcesList all the resources included within the model and which activities make use of them.\n✅\n✅\nN/A\nN/A\n✅\nN/A\n✅\n✅\n\n\n\n2.5.4 Components - queuesGive details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n✅\n✅\nN/A\nN/A\n✅\nN/A\n✅\n✅\n\n\n\n2.5.5 Components - entry/exit pointsGive details of the model boundaries i.e. all arrival and exit points of entities. Detail the arrival mechanism (e.g. ‘thinning’ to mimic a non-homogenous Poisson process or balking)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nData\n\n\n\n\n\n\n\n\n\n\n\n3.1 Data sourcesList and detail all data sources. Sources may include:• Interviews with stakeholders,• Samples of routinely collected data,• Prospectively collected samples for the purpose of the simulation study,• Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n3.2 Pre-processingProvide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers.\n✅\nN/A\nN/A\n✅\nN/A\nN/A\nN/A\n✅\n\n\n\n3.3 Input parametersList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:• Base case data• Data use in experimentation, where different from the base case.• Where optimisation or design of experiments has been used, state the range of values that parameters can take.• Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n🟡\n🟡\n✅\n🟡\n✅\n✅\n✅\n✅\n\n\n\n3.4 AssumptionsWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n✅\n❌\n✅\n✅\n❌\n✅\n✅\n✅\n\n\n\nExperimentation\n\n\n\n\n\n\n\n\n\n\n\n4.1 InitialisationReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n🟡\n❌\n❌\n🟡\n❌\n✅\n❌\n✅\n\n\n\n4.2 Run lengthDetail the run length of the simulation model and time units.\n✅\n✅\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\n4.3 Estimation approachState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n🟡\n🟡\n✅\n✅\n✅\nN/A\n✅\n✅\n\n\n\nImplementation\n\n\n\n\n\n\n\n\n\n\n\n5.1 Software or programming languageState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g. Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n\n\n\n5.2 Random samplingState the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n❌\n🟡\n❌\n❌\n❌\nN/A\n❌\n✅\n\n\n\n5.3 Model executionState the event processing mechanism used e.g. three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n🟡\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n5.4 System specificationState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n✅\n❌\n🟡\n🟡\n❌\n❌\n🟡\n🟡\n\n\n\nCode access\n\n\n\n\n\n\n\n\n\n\n\n6.1 Computer model sharing statementDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nI find this chart a really helpful visualisation, to see what is commonly being met, things that are often not applicable, and things often not met.\nBelow, I’ve reflected on the impact of each criteria being fulfilled, on the reproduction. I’ve also identified where I feel I’ve been a bit more lax in my evaluation. \n1.1 Purpose of the modelExplain the background and objectives for the model.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n1.2 Model outputsDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n\nOne paper was rated as “partially met” as many of the outcomes used were not defined.\nFor those marked fully met, this was mainly based on them mentioning or being pretty clear about what the outcomes were, but wasn’t super strict - ie. didn’t require equations, or to specify how and when they were calculated.\nWhilst this is important, the raw/basic outcomes themselves were generally pretty straightforward, and issues with outputs more-so related to:\n\nWhich output to use from the results table (if unclear names, or similar names)\nHow to apply any required transformations\n\n\n1.3 Experimentation aimsIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis – Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments – Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation – (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n\nIn all seven papers with scenarios, these were described. Didn’t necessarily provide a “name” for each scenario, nor a “rationale” for the choice of scenarios, but did describe them.\nAlthough the description of the scenario can feel clear from the paper, when the code was not provided, it could be quick tricky and time-consuming to work out how to appropriately change the code, in order to implement the scenario.\n\n2.1 Base model overview diagramDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n\nAll papers included a diagram, which was great.\n\n2.2 Base model logicGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n2.3 Scenario logicGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n\nOverlap with 1.3 (implicit in describing 1.3, that would describe this).\n\n2.4 AlgorithmsProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e. scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n\nThose partially met are for describing some but not all of the algorithms. However, it is worth noting that it could be hard to actually ensure all relevant algorithms were described, if I hadn’t identified them in the more complex models.\n\n2.5.1 Components - entitiesGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n\nPretty basic requirement that all meet (unsurprisingly, as implicit in description of model / logic)\n\n2.5.2 Components - activitiesDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n\nPretty basic requirement that all meet (unsurprisingly, as implicit in description of model / logic)\nDidn’t necessarily require explicit description of routing.\n\n2.5.3 Components - resourcesList all the resources included within the model and which activities make use of them.\n\nGenerally seem to be mentioned when included, particularly as often form part of output (e.g. resource utilisation)\nWhen not mentioned (and based on known structure of model), assume not relevant.\n\n2.5.4 Components - queuesGive details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n\nAs for 2.5.3\n\n2.5.5 Components - entry/exit pointsGive details of the model boundaries i.e. all arrival and exit points of entities. Detail the arrival mechanism (e.g. ‘thinning’ to mimic a non-homogenous Poisson process or balking).\n\nGenerally fairly implicit\nOverlap with 2.4 algorithms, and hence didn’t necessarily mark this down if not full detail of arrival mechanism.\n\n3.1 Data sourcesList and detail all data sources. Sources may include:• Interviews with stakeholders,• Samples of routinely collected data,• Prospectively collected samples for the purpose of the simulation study,• Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n\nAll meet.\n\n3.2 Pre-processingProvide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers.\n\nOften had to assume that none occurred if none described.\n\n3.3 Input parametersList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:• Base case data• Data use in experimentation, where different from the base case.• Where optimisation or design of experiments has been used, state the range of values that parameters can take.• Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n\nThis was very important as it allows to check that the code parameters were correct as - in several cases - the provided code did not include the base case parameters as described. When missing from the paper, it was not possible to check them.\n\n3.4 AssumptionsWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n\nAlthough not relevant for reproduction, would be very relevant for reuse and validity\n\n4.1 InitialisationReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n\nOften not reported, and would be handy as it’s a pretty basic/fundamental aspect to the model that would help readers to have better understanding of how the model is working.\n\n4.2 Run lengthDetail the run length of the simulation model and time units.\n\nImportant to mention for same reasons as 3.3\n\n4.3 Estimation approachState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n\nThe description of this criteria is quite confusing - generally just focussed on identifying whether it was multiple replications or a big run, and then if the numbers used were justified.\nPartially met cases are those with replications that are not justified.\n\n\n5.1 Software or programming languageState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g. Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n\nWill often mentioned the programming language, but not operating system or version numbers\nThis was pretty handy, when versions were given, for cases where no versions were given in the repository itself - although ideally, versions could just simply be there.\n\n5.2 Random samplingState the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n\nFrequently not described, despite some of the studies having implemented and used seeds.\n\n\n5.3 Model executionState the event processing mechanism used e.g. three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n\nOften not mentioned event processing machnism\nThis is a very long list of requirements in one category, and I found a bit difficult to evaluate, e.g. to check against all criteria\n\n5.4 System specificationState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n\nModel run time was really important to mention, but often not given. Particularly important for the models with longer run times, to know what to expect - including in light of reuse, and perhaps short times being necessary in certain contexts - or here, when troubleshooting, to know I might need to try lower numbers first while getting it working\nOverlaps with 5.3 parallel and 5.1 operating system.\n\n6.1 Computer model sharing statementDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n\nThis is inevitable/selection bias, given we chose papers that had links to code.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#des-checklist-derived-from-ispor-sdm",
    "href": "pages/paper_evaluation.html#des-checklist-derived-from-ispor-sdm",
    "title": "7  Evaluation of the article",
    "section": "7.3 DES checklist derived from ISPOR-SDM",
    "text": "7.3 DES checklist derived from ISPOR-SDM\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nModel conceptualisation\n\n\n\n\n\n\n\n\n\n\n\n1 Is the focused health-related decision problem clarified?…the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2 Is the modeled healthcare setting/health condition clarified?…the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n3 Is the model structure described?…the model’s conceptual structure was described in the form of either graphical or text presentation.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n4 Is the time horizon given?…the time period covered by the simulation was reported.\n✅\n✅\n✅\n✅\n❌\n✅\n✅\n✅\n\n\n\n5 Are all simulated strategies/scenarios specified?…the comparators under test were described in terms of their components, corresponding variations, etc\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\n6 Is the target population described?…the entities simulated and their main attributes were characterized.\n✅\n❌\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\nParamaterisation and uncertainty assessment\n\n\n\n\n\n\n\n\n\n\n\n7 Are data sources informing parameter estimations provided?…the sources of all data used to inform model inputs were reported.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n8 Are the parameters used to populate model frameworks specified?…all relevant parameters fed into model frameworks were disclosed.\n🟡\n🟡\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n9 Are model uncertainties discussed?…the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n🟡\n❌\n❌\n❌\n✅\nN/A\n✅\n✅\n\n\n\n10 Are sensitivity analyses performed and reported?…the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters’ plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n✅\n❌\n✅\n❌\nN/A\n✅\n❌\n✅\n\n\n\nValidation\n\n\n\n\n\n\n\n\n\n\n\n11 Is face validity evaluated and reported?…it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n❌\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n\n\n\n12 Is cross validation performed and reported…comparison across similar modeling studies which deal with the same decision problem was undertaken.\nN/A\n❌\n❌\n✅\n❌\n✅\n❌\n❌\n\n\n\n13 Is external validation performed and reported?…the modeler(s) examined how well the model’s results match the empirical data of an actual event modeled.\nN/A\nN/A\nN/A\n✅\n❌\n✅\n❌\n❌\n\n\n\n14 Is predictive validation performed or attempted? …the modeler(s) examined the consistency of a model’s predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\nN/A\nN/A\nN/A\nN/A\nN/A\n❌\nN/A\nN/A\n\n\n\nGeneralisability and stakeholder involvement\n\n\n\n\n\n\n\n\n\n\n\n15 Is the model generalizability issue discussed?…the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n✅\n✅\n✅\n❌\n🟡\n✅\n❌\n✅\n\n\n\n16 Are decision makers or other stakeholders involved in modeling?…the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\n17 Is the source of funding stated?…the sponsorship of the study was indicated.\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\n18 Are model limitations discussed?…limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\n✅\n🟡\n✅\n✅\n🟡\n✅\n✅\n✅",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#timings",
    "href": "pages/paper_evaluation.html#timings",
    "title": "7  Evaluation of the article",
    "section": "7.4 Timings",
    "text": "7.4 Timings\n\nShoaib and Ramamohan (2021) - 1h 56m\nHuang et al. (2019) - 1h 28m\nLim et al. (2020) - 1h 12m\nKim et al. (2021) - 2h 12m\nAnagnostou et al. (2022) - 53m\nJohnson et al. (2021) - 1h 32m\nHernandez et al. (2015) - 1h 11m\nWood et al. (2021) - 1h 24m",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#use-of-reporting-guidelines",
    "href": "pages/paper_evaluation.html#use-of-reporting-guidelines",
    "title": "7  Evaluation of the article",
    "section": "7.5 Use of reporting guidelines",
    "text": "7.5 Use of reporting guidelines\n\n\n\nRegarding whether each study mentioned using reporting guidelines:\n\nShoaib and Ramamohan (2021) - ❌\nHuang et al. (2019) - ❌\nLim et al. (2020) - ❌\nKim et al. (2021) - ❌\nAnagnostou et al. (2022) - ❌\nJohnson et al. (2021) - ✅ Consolidated Health Economic Evaluation Reporting Standards (CHEERS) - Husereau et al. (2013)\nHernandez et al. (2015) - ❌\nWood et al. (2021) - ✅ STRESS-DES: Strengthening The Reporting of Empirical Simulation Studies (Discrete-Event Simulation) - Monks et al. (2019)",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#references",
    "href": "pages/paper_evaluation.html#references",
    "title": "7  Evaluation of the article",
    "section": "7.6 References",
    "text": "7.6 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nHusereau, Don, Michael Drummond, Stavros Petrou, Chris Carswell, David Moher, Dan Greenberg, Federico Augustovski, Andrew H. Briggs, Josephine Mauskopf, and Elizabeth Loder. 2013. “Consolidated Health Economic Evaluation Reporting Standards (CHEERS) Statement.” Value in Health 16 (2): e1–5. https://doi.org/10.1016/j.jval.2013.02.010.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/framework.html",
    "href": "pages/framework.html",
    "title": "8  Modifying the framework",
    "section": "",
    "text": "8.1 Contents of the framework",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/framework.html#contents-of-the-framework",
    "href": "pages/framework.html#contents-of-the-framework",
    "title": "8  Modifying the framework",
    "section": "",
    "text": "Section from reflections\nConsideration for framework\n\n\n\n\nList required packages\nThe difference between listing all packages, and listing some that then have other dependencies. The various options there are for listing packages, and the downside of just assuming its obvious from imports. The options for guiding dependency management (e.g. providing scripts that download packages, and then environment management tools)\n\n\nProvide version for Python/R and for packages\nImportant to do so for both (not just packages). Challenge of implementing in R. Ease of implementing in Python. Importance of listing in repository itself. Options for doing this in R and Python. Suggestions. Need to explore more with R. Is it a matter of providing renv but suggesting someone could try with latest versions in first instance? As not realistic to guarantee maintenance of research projects. But know at least when it worked - as being clear, it won’t always, even in R\n\n\nModel in a “runnable” format\nThis was an unusual case, but I think could merge more under the example of providing code that aligns with the paper, as in that case, it was the code for a web app when the paper wasn’t about that\n\n\nModel designed to run programmatically, and don’t hard code parameters you want to change.\nConsider whether framework will given guidance on model structure. If so, it should be suggesting that (A) parameters are seperate from model and (B) run scenarios by changing elsewhere (and not by directly modifying the model code). Why do this? Because its simpler to run multiple versions of model with same script, and reduces likelihood of missing errors of inputing wrong parameters\n\n\nAvoid large amounts of code duplciation\nThis would also be a code structure thing (does it fit in framework?). Good to do in general - fairly standard coding practice. Here as it makes code more readable, makes it easier when want to change all scenarios, and reduces likelihood of introducing mistakes (which did see)\n\n\nSufficient comments in code\nLikewise, code structure. Standard practice like docstrings (which only one or two had). The standard structures that are available for docstrings (e.g. roxygen in R). And then also just general comments in code itself.\n\n\nQuicker models\nWorth considering for framework, as this was practically one of the big things for me when it came to using the models. Think about the options that are available for reducing model run time. And from the start, principles of keeping it simple and small. Things to avoid that make it slow. Alternatives to those things. The value of a quicker model (e.g. in being able to make an app, to rerun it all easily and spot mistakes, to just be easier to work with).\n\n\nState run time\nCould include in minimum documentation\n\n\nState memory usage and alterantives for lower spec machines\nCould include in enhanced documentation? Or minimum? Need to consider that this can seem a daunting thing if not familiar and wouldn’t know how to find this out or what this means\n\n\nProvide code for all scenarios\n\n\n\nInclude correct parameters in script\nSuggestion of default model parameters matching up to baseline in paper, and explaining that if not, issue becomes that its hard to check if model is all correct without code or paper listing every single parameter\n\n\nProvide all required parameters\n\n\n\nClearly present parameters in paper\nThis would not be relevant for STARS framework, but could feed into STRESS-DES work? How it being in a table was so much easier than it being described in the text, and how it being comprehensive of all parameters that get varied (or clear if those are the only things changed from baseline, and so on)\n\n\nProvide calculations\nAgain, might not quite fit into STARS? But if you are going to be mentioning the “pre-processed” values at all, then its important to include the calculation (ideally in the code, as that is the clearest demonstration of exactly what you did)\n\n\nSaves output to a file\nCode structure type thing. This was really handy, particularly for long run times. In doing this, set up in way that is easy to change name and location of output file, and not hard coded to one thing\n\n\nUnderstandable output tables\nNot sure where/if this might fit. But suggestion is to not provide alternative results for same metrics. And if a table or output might be unclear on what need or how to calculate, then providing some docs or something that supports\n\n\nUsing seeds\nExplain how, how it can be quite simple, and what benefit of this is. Could be in enhanced\n\n\nProvide code to produce (a) tables (b) figures (c) in-text results\nExplaining why this is handy (being able to reproduce gives confidence model is running right and also important in science) (plus useful for self too if need to make changes in review process and so on). Noting importance of not forgetting about “in-text” results\n\n\nInstructions on how to run model\nAlready in minimal documentation, but be aware, even as simple as “run this script and here is an example of how” is helpful\n\n\nGrid lines\nMinor, not sure if fits/to include\n\n\nData dictionaries\nMinor and intersects with good commenting, might not be relevant for all, but also worth considering, if people are uploading data as part of model, what the principles and practices and recommendations are for shairng of data and linking to those (as this is likely part of those, along with other things). If there is overlap, make it clear the synergies (e.g. if talks about archiving data, how could do alongside code, and so on).",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/framework.html#presentation-of-the-framework",
    "href": "pages/framework.html#presentation-of-the-framework",
    "title": "8  Modifying the framework",
    "section": "8.2 Presentation of the framework",
    "text": "8.2 Presentation of the framework\nChecklist table with categories, description and space to complete. But in multiple synced formats (perhaps auto conversion github action between them) - e.g. markdown, latex, docx\nDiagram: Think about any ways could adapt this and if would benefit from that. E.g. gradual reveal with GIF. more icons. alt layout. etc.\nWebsite: Could share like https://joss.readthedocs.io/en/latest/paper.html but would want to make sure we are adding sufficient detail on top of checklist.\nInteractive web app: Could make a web app that serves as a “form” for people to complete and then downloaded their completed framework like there is set up for the PRISMA checklist.",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/framework.html#reflections-from-the-evaluations",
    "href": "pages/framework.html#reflections-from-the-evaluations",
    "title": "8  Modifying the framework",
    "section": "8.3 Reflections from the evaluations",
    "text": "8.3 Reflections from the evaluations\n\nTBC - in progress below…\nAlthough we can comment on how commonly or uncommonly something met a criteria, it is only a small sample (n=6), so when covered in the DES best practice audit, that might be more informative, as baesd on far more studies.\nFrom evaluation of repository:\n\nOpen licence - available on about half, added on request for rest, a very important one as without it you technically cannot reuse someones code\nComplete materials - rarely, so alot of time spent on writing code and figuring out how best to do that\nDependency management - rarely done, and although not an issue in simpler cases, a big impact in more complex\nModel interface - provided for two although not helpful as (1) host no longer operating, and as not focus, didn’t try to run locally, and (2) other app was viewable but didn’t produce the figures and outcomes from paper (was more general, whilst paper delved into scenarios beyond app)\n\nFrom evaluation of paper:\n\nIt often took a long time (up to 2 hours). It would be valuable if the completed checklist were provided along with the paper, like how people often do with equator network reporting guidelines like PRISMA. Consider varying levels detail its possible to complete (yes/no, linking to sections, full detail). Linking to sections is minimum needed to be really useful. Yes/no doesn’t impact time to find information really.\nInput parameters - very handy, as if uncertain in code, use this to check or code\nSome of my evaluations were more light handed (e.g. STRESS 1.2 and 1.3) whilst some were more nit-picky\nRun length - important for knowing when its slow\nScenarios - important to be clear, felt provided in a table was the clearest\n\nReflections on STRESS-DES from having worked on it:\n\n1.2 “how and when calculated” might feel like excess detail\n1.3A providing name for each scenario doesn’t always seem necessary\n2.4 initially found confusing/irrelevant (e.g. “operation of conveyor”)\n3.2 would be helpful to have more examples\n5.1 listing all packages and their versions is never particularly achievable or feasible. More feasible is specifying main package used for model (e.g. simmer, simpy, base R, numpy/pandas) and providing finer details in code (although it was helpful to have version of this package in the paper, when no versions were mentioned in the GitHub)\n5.2 Algorithm used is tricky/uncommon knowledge\n5.3 Event-processing machanism is tricky/uncommon knowledge\nToo many different things in several (could benefit from being clearer, seperated, or into components like 2.5). This was the case for 1.3, 3.3, 4.1, 4.3, 5.1, 5.2, 5.3, 5.4\n\nReflections from comparing what is in STRESS-DES v.s. ISPOR-derived:\n\nScenarios - ISPOR has a simpler description\nModel uncertainities - nice that ISPOR requires this\nISPOR has lots of items that relate to validity and how “good” it is (9 to 18) - whilst STRESS doesn’t touch on this",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/relationship_reproduction_evaluation.html",
    "href": "pages/relationship_reproduction_evaluation.html",
    "title": "9  Relationship between reproduction and evaluation",
    "section": "",
    "text": "TODO: Add this page, to assess whether (and if so, how) there is a relationship between the evaluation (code and/or reporting) and how easy it was / able we were to reproduce\nTODO: I’m also interested actually particularly in relationship between “reflections” being met, and reproduction.\nthis could be more restructured into (a) reflections from evaluations, similar to as I have for reproductions, then supported by the side by side evidence, and then (b) adding that side by side evidence to the existing reflections from reproductions\nTODO: For reflections on evaluations, add reflection that a lack of provision of ORCID or citation information didn’t have a big impact here, as I was choosing repositories that I had found from papers, so I already at least knew who the paper authors were. Although that doesn’t mean that any attempted citation of the repository itself would’ve necessarily been correct, depending on whether the author list would be the same.\nadd some scatter plots to repo evaluation but not sure how helpful they are",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Relationship between reproduction and evaluation</span>"
    ]
  },
  {
    "objectID": "pages/other_studies.html",
    "href": "pages/other_studies.html",
    "title": "10  Comparison to other studies",
    "section": "",
    "text": "In-progress\nTODO: Add page comparing our findings to other studies. This could include:\n\nStudies that have gone through this same process of finding barriers in reproductions\nStudies that recommend how to share code (although, it might be simpler to just start with that first bullet and then decide whether to do both)\n\nDo we also compare the evaluation results? Or do we just use that in context of barriers? (e.g. Schwander et al. (2021) and Zhang, Lhachimi, and Rogowski (2020) for reporting… Laurinavichyute, Yadav, and Vasishth (2022) for code…)\nnot sure if want to be comparing the actual proportions reproduced… or maybe do, but important to bare in mind what is being compared and what define as success… e.g. some include studies that haven’t shared the code\nStarting points:\n\nKrafczyk et al. (2021) - results are recommendations based on experiences, which were:\n\ncan see completion in figure 1\nbeing clear on links between article, code and data (e.g. which code uses which data, and which parts of code made each bit of article)\ninclude scripts for each aspect of article, with it easy to locate the scripts needed, and the scripts including the parameters needed and clearly label\nbe clear about hardware needed, e.g. if large amount of computing resources would be required. at least report hardware used. ideally include “small test case that can be run by users with conventional hardware”\nlist software dependencies and versions\nuse seeds, and report seed you used\nmake all code and data available with an appropriate licence\ninclude master script that runs in computations in publication\nuse same terminology in code and article\nuse version control and specify the e.g. commit hash that identifies the version used\navoid hard coding parameters\ndesign scripts in a way that allows people to easily change parameters and run again\navoid hard coding file paths\nprovide script that checks whether users results match original (within expected deviation)\nif compare against competing methods, include info on how those were implemented and tested\nuse build system for C/C++ code\nprovide scripts to make the figures and tables\n\nWood, Müller, and Brown (2018)\n\ncomplete data: 27 comparable results, 5 minor differences, 0 major differences\nincomplete data: 10 comparable, 4 minor differences, 1 major differences\nmain issue was the code and data not being shared\n\nSchwander et al. (2021)\n\nreproduction success for 3 out of 4 models\nfacilitators:\n\n“Model structure and possible state transitions were presented in a state transition diagram”\n“Overview of input parameters was provided in table format”\n\nhurdles:\n\n“PSAs were performed” (probablistic sensitivity analysis)\n“Relevant PSA values for PSA result reproduction were provided (type of distribution and either mean and standard deviation or distribution parameters were provided)”\n“Clinical event simulation results were provided (which are very helpful to guide potential assumptions to be made for rebuilding the model and which provide an additional means of testing the fit of the replication)”\n“Relevant details on the underlying life tables were provided (including year of data)”\n“Several self-created regression equations were introduced but without details on how to apply/solve the provided regressions correctly”\n\n\nLaurinavichyute, Yadav, and Vasishth (2022)\nKonkol, Kray, and Pfeiffer (2019)\nHardwicke et al. (2021)\nMonks and Harper (2023) (although overlap - look at how many of mine are from the review - some were identified from that, but some were not)\nHenderson et al. (2024)\nSamuel and Mietchen (2024)\n\nReproduction success:\n\n27,271 jupyter notebooks (2660 GitHub repositories, 3467 publications, majority python)\n15,817 included dependencies in standard requirement files\nOf those, 10,388 could be installed successfully\nOf those, 1203 ran without error\nOf those, 879 produced results identical to those reported in the original notebook\n\nReferences several other studies that have attempted to re-run jupyter notebooks\nModuleNotFoundError, ImportError and FileNotFounderError were the top 3 common exceptions\nAlso used flakenb to look for code styling errors\nImplications:\n\nLow reproducibility as in prior studies\nReview processes don’t pay attention to journal reproducibiltiy\nCommon errors around dependencies… Use exiting approaches like requirements, conda and poetry\nSome journals, article types, levels of documentation and research fields had more reproduced notebooks than others. Worth considering procedures at those journals with most reproduced (iScience). Also, notebooks combining computation and narrative.\nThings go out of data… importance of pre-prints…\nReproducibility badges, reproducibility platforms like REScience, nanopublications\n\n\n\n\n\n\n\nHardwicke, Tom E., Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michèle B. Nuijten, Benjamin N. Peloquin, Benjamin E. deMayo, Bria Long, Erica J. Yoon, and Michael C. Frank. 2021. “Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal Psychological Science: An Observational Study.” Royal Society Open Science 8 (1): 201494. https://doi.org/10.1098/rsos.201494.\n\n\nHenderson, Alec S., Roslyn I. Hickson, Morgan Furlong, Emma S. McBryde, and Michael T. Meehan. 2024. “Reproducibility of COVID-Era Infectious Disease Models.” Epidemics 46 (March): 100743. https://doi.org/10.1016/j.epidem.2024.100743.\n\n\nKonkol, Markus, Christian Kray, and Max Pfeiffer. 2019. “Computational Reproducibility in Geoscientific Papers: Insights from a Series of Studies with Geoscientists and a Reproduction Study.” International Journal of Geographical Information Science 33 (2): 408–29. https://doi.org/10.1080/13658816.2018.1508687.\n\n\nKrafczyk, M. S., A. Shi, A. Bhaskar, D. Marinov, and V. Stodden. 2021. “Learning from Reproducing Computational Results: Introducing Three Principles and the Reproduction Package.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 379 (2197): 20200069. https://doi.org/10.1098/rsta.2020.0069.\n\n\nLaurinavichyute, Anna, Himanshu Yadav, and Shravan Vasishth. 2022. “Share the Code, Not Just the Data: A Case Study of the Reproducibility of Articles Published in the Journal of Memory and Language Under the Open Data Policy.” Journal of Memory and Language 125 (August): 104332. https://doi.org/10.1016/j.jml.2022.104332.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\nSamuel, Sheeba, and Daniel Mietchen. 2024. “Computational Reproducibility of Jupyter Notebooks from Biomedical Publications.” GigaScience 13 (January): giad113. https://doi.org/10.1093/gigascience/giad113.\n\n\nSchwander, Björn, Mark Nuijten, Silvia Evers, and Mickaël Hiligsmann. 2021. “Replication of Published Health Economic Obesity Models: Assessment of Facilitators, Hurdles and Reproduction Success.” Pharmacoeconomics 39 (4): 433–46. https://doi.org/10.1007/s40273-021-01008-7.\n\n\nWood, Benjamin D. K., Rui Müller, and Annette N. Brown. 2018. “Push Button Replication: Is Impact Evaluation Evidence for International Development Verifiable?” PLOS ONE 13 (12): e0209416. https://doi.org/10.1371/journal.pone.0209416.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparison to other studies</span>"
    ]
  },
  {
    "objectID": "pages/acknowledgements.html",
    "href": "pages/acknowledgements.html",
    "title": "11  Acknowledgements",
    "section": "",
    "text": "We would like to thank the authors who made their code available under open licences, facilitating our research. We are especially grateful to the following authors for their helpful communication and support throughout the project:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Acknowledgements</span>"
    ]
  }
]