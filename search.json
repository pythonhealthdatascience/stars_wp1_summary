[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STARS Work Package 1: Computational Reproducibility Assessments",
    "section": "",
    "text": "Overview\nThis book describes the findings from work package 1 of the project STARS: Sharing Tools and Artefacts for Reproducible Simulations in healthcare.\nUse the sidebar to navigate through:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "STARS Work Package 1: Computational Reproducibility Assessments",
    "section": "",
    "text": "Introduction - background on STARS, pilot work, and the aim of work package 1\nMethods - summarises the methods which are described in detail in our protocol\nResults - describes the results of the reproductions and evaluations, and reflections from the process\nDiscussion - considers and applies findings",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "STARS Work Package 1: Computational Reproducibility Assessments",
    "section": "Funding",
    "text": "Funding\nThis work is supported by the Medical Research Council [grant number MR/Z503915/1].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "STARS Work Package 1: Computational Reproducibility Assessments",
    "section": "Licence",
    "text": "Licence\nThis book is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "pages/background.html",
    "href": "pages/background.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 General context\nBaker (2016) - survey of 1576 researchers across disciplines, finding:\nDifferences between reproduction and replication\n(I.e. terminology I researched previously, computational reproducibiltiy etc, similar to how lots of articles define it, e.g. Samuel and Mietchen (2024)).\nKorbmacher et al. (2023) - great discussion of the replication crisis\nOn the protocol page, several reproducibility studies are mentioned, which were used to help inform the methods in our reproducibility assessments.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#general-context",
    "href": "pages/background.html#general-context",
    "title": "2  Introduction",
    "section": "",
    "text": "“More than 70% of researchers have tried and failed to reproduce another scientist’s experiments”\n“More than half have failed to reproduce their own experiments”\n52% “agree that there is a significant ‘crisis’ of reproducibility”\n“Less than 31% think that failure to reproduce published results means that the result is probably wrong, and most say that they still trust the published literature”\n\n\n\nReproduction - using their code\nReplication - writing your own code",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#pilot-work",
    "href": "pages/background.html#pilot-work",
    "title": "2  Introduction",
    "section": "2.2 Pilot work",
    "text": "2.2 Pilot work\n\n2.2.1 Review of healthcare simulation sharing practices\nMonks and Harper (2023) explored how discrete-event simulation (DES) models used in a health context (e.g. health services, health economics) were shared and whether this sharing adhered to best practice. The full study can be viewed at:\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\nIn summary, they identified 564 papers describing a DES model from a health context published from 2019 to 2022. Of these, only 8.3% (47/564) had available model code (either sharing the code themselves, or citing an openly available model). Looking by year, this rose from 4.0% for studies published in 2019, to 9.0% for 2022.\nFurther findings:\n\nMore likely that code was shared if model was:\n\nCreated using free and open source software (FOSS) (28.7%, 29/101)\nA COVID-19 model (24.6%, 17/69)\n\nOf the papers that did share a model:\n\nMost were written in a programming language (66%, 31/47), the rest in a commercial off the shelf Visual Interactive Modelling (VIM) software\nThese were evaluated in a best practice audit, with the results as follows: “In general, computer models and artefacts were published without a DOI (n = 7); rarely included ORCIDs for authors (n = 6); rarely included an open licence (n = 21); were mostly supported by a README file (n = 28); rarely included documentation detailing how to run the model (n = 15); provided no form of formal or informal dependency management (n = 21); did not include any evidence of model testing (n = 3); were almost all downloadable (n = 38); and rarely executable via a cloud-based platform (n = 10).” (Monks and Harper (2023))\n\nFew studies used a reporting guideline (12.8%, 72/564) - mostly using:\n\nOne of the International Society for Pharmacoeconomics and Outcomes Research (ISPOR) publications (n=37), or\nThe Strengthening the Reporting of Empirical Simulation Studies (STRESS) guidelines from Monks et al. (2019) (n=22)\n\n\nThe review concludes that “there are many (simple) best practices the community can adopt, such as the use of trusted archives, and documentation, to improve its sharing”. (Monks and Harper (2023))\n\n\n2.2.2 Pilot STARS framework\nMonks, Harper, and Mustafee (2024) introduces a pilot framework for sharing DES models called STARS: Sharing Tools and Artefacts for Reusable Simulations. Note that this “reusable” is changed into “reproducible” for the STARS project as we build on this work in the current project.\nThe pilot STARS framework consists of essential components (minimum to make models “long-term, citable, functional, appropriately licenced”) and optional components (enhance model “accessibility, understanding, and maintainability”). (Monks, Harper, and Mustafee (2024))\nThe essential components are:\n\nOpen licence\nDependency management\nFOSS model\nMinimum documentation\nOpen Researcher and Contributor IDs (ORCID)\nCitation information\nRemote code repository\nOpen science archive\n\nThe optional components are:\n\nEnhanced documentation\nDocumentation hosting\nOnline coding environment\nModel interface\nWeb app hosting\n\nThis is summarised in the diagram below…\n\n\n\nSTARS framework overview\n\n\nThis was supported by example implementations in Python:\n\nExample 1 - stars-treat-sim\nExample 2 - stars-streamlit-example and stars-simpy-example-docs - with web app and hosted docs\nExample 3 - stars-ciw-example - with web app and hosted docs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#stars-project",
    "href": "pages/background.html#stars-project",
    "title": "2  Introduction",
    "section": "2.3 STARS project",
    "text": "2.3 STARS project\nThe MRC-funded STARS project builds on this pilot work. As stated in the funding application, the objective of this project is “to improve the quality and quantity of shared discrete-event simulation models, tools and other research artefacts:\n\nIdentify barriers, and good practices for sharing simulation models;\nDevelop a new framework for sharing computer models applicable the most common free and open-source languages;\nTest the framework in both retrospective and prospective case studies;\nDevelop online interactive training materials;\nTransfer knowledge of our STARS framework to health data science researchers;\nEnsure sustainability of materials;\nSupport our partner archival journals adopt open science principles and our findings;”\n\nThis objectives will be achieved through four work packages:\nWork package 1: Reproducibility of computational results\n\nAssess the computational reproducibility of six published DES models created in Python and R.\nEvaluate the publication, code and associated artefacts against reporting guidelines, best practice for code sharing, and criteria from journal artefact badges.\n\nWork package 2: R and Python framework for sharing DES models\n\nImprove the pilot framework (e.g. extend baesd on barriers and enablers to reproduction observed in work package 1, and making it relevant to R models)\nProvide time-saving measures for researchers (e.g. automated support for STRESS, use of large language models (LLM) to support creating of summaries, automated testing of models, continuous integration tools)\n\nWork package 3: Prospective and retrospective application of the framework\n\nApply STARS framework within two case studies (one prospective and one retrospective)\n\nWork package 4: Training\n\nDevelop online interactive training materials to support researchers in using the STARS framework\n\nFrom this point onwards, this site/book summarises the findings from work package 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#references",
    "href": "pages/background.html#references",
    "title": "2  Introduction",
    "section": "2.4 References",
    "text": "2.4 References\n\n\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nKorbmacher, Max, Flavio Azevedo, Charlotte R. Pennington, Helena Hartmann, Madeleine Pownall, Kathleen Schmidt, Mahmoud Elsherif, et al. 2023. “The Replication Crisis Has Led to Positive Structural, Procedural, and Community Changes.” Communications Psychology 1 (1): 3. https://doi.org/10.1038/s44271-023-00003-2.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.\n\n\nSamuel, Sheeba, and Daniel Mietchen. 2024. “Computational Reproducibility of Jupyter Notebooks from Biomedical Publications.” GigaScience 13 (January): giad113. https://doi.org/10.1093/gigascience/giad113.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html",
    "href": "pages/protocol.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Protocol summary\nFor this work, six published healthcare DES models were selected. These were models with publicly available code under an open licence (either already available or add on request from the STARS team). For each model, the follow stages of work were conducted:\nStage 1: Reproduction - assessing the computational reproducibility of each study\nStage 2: Evaluation - evaluating the publication, code and associated artefacts against sharing and reporting standards\nStage 3: Report and research compendium - summary report and organised repository\nFor each study, a quarto site was produced which shared the results from the reproduction and evaluation and the summary report. Throughout the work, a detailed logbook was kept to keep track of timings and to record work on each stage, such as detailing troubleshooting steps during the reproduction, or uncertainities discussed with another STARS team member during the evaluation.\nSummary diagram\nThis process is captured in the diagram below:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#protocol-summary",
    "href": "pages/protocol.html#protocol-summary",
    "title": "3  Methods",
    "section": "",
    "text": "Informed authors about study and, if not available, asked if they would be happy to add an open licence to their code\nSet up repository for reproduction with the article and code\nRead the article and defined the scope of the reproduction, archiving the scope (and repository) on Zenodo before proceeding\nLooked over the model code, created a suitable environment with the software and packages required, and then ran the model. For each study, with any issues faced in running the model, troubleshooting was performed such as modifying or writing code. If troubleshooting was exhaused and there were still issues or discrepancies in the results, the original study authors were informed and provided the opportunity to advice on the reason for this (although with no pressure or requirement to do so)\nFor each item in the scope, a decision was made as to whether this had been successfully reproduced or not. This was a subjective decision which allowed some expected deviation due to model stochasticity (for example, lack of seed control).\nThis is timed (including time to produce each item in the scope), and limited to a maximum of 40 horus\n\n\n\nThe publication was evaluated against reporting guidelines for DES:\n\nMonks et al. (2019) - STRESS-DES: Strengthening The Reporting of Empirical Simulation Studies (Discrete-Event Simulation) (Version 1.0).\nZhang, Lhachimi, and Rogowski (2020) - The generic reporting checklist for healthcare-related discrete event simulation studies derived from the the International Society for Pharmacoeconomics and Outcomes Research Society for Medical Decision Making (ISPOR-SDM) Modeling Good Research Practices Task Force reports.\n\nThe model code and associated artefacts (e.g. the GitHub repository shared by the authors) was evaluated against:\n\nThe criteria of badges related to reproducibility from various organisations and journals - namely:\n\nNational Information Standards Organisation (NISO)(NISO Reproducibility Badging and Definitions Working Group (2021))\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\nCenter for Open Science (COS) (Blohowiak et al. (2023))\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\nRecommendations from the pilot STARS framework for the sharing of code and associated materials from discrete-event simulation models (Monks, Harper, and Mustafee (2024)).\n\nThis is timed\n\n\n\nWrote a report summarising the computational reproducibility assessment and evaluation\nRestructed the reposuitory into a “research compendium”, which essentially consisted of organising the repository to ensure it is easy and clear for someone else to re-run. Steps included:\n\nAdding run times to the model notebooks\nWrite a README for the reproduction folder\nMoving data, methods and outputs into seperate folders\nCreating tests which check if a user can get the same results from the model as we did during the reproduction\nA Dockerfile and Docker image published on the GitHub container registry\n\nA second researcher from the STARS team then attempted to use the repository and confirm whether they were able to reproduce the results of the first researcher\nFinally, the repository was archived on Zenodo, and the authors were informed.\n\n\n\n\n\n\n\nWorkflow for STARS work package 1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#inspiration",
    "href": "pages/protocol.html#inspiration",
    "title": "3  Methods",
    "section": "3.2 Inspiration",
    "text": "3.2 Inspiration\nAs cited in Heather et al. (2024), the protocol was informed by several prior studies assessing computational reproducibility:\n\nKrafczyk et al. (2021): Assessed the reproducibility of seven articles published in the Journal of Computational Physics.\nB. D. K. Wood, Müller, and Brown (2018) and B. Wood et al. (2018): Assessed the reproducibility of 109 published impact evaluations in low- and middle-income countries. Conducted in association with the replication programme of the International Initiative for Impact Evaluation (3ie).\nSchwander et al. (2021): Assessed the reproducibility of four health economic obesity models.\nLaurinavichyute, Yadav, and Vasishth (2022): Assessed the reproducibility of 118 articles published in the Journal of Memory and Language.\nKonkol, Kray, and Pfeiffer (2019): Assessed the reproducibility of 41 geoscientific articles from Copernicus Publications and the Journal of Statistical Software.\n\nIt was also informed by:\n\nAyllón et al. (2021): Article with recommendations on keeping modelling notebooks to support completion of TRACE (TRAnsparent and Comprehensive model Evaluation) documents.\nBerkeley Initiative for Transparency in the Social Sciences (2022): Guidelines on conducting reproductions of published social science research.\nMcManus, Turner, and Sach (2019): Article proposing several possible definitions for success in reproducing or replicating models in health economics.\nMarwick, Boettiger, and Mullen (2018): Article recommending how to structure data analytical work as research compendiums using the R package structure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#major-deviations-from-the-protocol",
    "href": "pages/protocol.html#major-deviations-from-the-protocol",
    "title": "3  Methods",
    "section": "3.3 Major deviations from the protocol",
    "text": "3.3 Major deviations from the protocol\nThere was one major deviation from the protocol…\n\n\n\n\n\n\n\nDeviation\nDescription and reason for the change\n\n\n\n\nExpanding from 6 to 8 studies\nWhilst partway through the 6th study, we reflected on what we had found so far, and felt it would be beneficial to do a few more studies. The primary motivation for this was to try and include a selection of studies that reflect the range of studies in the literature. So far, we had a few studies where the model code/run times were very large or very small, and we felt it beneficial to try and include some more “medium-sized” studies, as these are more typical of the literature. We also thought it would be good to include an older study (for example, about 10 years ago, if we can find one), as all those included so far were within the last few years, but working with more outdated code will be/can be a problem people encounter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#minor-deviations-from-the-protocol",
    "href": "pages/protocol.html#minor-deviations-from-the-protocol",
    "title": "3  Methods",
    "section": "3.4 Minor deviations from the protocol",
    "text": "3.4 Minor deviations from the protocol\nThere were some minor deviations from the protocol, which are explained below…\n\n\n\n\n\n\n\nDeviation\nDescription and reason for the change\n\n\n\n\nUsing the latest software packages\nIn the protocol, I had planned that - if no versions were provided we select a version of the software and each package that is closest to but still prior to the date of the code archive or paper publication. I kept to this for the Python models (easily set using a conda/mamba environment). However, I had great difficulties attempting to do this in R, and could not successfully backdate both. As such, I used the latest versions of R and each package for those studies\n\n\nUsing percentage difference in results to help decided reproduction success\nThis is not particularly deviation, as I did explore this, but I ultimately found it very unhelpful, as the percentage difference could be greatly impacted by scale (for example, 0.1 vs 0.2 would appear much greater than 3 vs 4, but the actual meaning of these differences might be similar (e.g. both might be considered a small difference) depending on the scale used and what is being compared - whilst in another context with a different scale, 0.1 vs 0.2 might actually reflect a huge difference!).\n\n\nMoving onto evaluation stage before receive author response regarding reproduction discrepancy or before getting consensus on reproduction\nIn the protocol, we required that authors are contacted if there are any remaining difficulties in running the code or items in the scope that were not reproduced. The authors were given a total of four weeks to respond if they chose to. We had implied that we must wait for this time to pass before continuing to the evaluation stage (since the three stages were presented as being completed one after another). The rationale for this was that the timings for the reproduction would be influenced by whether the evaluation had been completed or not, and vice versa. However, given the many possible influences on the timings, this was considered negligble.We also required consensus on whether items had been succesfully reproduced or not before moving on. In some cases, this was not done, as other team members were not available (e.g. busy or on annual leave) and so could not yet give a second opinion, and so I progressed with the evaluation and got consensus on reproductions afterwards, once they were available.\n\n\nOrganisation of the repository for the research compendium\nIn the protocol, we had planned that seperate folders were created for data, methods and outputs. This was generally followed but, if an alternative structure seemed more suitable. For example, if the original study already divided items well, but perhaps with different folder names or with multiple scripts folders or so on, we might have used that original structure, as it still served the purpose of being clear and easy to re-run, whilst reducing the number of differences compared with the original study.\n\n\nRetrospective archiving\nFor Johnson et al. 2021, I forgot to create a release to archive this repository after defining scope and before proceeding with reproduction. However, I was easily able to resolve this by setting the release to a prior commit, choosing the commit from that timepoint (between scope and reproduction), so it was as if it had been done at the time\n\n\nIdentification of papers\nAlthough 7 papers were identified from the review by Monks and Harper (2023), 1 was identified from additional searches (informal, not systematic) - this was so we could find an older paper.\n\n\nReflections\nI effectivelly created an “additional” stage, which was to create a page of reflections on the reproduction (barriers and faciliators). This was not formally part of the original protocol, but implicit.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#timings",
    "href": "pages/protocol.html#timings",
    "title": "3  Methods",
    "section": "3.5 Timings",
    "text": "3.5 Timings\nAs per the protocol, the reproduction and evaluation stages were timed. Although this was conducted carefully and thoroughly, it will not be perfect, and we recognise some of the potential sources of variation in timings between studies, such as:\n\nWhether we consistently included amendments to the quarto site and repository and time spent on GitHub commits etc. within the timings for the reproduction.\nFor the first R study (Huang et al. (2019)), I initially tried to create an environment with R and package versions prior to the article publication date, although had great difficulties with this and ended up using the latest versions. This contributed to the set-up time during this reproduction, but on later R models (Kim et al. (2021) and Johnson et al. (2021)), based on that experience, I did not attempt to backdate them when getting started.\nAny estimated times (for example, if I were partway through working but someone in the office came to talk to me and I forgot to note the time of that, I might estimate if that were about five or ten minutes of conversation, and set the time accordingly).\nTimings from consensus discussions regarding uncertainities in the evaluation or reproduction (as these might be longer if done in person rather than over email - or vice versa - and I sometimes spent longer on sorting/tidying these for some studies than others, which I would have included in the time)\nWhether subjectively feel that need to add random seeds during reproduction stage, if results vary considerably between each run, and so a certain seed could get a much more similar result than another\n\nAt an estimate, this uncertainty between study timings would lead me to conclude that the timings are approximately correct, give or take up to about four hours. However, this is just an estimate, and it is worth noting that Krafczyk et al. (2021), who also conducted computational reproducibility assessments in a different context, estimated that human error introduced a maximum of 8 hours ambiguity in the timings, due to the “non-precise nature of starting and stopping the watch consistently”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#references",
    "href": "pages/protocol.html#references",
    "title": "3  Methods",
    "section": "3.6 References",
    "text": "3.6 References\n\n\n\n\nAssociation for Computing Machinery (ACM). 2020. “Artifact Review and Badging Version 1.1.” ACM. https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nAssociation for Psychological Science (APS). 2023. “Psychological Science Submission Guidelines.” APS. https://www.psychologicalscience.org/publications/psychological_science/ps-submissions.\n\n\nAyllón, Daniel, Steven F. Railsback, Cara Gallagher, Jacqueline Augusiak, Hans Baveco, Uta Berger, Sandrine Charles, et al. 2021. “Keeping Modelling Notebooks with TRACE: Good for You and Good for Environmental Research and Management Support.” Environmental Modelling & Software 136 (February): 104932. https://doi.org/10.1016/j.envsoft.2020.104932.\n\n\nBerkeley Initiative for Transparency in the Social Sciences. 2022. “Guide for Advancing Computational Reproducibility in the Social Sciences.” https://bitss.github.io/ACRE/.\n\n\nBlohowiak, Ben B., Johanna Cohoon, Lee de-Wit, Eric Eich, Frank J. Farach, Fred Hasselman, Alex O. Holcombe, Macartan Humphreys, Melissa Lewis, and Brian A. Nosek. 2023. “Badges to Acknowledge Open Practices.” https://osf.io/tvyxz/.\n\n\nHardwicke, Tom E., and Simine Vazire. 2023. “Transparency Is Now the Default at Psychological Science.” Psychological Science 0 (0). https://doi.org/https://doi.org/10.1177/09567976231221573.\n\n\nHeather, Amy, Thomas Monks, Alison Harper, Navonil Mustafee, and Andrew Mayne. 2024. “Protocol for Assessing the Computational Reproducibility of Discrete-Event Simulation Models on STARS,” June. https://zenodo.org/records/12179846.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nInstitute of Electrical and Electronics Engineers (IEEE). n.d. “About Content in IEEE Xplore.” IEEE Explore. Accessed May 20, 2024. https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-content.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nKonkol, Markus, Christian Kray, and Max Pfeiffer. 2019. “Computational Reproducibility in Geoscientific Papers: Insights from a Series of Studies with Geoscientists and a Reproduction Study.” International Journal of Geographical Information Science 33 (2): 408–29. https://doi.org/10.1080/13658816.2018.1508687.\n\n\nKrafczyk, M. S., A. Shi, A. Bhaskar, D. Marinov, and V. Stodden. 2021. “Learning from Reproducing Computational Results: Introducing Three Principles and the Reproduction Package.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 379 (2197): 20200069. https://doi.org/10.1098/rsta.2020.0069.\n\n\nLaurinavichyute, Anna, Himanshu Yadav, and Shravan Vasishth. 2022. “Share the Code, Not Just the Data: A Case Study of the Reproducibility of Articles Published in the Journal of Memory and Language Under the Open Data Policy.” Journal of Memory and Language 125 (August): 104332. https://doi.org/10.1016/j.jml.2022.104332.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using R (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\nMcManus, Emma, David Turner, and Tracey Sach. 2019. “Can You Repeat That? Exploring the Definition of a Successful Model Replication in Health Economics.” PharmacoEconomics 37 (11): 1371–81. https://doi.org/10.1007/s40273-019-00836-y.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.\n\n\nNISO Reproducibility Badging and Definitions Working Group. 2021. “Reproducibility Badging and Definitions.” https://doi.org/10.3789/niso-rp-31-2021.\n\n\nSchwander, Björn, Mark Nuijten, Silvia Evers, and Mickaël Hiligsmann. 2021. “Replication of Published Health Economic Obesity Models: Assessment of Facilitators, Hurdles and Reproduction Success.” Pharmacoeconomics 39 (4): 433–46. https://doi.org/10.1007/s40273-021-01008-7.\n\n\nWood, Benjamin D. K., Rui Müller, and Annette N. Brown. 2018. “Push Button Replication: Is Impact Evaluation Evidence for International Development Verifiable?” PLOS ONE 13 (12): e0209416. https://doi.org/10.1371/journal.pone.0209416.\n\n\nWood, Benjamin, Annette Brown, Eric Djimeu, Maria Vasquez, Semi Yoon, and Jane Burke. 2018. “Replication Protocol for Push Button Replication (PBR).” OSF, January. https://doi.org/https://doi.org/10.17605/OSF.IO/YFBR8.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html",
    "href": "pages/reproduction.html",
    "title": "4  Reproduction",
    "section": "",
    "text": "4.1 Studies\nShoaib and Ramamohan (2021): Uses python (salabim) to model primary health centres (PHCs) in India. The model has four patient types: outpatients, inpatients, childbirth cases and antenatal care patients. Four model configurations are developed based on observed PHC practices or government-mandated operational guidelines. The paper explores different operational patterns for scenarios where very high utilisation was observed, to explore what might help reduce utilisation of these resources. Note: The article was as Shoaib and Ramamohan (2022), but we used the green open access pre-print Shoaib and Ramamohan (2021). Link to reproduction.\nHuang et al. (2019): Uses R (simmer) to model an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway. The paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR. Link to reproduction.\nLim et al. (2020): Uses python (NumPy and pandas) to model the transmission of COVID-19 in a laboratory. It examines the proportion of staff infected in scenarios varying the: number of shifts per day; number of staff per shift; overall staff pool; shift patterns; secondary attack rate of the virus; introduction of protective measures (social distancing and personal protective equipment). Link to reproduction.\nKim et al. (2021): Adapts a previously developed R (Rcpp, expm, msm, foreach, iterators, doParallel) model for abdominal aortic aneurysm (AAA) screening of men in England. The model is adapted/used to explore different approaches to resuming screening and surgical repair for AAA, as these services were paused or substantially reduced during COVID-19 due to concerns about virus transmission. Link to reproduction.\nAnagnostou et al. (2022): This paper includes two models - we have focussed just on the dynamiC Hospital wARd Management (CHARM) model. CHARM uses python (SimPy) to model intensive care units (ICU) in the COVID-19 pandemic (as well as subsequent stays in a recovery bed). It includes three types of admission to the ICU (emergency, elective or COVID-19). COVID-19 patients are kept seperate, and if they run out of capacity due to a surge in COVID-19 admissions, additional capacity can be pooled from the elective and emergency capacity. Link to reproduction.\nJohnson et al. (2021): This study uses a previously validated discrete-event simulation model, EPIC: Evaluation Platform in chronic obstructive pulmonary disease (COPD). The model is written in C++ with an R interface, using R scripts for execution. The model is adapted to evaluate the cost-effectiveness of 16 COPD case detection strategies in primary care, comparing costs, quality-adjusted life years (QALYs), incremental cost-effectiveness ratios (ICER), and incremental net monetary benefits (INMB) across scenarios. Sensitivity analyses are also conducted. Link to reproduction.\nHernandez et al. (2015): This study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense counter-measures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput. They use python for most of the analysis (with SimPy for the simulation component), but R to produce the plots and tables for the paper. Link to reproduction.\nWood et al. (2021): This study uses discrete-event simulation (R (base R, dplyr, tidyr)) to explore the deaths and life years lost under different triage strategies for an intensive care unit, relative to a baseline strategy. The unit is modelled with 20 beds (varied from 10 to 200 in sensitivity analyses). Three different triage strategies are explored, under three different projected demand trajectories. Link to reproduction.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#scope-and-reproduction",
    "href": "pages/reproduction.html#scope-and-reproduction",
    "title": "4  Reproduction",
    "section": "4.2 Scope and reproduction",
    "text": "4.2 Scope and reproduction\n\n\n\n\n\n\n\n\n\nStudy\nScope\nSuccess\nTime\n\n\n\n\nShoaib and Ramamohan 2022\n17 items:• 1 table• 9 figures• 7 in-text results\n16 out of 17 (94%)\n28h 14m\n\n\nHuang et al. 2019\n8 items:• 5 figures• 3 in-text results\n3 out of 8 (37.5%)\n24h 10m\n\n\nLim et al. 2020\n9 items:• 5 tables• 4 figures\n9 out of 9 (100%)\n12h 27m\n\n\nKim et al. 2021\n10 items:• 3 tables• 6 figures• 1 in-text result\n10 out of 10 (100%)\n14h 42m\n\n\nAnagnostou et al. 2022\n1 item:• 1 figure\n1 out of 1 (100%)\n2h 11m\n\n\nJohnson et al. 2021\n5 items:• 1 table• 4 figures\n4 out of 5 (80%)\n19h 49m\n\n\nHernandez et al. 2015\n8 items:• 6 figures• 2 tables\n1 out of 8 (12.5%)\n17h 56m\n\n\nWood et al. 2021\n5 items:• 4 figures• 1 table\n5 out of 5 (100%)\n3h 50m\n\n\n\n\n\n\n\n\n\nReproduction reflections\n\n\n\n\n\nFor the studies where I didn’t manage to fully reproduce results despite troubleshooting, my reflections on what I think to be the primary reason for not managing to reproduce results in each case were:\nShoaib and Ramamohan (2021): No specific suggestions - had to write code to run scenarios and process results, so it could be that I hadn’t done this all exactly the same as in the original study.\nHuang et al. (2019): No specific suggestions - had to write code to run scenarios and process results, so it could be that I hadn’t done this all exactly the same as in the original study.\nJohnson et al. (2021): No specific suggestions - although note that their results on GitHub appeared to likewise have the same discrepancy compared with the article, so it appears there might have been a minor change to the code/parameters made from that used to produce the article figure that might explain this.\nHernandez et al. (2015): This appears likely to be due to a parameter somewhere being not quite right. Ivan Hernandez suggested that it could be a random seed or that the optimisation doesn’t have enough runs - although was using the same seed and played around with run number. The other suggestion was that the version on GitHub might not have exactly the right version of inputs - and interestingly, he had an earlier version of the paper that had more similar results (our discrepancy was in range on Y axis, and he had an earlier version with range of 0 to 15000). This reaffirms that a minor parameter difference is the likely reason for the discrepancy, with that having impacted across nearly all the results.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#time-to-complete-reproductions",
    "href": "pages/reproduction.html#time-to-complete-reproductions",
    "title": "4  Reproduction",
    "section": "4.3 Time to complete reproductions",
    "text": "4.3 Time to complete reproductions\n\nNon-interactive figure:\n\n\n\n\n\n\n\n\n\nInteractive figure:\n\n\n                                                \n\n\n\n\n\n\n\n\nTiming reflections\n\n\n\n\n\nThere was a large amount of variation in the time each reproduction took. For each study, I have also reflected on what I think were the primary reasons behind things being quicker or slower:\nShoaib and Ramamohan (2021) - 28h 14m:\n\nThis had lots of items to reproduce (17)\nTook me longer than usual to set up environment as needed specific package versions and had some confusion around package statistics which is base but I had mixed up with one that can be imported from conda/pypi\nMost of the time was dedicated to identifying parameters for each scenario, writing code to run the scenarios (and run these programmatically), and writing code to process the results into figures and tables.\n\nHuang et al. (2019) - 24h 10m:\n\nTime-consuming aspects were modifying the code from the app so I could run it, writing code for each scenario, and writing code for figures (it took me quite a whiole to work out transformations for axes and how to standardise density)\n\nLim et al. (2020) - 12h 27m:\n\nLots of time was spent setting up model so could run programmatically, writing code for scenarios, and writing code to produce figures.\n\nKim et al. (2021) - 14h 42m:\n\nTime-consuming aspect was largely writing code to produce tables and figures (including figuring out how to process).\n\nAnagnostou et al. (2022) - 2h 11m:\n\nThis only had one item to reproduce.\nIt required very little troubleshooting - just had to write code to produce figure, although it was relatively simple.\n\nJohnson et al. (2021) - 19h 49m:\n\nMost time was spent on writing code for sensitivity analysis, and to produce tables and figures (which included working out which results tables / columns / scenarios to use, how to transform columns, and how to calculate features like efficiency frontier)\n\nHernandez et al. (2015) - 17h 56m:\n\nTime-consuming aspects were writing code for scenarios and for figures and tables, and troubleshooting parameter discrepancies.\n\nWood et al. (2021) - 3h 50m:\n\nRan quick as code for model didn’t require troubleshooting (included all correct parameters and scenarios), and as they provided code to produce the figures and tables.\n\nRegarding run time of the models themselves, for longer models, I sometimes experimented with parameters to run it quicker, in order to more easily troubleshoot, else I would run for a long time and then discover XYZ is wrong. Although the model run time itself is not included in the times, it is important to note, as some models took several days, and so in practice, this would impact on someone if they were trying to run a model within a short amount of time.\nRegarding the reproduction run times, I think the main reflection from above is that including code with correct parameters and scenarios, and code to make the figures and tables, has a really big impact.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#model-run-times",
    "href": "pages/reproduction.html#model-run-times",
    "title": "4  Reproduction",
    "section": "4.4 Model run times",
    "text": "4.4 Model run times\nFor reference, the run times for the models are detailed below. It’s worth being aware that these are not compared on a “level playing field” as they were run on different machines, and with/without parallel processing or parallel terminals.\n\n\n\n\n\n\n\n\n\nStudy\nMachine specs\nModel run time\nAdditional details\n\n\n\n\nShoaib and Ramamohan (2021)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\n22 minutes 26 seconds\nThis is based on the combined runtime of the notebooks which use 10 replications (rather than 100) and parallel processing, but with each notebook run seperately.\n\n\nHuang et al. (2019)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\n29 minutes 10 seconds\nCombined time from notebooks run seperately.\n\n\nLim et al. (2020)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\n49 minutes and 17 seconds\n\n\n\nKim et al. (2021)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\n6 hours 53 minutes\nWe reduced the number of patients in the simulation from 10 million to 1 million, to improve run times. Note, you can expect the runtime to be notably longer on machines with lower specs than this. For example, I ran surveillance scenario 0 on an Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux. The runtime increased from 4 minutes 28 seconds up to 21 minutes 59 seconds.\n\n\nAnagnostou et al. (2022)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\nOnly a few seconds.\n\n\n\nJohnson et al. (2021)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\nAt least 1 day 11 hours\nIt was run from the command line via multiple terminals simultaneously - hence, the exact times are impacted by the number being ran at once. The base case scenarios were ran at the same time, and took an overall total time of 20 hours 40 minutes. The sensitivity analysis scenario were all ran at the same time, and took an overall total of 1 day 10 hours 57 minutes. Four files (scenarios 2 and 3) had to be re-run seperately after fixing a mistake, and when just those were run, the total was 21 hours 26 minutes (quicker than when they were run as part of the full set of scenarios, when they took up to 1.3 days). Based on this, if you were to run all scenarios at once in parallel in seperate terminals, you could expect a run time of at least 1 day 11 hours, but would be higher than that (since the more run at once, the lower it takes, as you can see from the scenario 2 and 3 times above).\n\n\nHernandez et al. (2015)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\n9 hours 16 minutes\nThis involved running the models within each experiment in parallel, but each of the experiment files seperately. If these are run at the same time (which I could do without issue), then you will be able to run them all within 4 hours 28 minutes (the longest experiment) (or a little longer, due to slowing speeds from running at once). Also, this has excluded one of the variants for Experiment 3, which I did not run as it had a very long run time (quoted to be 27 hours in the article) and as, regardless, I had not managed to reproduce the other sub-plots in the figure for that experiment.\n\n\nWood et al. (2021)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\n48 hours 25 minutes\nIt ran in a single loop, so required the computer to remain on for that time. This time includes all scenarios - but, for just the base scenario, the run time was 2 hours 3 minutes.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#references",
    "href": "pages/reproduction.html#references",
    "title": "4  Reproduction",
    "section": "4.5 References",
    "text": "4.5 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\n———. 2022. “Simulation Modeling and Analysis of Primary Health Center Operations.” SIMULATION 98 (3): 183–208. https://doi.org/10.1177/00375497211030931.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html",
    "href": "pages/reflections.html",
    "title": "5  Reflections from reproductions",
    "section": "",
    "text": "5.1 Summary\nThis is a summary of all the items that were evaluated (with ✅🟡❌) below.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#environment",
    "href": "pages/reflections.html#environment",
    "title": "5  Reflections from reproductions",
    "section": "5.2 Environment",
    "text": "5.2 Environment\n\n\n\n\n\n\nList required packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n✅\n✅\n🟡\n🟡\n❌\n\n\n\nShoaib and Ramamohan (2021): Not met. This became a time-consuming issue as it took a while to identify a dependency that was needed for the code to work (greenlet) (based on reading the documentation for salabim), and a while longer to realise I had installed another package when the package I needed was in base (statistics).\nHuang et al. (2019): Not met. However, this was fairly easily resolved based on imports to .R script, and then on extra imports suggested by RStudio when I tried and failed to run the script.\nLim et al. (2020): Partially met. The only packages needed (numpy and pandas) are mentioned in the paper (although only listed as imports in the script).\nKim et al. (2021): Fully met. Provides commands to install packages required at the start of scripts, which I could then easily base renv on automatically (as it detects them).\nAnagnostou et al. (2022): Fully met. Provides requirements.txt\nJohnson et al. (2021): Partially met. DESCRIPTION file accompanying epicR package contained some but not all dependencies.\nHernandez et al. (2015): Partially met. Some (but not all) of the required packages were listed in the paper. Of particular note, this depended on having a local package myutils/, which was another GitHub repository from the author. This was not mentioned anywhere, and so required to notice this was needed.\nWood et al. (2021): Not met. However, easily resolved based on imports to .R script.\nReflections:\n\nThe import statements can be sufficient in indicating all the packages required but this is not always the case if there are “hidden”/unmentioned dependencies that don’t get imported\n\nTom: Given that import statements are not always enough, then I would argue more is needed.\nTom: Useful to know that renv “detects” dependencies listed as imports in scripts.\n\nThere are various options for listing the packages (e.g. comprehensive import statements, installation lines in the script, environment files, package DESCRIPTION file).\nIdeally mention this in repository, not just the paper.\nIf there are local dependencies (e.g. other GitHub repositories), make sure to (a) mention and link to these repositories, so it is clear they are also required, and (b) include licence/s in those repositories also, so they can be used.\nThis was a common issue.\n\n\n\n\n\n\n\n\n\n\nBe aware of potential system dependencies\n\n\n\n\n\nThere can also be system dependencies, which will vary between systems, and may not be obvious if researchers already have these installed. We identified these when setting up the docker environments (which act like “fresh installs”):\n\nShoaib and Ramamohan (2021), Lim et al. (2020), Anagnostou et al. (2022) - no dependencies\nHuang et al. (2019), Kim et al. (2021), Johnson et al. (2021) and Wood et al. (2021) - libcurl4-openssl-dev, libssl-dev, libxml2-dev, libglpk-dev, libicu-dev - as well as tk for Johnson et al. 2021\nHernandez et al. (2015) - wget, build-essential, libssl-dev, libffi-dev, libbz2-dev, libreadline-dev, libsqlite3-dev, zlib1g-dev, libncurses5-dev, libgdbm-de, libnss3-dev, tk-dev, liblzma-dev, libsqlite3-dev, lzma, ca-certificates, curl, git\n\nAlthough it would be unreasonable for authors to be aware of and list all system dependencies, given they may not be aware of them, this does show the benefit of creating something like docker in identifying them and making note of them within the docker files.\nThis issue was specific to (a) R studies, and (b) the study with an unsupported version of Python that required building it from source in the docker file.\n\n\n\n\n\n\n\n\n\nProvide versions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n\n\n\nShoaib and Ramamohan (2021): Not met. I had to backdate the package versions as the model didn’t work with the latest.\nHuang et al. (2019): Not met. I initially tried to create an environment with R and package versions that were prior to the article publication date. However, I had great difficulties implementing this with R, and never managed to successfully do this. This was related to:\n\nThe difficulty of switching between R versions\nProblems in finding available/a source for specific package versions for specific versions or R\n\nLim et al. (2020): Partially met. Provides major Python version, but chose minor and the package versions based on article publication date.\nKim et al. (2021): Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.\nAnagnostou et al. (2022): Partially met (depending on how strict you are being). The Python version was stated in the paper, and the SimPy version was stated in the complementary app repository (although neither were mentioned in the model repository itself). Requirements file just contains one thing - simpy - with no version.\nJohnson et al. (2021): Partially met. R version given in paper. DESCRIPTION file contains minimum versions for some but not all packages.\nHernandez et al. (2015): Partially. Versions of Python, R and some (but not all) packages given in the paper. Some versions weren’t very specific (e.g. Python 2.7 v.s. something specific like 2.7.12)\nWood et al. (2021): Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.\nReflections:\n\nModels will sometimes work with the latest versions of packages, but likewise, you will sometimes need to backdate as it no longer works with the latest\nFor Python, it was very easy to “backdate” the python and package versions. However, I found this very difficult to in R, and ended up always using the latest versions.\nVersions are sometimes provided elsewhere (e.g. in paper, in other repositories), but would be handy to be in model repository itself.\nHandy to provide specific versions too, particularly when there can be reasonably large changes between minor versions.\nThis was a very common issue.\nTom: For R, we are sort of moving towards a pre-built container for an R reproducible pipeline\n\nResponse: Though being aware that it is possible to successfully reproduce without backdating - didn’t run into issues with it for the R models - though that doesn’t mean you wouldn’t - but it is a sort of “characteristic” of R, that it is supposed to be less changeable than Python in this regard. Though obviously no guarantees. And not a fair comparison, as I didn’t try to run the Python ones without backdating.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#structure-of-code-and-scripts",
    "href": "pages/reflections.html#structure-of-code-and-scripts",
    "title": "5  Reflections from reproductions",
    "section": "5.3 Structure of code and scripts",
    "text": "5.3 Structure of code and scripts\n\n\n\n\n\n\nModel is provided in a “runnable” format\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Fully met. Provided as a single .py file which ran model with function main().\nHuang et al. (2019): Not met. The model code was provided within the code for a web application, but the paper was not focused on this application, and instead on specific model scenarios. I had to extract the model code and transform it into a format that was “runnable” as an R script/notebook.\nLim et al. (2020): Fully met. Provided as a single .py file which ran the model with a for loop.\nKim et al. (2021): Fully met. Has seperate .R scripts for each scenario which ran the model by calling functions from elsewhere in repository.\nAnagnostou et al. (2022): Fully met. Can run model from command line.\nJohnson et al. (2021): Fully met. Model provided as a package (which is an R interface for the C++ model).\nHernandez et al. (2015): Fully met. The model (python code) can be run from main.py.\nWood et al. (2021): Fully met. Provided as a single .R file which ran the model with a for loop.\nReflections:\n\nIf you are presenting the results of a model, then provide the code for that model in a “runnable” format.\nThis was an uncommon issue.\n\n\n\n\n\n\n\n\n\n\nModel is designed to be run programmatically (i.e. can run model with different parameters without needing to change the model code)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The model is set up as classes and run using a function. However, it is not designed to allow any variation in inputs. Everything uses default inputs, and it designed in such a way that - if you wish to vary model parameters - you need to directly change these in the script itself.\nHuang et al. (2019): Fully met. Model was set up as a function, with many of the required parameters already set as “changeable” inputs to that function.\nLim et al. (2020): Fully met. The model is created from a series of functions and run with a for loop that iterates through different parameters. As such, the model is able to be run programmatically (within that for loop, which varied e.g. staff per shift and so on and re-ran the model).\nKim et al. (2021): Fully met. Each scenario is an R script which states different parameters and then calls functions to run model.\nAnagnostou et al. (2022): Fully met. Change inputs in input .csv files.\nJohnson et al. (2021): Fully met. Creates a list of input which are then used by a run() function.\nHernandez et al. (2015): Fully met. Model created from classes, which accept some inputs and can run the model.\nWood et al. (2021): Fully met. Changes inputs to run all scenarios from a single .R file.\nReflections:\n\nDesign model so that you can re-run it with different parameters without needing to make changes to the model code itself.\n\nThis allows you to run multiple versions of the model with the same script.\nIt also reduces the likelihood of missing errors (e.g. if miss changing an input parameter somewhere, or input the wrong parameters and don’t realise).\n\nThis was an uncommon issue.\nNote, this just refers to the basic set-up, with items below like hard coding parameters also being very important in this context.\n\n\n\n\n\n\n\n\n\n\nDon’t hard code parameters that you will want to change for scenario analyses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n🟡\n❌\n✅\nN/A\n✅\n🟡\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. Although some parameters sat “outside” of the model within the main() function (and hence were more “changeable”, even if not “changeable” inputs to that function, but changed directly in script). However, many of the other parameters were hard-coded within the model itself. It took time to spot where these were and correctly adjust them to be modifiable inputs.\nHuang et al. (2019): Partially met. Pretty much all of the parameters that we wanted to change were not hard coded and were instead inputs to the model function simulate_nav(). However, I did need to add an exclusive_use scenario which conditionally changed ir_resources, but that is the only exception. I also add ed_triage as a changeable input but didn’t end up needing that to reproduce any results (was just part of troubleshooting). I also\nLim et al. (2020): Not met. Some parameters were not hard coded within the model, but lots of them were not.\nKim et al. (2021): Fully met. All model parameters could be varied from “outside” the model code itself, as they were provided as changeable inputs to the model.\nAnagnostou et al. (2022): N/A as no scenarios.\nJohnson et al. (2021): Fully met. All model parameters could be varied from “outside” the model code itself, as they were provided as changeable inputs to the model.\nHernandez et al. (2015): Partially met. Did not hard code runs, population, generations, and percent pre-screened. However, did hard code other parameters like bi-objective v.s tri-objective model and bounding. Also, it was pretty tricky to change percent pre-screened, as it assumed you provided a .txt file for each %.\nWood et al. (2021): Fully met. All model parameters for the scenarios/sensitivity analysis could be varied from “outside” the model code itself.\nReflections:\n\nIt can be quite difficult to change parameters that are hard coded into the model. Ideally, all the parameters that a user might want to change should be easily changeable and not hard coded.\nThis is a relatively common issue.\nThere is overlap between this and whether the code for scenarios is provided (as typically, the code for scenario is conditionally changing parameter values, although this can be facilitated by not hard coding the parameters, so you call need to change the values from “outside” the model code, rather than making changes to the model functions themselves). Hence, have included as two seperate reflections.\nImportant to note that we evaluate this in the context of reproduction - and have not checked for hard-coded parameters outside the specified scenario analyses, but that someone may wish to alter if reusing the model for a different analysis/context/purpose.\n\n\n\n\n\n\n\n\n\n\nUse relative file paths\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\nN/A\nN/A\n✅\n✅\nN/A\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Fully met. Just provides file path, so file is saved into current/run directory.\nHuang et al. (2019): Not applicable. All inputs defined within script. Outputs were not saved to file/s.\nLim et al. (2020): Not applicable. All inputs defined within script. Outputs were not saved to file/s.\nKim et al. (2021): Fully met. Uses relative file paths for sourcing model and input parameters (gets current directory, then navigates from there).\nAnagnostou et al. (2022): Fully met. Uses relative imports of local code files.\nJohnson et al. (2021): Not applicable. All inputs defined within script. Outputs are not specifically saved to a file (just that the .md and image files were automatically saved when the .Rmd file was knit). EpicR is package import.\nHernandez et al. (2015): Fully met. Creates folder in current working directory based on date/time to store results.\nWood et al. (2021): Fully met. Although I then changed things a bit as reorganised repository and prefer not to work with setwd(), these were set up in such a way that it would be really easy to correct file path, just by setting working directory at start of script.\nReflections:\n\nThis was not an issue for any studies - but included to note this was a “facilitator”, as would have needed to amend if they weren’t (and Tom noted that this is a common problem that he runs into elsewhere).\n\n\n\n\n\n\n\n\n\n\nAvoid large amounts of code duplication\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The model often contained very similar blocks of code before or after warm-up.\nHuang et al. (2019): Fully met.\nLim et al. (2020): Fully met.\nKim et al. (2021): Not met. There was alot of duplication when running each scenario (e.g. repeated calls to Eventsandcosts, and repeatedly defining the same parameters). This meant, if changing a parameter that you want to be consistent between all the scripts (e.g. number of persons), you had to change each of the scripts one by one.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): Not met. There was alot of duplication when running each scenario. This meant, when amending these for the sensitivity analysis, I would need to change the same parameter 12 times within the script, and for changes to all, changing it 12 times in 14 duplicate scripts. Hence, it was simpler to write an R script to do this than change it directly, but for base case, I had to make sure I carefully changed everything in both files.\nHernandez et al. (2015): Fully met.\nWood et al. (2021): Fully met.\nReflections: Large amounts of code duplication are non-ideal as they can:\n\nMake code less readable\nMake it trickier to change universal parameters\nIncrease the likelihood of introducing mistakes\nMake it trickier to set up scenarios/sensitivity analyses\n\n\n\n\n\n\n\n\n\n\nInclude sufficient comments in the code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n✅\n🟡\n🟡\n❌\n🟡\n❌\n\n\n\nShoaib and Ramamohan (2021) and Huang et al. (2019): Not met. Would have benefitted from more comments, as it took some time to ensure I have correctly understood code, particularly if they used lots of abbreviations.\nLim et al. (2020): Fully met. There were lots of comments in the code (including doc-string-style comments at the start of functions) that aided understanding of how it worked.\nKim et al. (2021): Partially met. Didn’t have any particular issues in working out the code. There are sufficient comments in the scenario scripts and at the start of the model scripts, although within the model scripts, there were sometimes quite dense sections of code that would likely benefit from some additional comments.\nAnagnostou et al. (2022): Partially met. Didn’t have to delve into the code much, so can’t speak from experience as to whether the comments were sufficient. From looking through the model code, several scripts have lots of comments and docstrings for each function, but some do not.\nJohnson et al. (2021): Not met. Very few comments in the Case_Detection_Results...Rmd files, which were the code files provided.\nHernandez et al. (2015): Partially met. There are some comments and doc-strings, but not comprehensively.\nWood et al. (2021): Not met. Very few comments, so for the small bit of the code that I did delve into, took a bit of working out what different variables referred to.\nReflections:\n\nWith increasing code complexity, the inclusion of sufficient comments becomes increasingly important, as it can otherwise be quite time consuming to figure out how to fix and change sections of code\nDefine abbreviations used within the code\nGood to have consistent comments and docstrings throughout (i.e. on all scripts, on not just some of them)\nCommon issue\nTom: I guess this one isn’t strictly necessary for reproducibility. The main issue was that the studies required a fair bit of manual work to get them to reproduce the results from teh mixed issues you listed above. This is sort of a “failsafe option” for reproducibility or perhaps more relevant for reuse/adaptation.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#run-time-and-memory-usage",
    "href": "pages/reflections.html#run-time-and-memory-usage",
    "title": "5  Reflections from reproductions",
    "section": "5.4 Run time and memory usage",
    "text": "5.4 Run time and memory usage\n\n\n\n\n\n\nQuicker models are easier to work with\n\n\n\n\n\nI have not evaluated like as a criteria, as a long run time is not inherently a bad thing. However, I definitely found that the run time of models had a big impact on how easy it was to reproduce results as longer run times meant it was tricky (or even impossible) to run in the first place, or tricky to re-run.\nThe studies where I made adjustments were:\n\nShoaib and Ramamohan (2021): Add parallel processing and ran fewer replications\nHuang et al. (2019): No changes made.\nLim et al. (2020): Add parallel processing\nKim et al. (2021): Reduced number of people in simulation, and switched from serial to the provided parallel option.\nAnagnostou et al. (2022): Model was super quick which made it really easy to run and re-run each time\nJohnson et al. (2021): Experimented with using a fewer number of agents for troubleshooting (although ultimately had to run with full number to reproduce results), and ran the scripts in parallel by opening seperate terminals simultaneously. Note: Long run time also meant it took a longer time to do this reproduction - although we excluded computation time in our timings, it just meant e.g. when I made a mistake in coding of scenario analysis and had to re-run, I had to wait another day or two for that to finish before I could resume.\nHernandez et al. (2015): Add parallel processing, did not run one of the scenarios (it was very long, and hadn’t managed to reproduce other parts of same figure regardless), and experimented with reducing parameters for evolutionary algorithm (but, in the end, ran with full parameters, though lower were helpful while working through and troubleshooting).\nWood et al. (2021): No changes made, but unlike other reproduction, didn’t try to run at smaller amounts - just set it to run as-is over the weekend.\n\nIn one of the studies, there was a minor error which needed fixing, which we anticipated to likely be present due to long run times meaning the model wasn’t all run in sequence at the end.\nReflections:\n\nReduce model run time if possible as it makes it easier to work with, and facilitates doing full re-runs of all scenarios (which can be important with code changes, etc).\n\nRelatedly, it is good practice to re-run all scripts before finishing up, as then you can spot any errors like the one mentioned for Kim et al. (2021)\n\nCommon issue (to varying degrees - i.e. taking 20 minutes, up to taking several hours or even day/s).\nTom: Long run times are inevitable for some models, but this does suggest that some extra work to build confidence the model is working is expected is beneficial, like one or a small set of verification scenarios that are quick to run.\n\n\n\n\n\n\n\n\n\n\nFor slow models, state the expected run time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n❌\n❌\nN/A\n✅\n🟡\n🟡\n\n\n\nShoaib and Ramamohan (2021): Fully met. Run time stated in paper (but not repository).\nHuang et al. (2019): Not met.\nLim et al. (2020): Not met.\nKim et al. (2021): Not met. A prior paper describing the model development mentions the run time, but not the current paper or repository, so this is easily missed.\nAnagnostou et al. (2022): Not applicable. Very quick! Seconds! So not particularly relevant - although, you could argue, potentially still important if there were some error that made it look like the model were running continuously (e.g. stuck in a loop) - although this is relatively unlikely.\nJohnson et al. (2021): Fully met. In the README, they state the the run time with 100 million agents is 16 hours, which was very handy to know, as I then just got stuck in running with fewer agents while troubleshooting.\nHernandez et al. (2015): Partially met. Some of the run times are mentioned in the paper, but not all, although this did help indicate that we would anticipate other s scenarios to similarly take hours to run.\nWood et al. (2021): Partially met. In the paper, they state that it takes less than five minutes for each scenario, but this feels like half the picture, given the total run time was 48 hours.\nReflections:\n\nFor long models with no statement, it can take a while to realise that it’s not an error in the code or anything, but actually just a long run time! And hard to know how long to expect, and whether it is without the capacities of your machine and so on.\nIdeally include statement of run time in repository as well as paper.\nIdeally include run time of all components of analysis (e.g. all scenarios).\nCommon issue.\nTom: This supports the inclusion of section 5.4 in the STRESS-DES guidelines\n\nResponse: But think it is also important that this is in the repository itself, and not just the paper.\n\n\n\n\n\n\n\n\n\n\n\nFor computationally expensive models, state memory usage and provide alternatives for lower spec machines\n\n\n\n\n\nAs I felt this was relatively subjective, as depending on what I felt to be “computationally expensive”, as I didn’t record the memory usage of all models, it felt unfair to do this as a checklist, and so have just informally noted below:\n\nShoaib and Ramamohan (2021), Huang et al. (2019), and Lim et al. (2020): Not applicable. Didn’t find it to be too computationally expensive for my machine.\nKim et al. (2021): Unable to run on my machine (serial took too long to run (would have to leave laptop on for many many hours which isn’t feasible), and parallel was too computationally expensive and crashed the machine (with the original number of people)). This is not mentioned in the repository or paper, but only referred to in a prior publication. Would’ve been handy if it included suggestions like reducing number of people and so on (which is what I had to do to feasibly run it).\nAnagnostou et al. (2022): Not applicable. Runs in seconds.\nJohnson et al. (2021): It becomes more computationally expensive if try to run lots at once in simultaneous terminals. Didn’t try running one on local machine with full parameter due to long run time making it infeasible, but knowing my system specs, it should have been able to if did.\nHernandez et al. (2015): This had long run times but I don’t know if it was computationally expensive or not - I just know that I didn’t run into any issues (but I didn’t record memory usage, so its possible a lower-specced machine might).\nWood et al. (2021): Not applicable. As stated in their prior paper, the model is constrained by processing time, not computer memory.\n\nReflections:\n\nSome models are so computationally expensive that it may be simply impossible to run it a feasible length of time without a high powered machine.\nHandy to mention memory requirements so someone with lower spec machine can ensure they would be able to run it.\nIf a model is computationally expensive, it would be good to provide suggested alternatives that allow it to be run on lower spec machines\nNot a common problem - only relevant to computationally expensive models\nTom: Agree it makes sense to report this, and is captured in reporting guidelines like STRESS-DES.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#parameters-scenarios-and-outputs",
    "href": "pages/reflections.html#parameters-scenarios-and-outputs",
    "title": "5  Reflections from reproductions",
    "section": "5.5 Parameters, scenarios and outputs",
    "text": "5.5 Parameters, scenarios and outputs\n\n\n\n\n\n\nProvide code for all scenarios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\nN/A\n🟡\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. There were several instances where it took quite a while to understand how and where to modify the code in order to run scenarios (e.g. no arrivals, transferring admin work, reducing doctor intervention in deliveries).\nHuang et al. (2019): Not met. Set up a notebook to programmatically run the model scenarios. It took alot of work to modify and write code that could run the scenarios, and I often made mistakes in my interpretation for the implementation of scenarios, which could be avoided if code for those scenarios was provided.\nLim et al. (2020): Not met. Several parameters or scenarios were not incorporated in the code, and had to be added (e.g. with conditional logic to skip or change code run, removing hard-coding, adding parameters to existing).\nKim et al. (2021): Not met. Took alot of work to change model from for loop to function, to set all parameters as inputs (some were hard coded), and add conditional logic of scenarios when required.\nAnagnostou et al. (2022): Not applicable. No scenarios.\nJohnson et al. (2021): Partially met. Has all base case scenarios, but not sensitivity analysis.\nHernandez et al. (2015): Not met. Took a while to figure out how to implement scenarios.\nWood et al. (2021): Fully met.\nReflections:\n\nCommon issue\nTime consuming and tricky to resolve\nTom: This is a headline. Also, links to importance of reproducible analytical pipelines (RAP) for simulation.\n\n\n\n\n\n\n\n\n\n\nAll the required outputs are calculated/provided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. Had to add some outputs and calculations (e.g. proportion of childbirth cases referred, standard deviation)\nHuang et al. (2019): Not met. It has a complicated output (standardised density of patient in queue) that I was never certain on whether I correctly calculated. Although it outputs the columns required to calculate it, due its complexity, I feel this was not met, as it feels like a whole new output in its own right (and not just something simple like a mean).\nLim et al. (2020): Not met. The model script provided was only set up to provide results from days 7, 14 and 21. The figures require daily results, so I needed to modify the code to output that.\nKim et al. (2021): Not met. Had to write code to find aorta sizes of people with AAA-related deaths.\nAnagnostou et al. (2022): Fully met. Although worth noting this only had one scenario/version of model and one output to reproduce.\nJohnson et al. (2021): Note met. It has an output that is in “per 1000” and, although it outputs the columns required to calculate this, I found it very tricky to work out which columns to use and how to transform them to get this output, and so feel this is not met (as feels like a seperate output, and not something simple like a mean, and as it felt so tricky to work out).\nHernandez et al. (2015): Fully met.\nWood et al. (2021): Fully met.\nReflections:\n\nCalculate and provide all the outputs required\nAppreicate this can be a bit “ambiguous” (e.g. if its just plotting a mean or simple calculation, then didn’t consider that here) (however, combined with other criteria, we do want them to provide code to calculate outputs, so we would want them to provide that anyway)\nTom: This is a headline. I suspect we can find supporting citations elsewhere from other fields. Its a reporting guideline thing too, but in natural language things can get very ambiguous still! Would be good to make that point as well I think.\n\n\n\n\n\n\n\n\n\n\nInclude correct parameters in the script (even if just for one scenario)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n🟡\n❌\n🟡\n✅\n✅\n✅\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Partially met. Script is set with parameters for base configuration 1, with the exception of number of replications.\nHuang et al. (2019): Not met. The baseline model in the script did not match the baseline model (or any scenario) in the paper, so had to modify parameters.\nLim et al. (2020): Partially met. The included parameters were corrected, but the baseline scenario included varying staff strength to 2, and the provided code only varied 4 and 6. I had to add some code that enabled it to run with staff strength 2 (as there were an error that occured if you tried to set that).\nKim et al. (2021): Fully met.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): Fully met. Base case parameters all correct.\nHernandez et al. (2015): Not met. As agreed with the author, this is likely the primary reason for the discrepancy in these results - they are very close, and we see similar patterns, but not reproduced. Unfortunately, several parameters were wrong, and although we changed those we spotted, we anticipate there could be others we hadn’t spotted that might explain the remaining discrepancies.\nWood et al. (2021): Fully met.\nReflections:\n\nAt least provide a script that can run the baseline model as in the paper (even if not providing the scenarios)\nThis can introduce difficulties - when some parameters are wrong, you rely on the paper to check which parameters are correct or not, but if the paper doesn’t mention every single parameter (which is reasonably likely, as this includes those not varied by scenarios), then you aren’t able to be sure that the model you are running is correct.\nThis can make a really big difference, and be likely cause of managing to reproduce everything v.s. nothing, if it impacts all aspects of the results.\nTom: I think this comes back to minimum verification as well. I think the “at least for one scenario” idea of yours is excellent.\n\n\n\n\n\n\n\n\n\n\nProvide all the required parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n✅\n✅\n✅\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Some parameters that could not be calculated were not provided - ie. what consultation boundaries to use when mean length of doctor consultation was 2.5 minutes\nHuang et al. (2019): Not met. In this case, patient arrivals and resource numbers were listed in the paper, and there were several discprepancies between this and the provided code. However, for many of the model parameters like length of appointment, these were not mentioned in the paper, and so it was not possible to confirm whether or not those were correct. Hence, marked as not met, as the presence of discrepenancies for several other parameters puts these into doubt.\nLim et al. (2020): Not met. For Figure 5, had to guess the value for staff_per_shift.\nKim et al. (2021): Fully met.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): Fully met. Could determine appropriate parameters for sensitivity analysis from figures in article.\nHernandez et al. (2015): Not met. The results have a large impact by the bounding set, but this was not mentioned in the paper or repository, and required me looking at the numbers in results and GitHub commit history to estimate the appropriate bounds to use.\nWood et al. (2021): Fully met.\nReflections:\n\nProvide all required parameters\nTom: Evidence to support STRESS-DES 3.3\n\n\n\n\n\n\n\n\n\n\nIf not provided in the script, then clearly present all parameters in the paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\nN/A\nN/A\n✅\n🟡\nN/A\n\n\n\nShoaib and Ramamohan (2021): Not met. Although there was a scenario table, this did not include all the parameters I would need to change. It was more challenging to identify parameters that were only described in the body of the article. There were also some discrepancies in parameters between the main text of the article, and the tables and figures. Some scenarios were quite ambiguous/unclear from their description in the text, and I initially misunderstood the required parameters for the scenarios.\nHuang et al. (2019): Not met. As described above, paper didn’t adequately describe all parameters.\nLim et al. (2020): Partially met. Nearly all parameters are in the paper table, and others are described in the article. However, didn’t provide information for the staff_per_shift for Figure 5.\nKim et al. (2021) and Anagnostou et al. (2022): Not applicable. All provided.\nJohnson et al. (2021): Fully met. All parameters clearly in the two figures presenting the sensitivity analysis, and didn’t have to look elsewhere beyond that.\nHernandez et al. (2015): Most parameters are relatively easily identified from the text or figure legends (though would be easier if provided in a table or similar). Parameter for bounding was not provided in paper.\nWood et al. (2021): Not applicable. All provided.\nReflections:\n\nProvide parameters in a table (including for each scenario), as it can be difficult/ambiguous to interpret them from the text, and hard to spot them too.\n\nTom: The ambiguity of the natural language for scenarios was an important finding.\n\nBe sure to mention every parameter that gets changed (e.g. for Lim et al. (2020), as there wasn’t a default staff_per_shift across all scenarios, but not stated for the scenario, had to guess it).\nTom: Evidence to support STRESS-DES 3.3\n\n\n\n\n\n\n\n\n\n\nIf will need to process parameters, provide required calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n\n\n\nShoaib and Ramamohan (2021): Not met. It was unclear how to estimate inter-arrival time.\nHuang et al. (2019): Fully met. The calculations for inter-arrival times were provided in the code, and the inputs to the code were the number of arrivals, as reported in the paper, and so making it easy to compare those parameters and check if numbers were correct or not.\nLim et al. (2020): Not applicable. The parameter not provided is not one that you would calculate.\nKim et al. (2021) and Anagnostou et al. (2022): Not applicable. All provided.\nJohnson et al. (2021): Not applicable. No processing of parameters required.\nHernandez et al. (2015): Not applicable.\nWood et al. (2021): Not applicable. All provided.\nReflections:\n\nIf you are going to be mentioning the “pre-processed” values at all, then its important to include the calculation (ideally in the code, as that is the clearest demonstration of exactly what you did)\nTom: This is a very good point for RAP.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#output-format",
    "href": "pages/reflections.html#output-format",
    "title": "5  Reflections from reproductions",
    "section": "5.6 Output format",
    "text": "5.6 Output format\n\n\n\n\n\n\nSaves output to a file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n❌\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Fully met. Outputs to .xlsx files\nHuang et al. (2019), Lim et al. (2020), and Kim et al. (2021): Not met. Outputs to dataframe/s.\nAnagnostou et al. (2022): Outputs to OUT_STATS.csv. Note: Although not needed for the reproduction itself, when I tried to amend the name and location of the csv file output the model for use in tests, this was very tricky to do as it was hard coded into the scripts and I found difficult to amend due to how the model is run and set up.\nJohnson et al. (2021): Not met. Outputs to dataframe/s that are not saved as files (although can see within the kept .md file from knitting).\nHernandez et al. (2015): Fully met. Outputs to .txt files.\nWood et al. (2021): Fully met. Outputs to .csv files.\nReflections:\n\nCommon issue\nParticularly important if model run time is even slightly long (even just minutes long, but even more so as becomes many minutes / hours), so don’t always have to re-run it each time to get results\nSet up this in such a way that it is easy to change the name and location of the output file.\n\n\n\n\n\n\n\n\n\n\nUnderstandable output tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. There were two alternative results spreadsheets with some duplicate metrics but sometimes differing results between them, which made it a bit confusing to work out what to use.\nHuang et al. (2019), Lim et al. (2020), and Anagnostou et al. (2022): Fully met. Didn’t experience issues interpreting the contents of the output table/s.\nKim et al. (2021): Not met. It took me a little while to work out what surgery columns I needed, and to realise I needed to combine two of them. This required looking at what inputs genreated this, and referring to a input data dictionary.\nJohnson et al. (2021): Not met. I had mistakes and confusion figuring out which results tables I needed, which columns to use, and which scenarios to use from the tables.\nHernandez et al. (2015): Fully met. Straightforward with key information provided.\nWood et al. (2021): Fully met. I didn’t need to work with the output tables, but from looking at them now, they make sense.\nReflections:\n\nDon’t provide alternative results for the same metrics\nMake it clear what each colum/category in the results table means, if it might not be immediately clear.\nMake differences between seperate results tables clear.\nTom: In a RAP for simulation world we have a env + model + script that gets you to the exact results table you see in the paper and this isn’t a problem (although more time consuiming to setup).\n\n\n\n\n\n\n\n\n\n\nAvoid large file sizes if possible\n\n\n\n\n\nI have not evaluated like as a criteria, as a large file size is not inherently a bad thing, and might be difficult to avoid. However, when files are very large, this can make things trickier, such as with requiring compression and use of GitHub Large File Storage (LFS) for tracking, which has limits on the free tier.\nRegarding file sizes in each study:\n\nShoaib and Ramamohan (2021): Not relevant (results files &lt;35 kB)\nHuang et al. (2019): Provided code didn’t save results to file. When I saved to file, these were large, so I compressed to .csv.gz, which made them small enough that GitHub was still happy (26 MB).\nLim et al. (2020): Provided code didn’t save results to file. When I saved to file, these were small, so not relevant (results files &lt;60 kB)\nKim et al. (2021): Provided code didn’t save results to file. When I saved to file, these were small, so not relevant (results files &lt;1 kB)\nAnagnostou et al. (2022): Not relevant (results file 34 kB)\nJohnson et al. (2021): Provided code didn’t save results to file. When I saved to .csv files, these were small, so not relevant (results files &lt;3.6kB)\nHernandez et al. (2015): Not relevant (aggregate results files &lt;10 kB).\nWood et al. (2021): Aggregate results files are small (327 kB), but raw results files are very large (2.38 GB), and even when compressed to .csv.gz (128 MB) require using of GitHub LFS\n\nReflections:\n\nFor most studies, this was not relevant, with outputs relatively small.\nI only really found this to be an issue when files exceeded GitHub threshold. GitHub give warning over 50 MB, blocks files over 100 MB, requiring you to use GitHub LFS. Recommends repositories ideally &lt;1 GB and for sure &lt; 5GB. GitHub LFS has limits on storage and bandwith use (1GB of each).\nReducing file size isn’t the only solution. In cases where you have large files, a good option can be storing it elsewhere and then pulling from there into your workflow - for example, storing in zenodo and fetching using pooch.\n\nTom: Approach with zenodo seems sensible. There’s a simple workflow for submitting to a journal with this approach as well. Plus scenarios/scripts can link to raw datafiles on zenodo for comparion.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#seeds",
    "href": "pages/reflections.html#seeds",
    "title": "5  Reflections from reproductions",
    "section": "5.7 Seeds",
    "text": "5.7 Seeds\n\n\n\n\n\n\nUse seeds to control stochasticity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n✅\n✅\n✅\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The lack of seeds wasn’t actually a barrier to the reproduction though due to the replication number. I later add seeds so my results could be reproduced, and found that the ease of setting seeds with salabim was a greater facilitator to the work. I only had to change one or two lines of code to then get consistent results between runs (unlike other simulation software like SimPy where you have to consider the use of seeds by different sampling functions). Moreover, by default, salabim would have set a seed (although overridden by original authors to enable them to run replications).\nHuang et al. (2019): Not met. It would have been beneficial to include seeds, as there was a fair amount of variability, so with seeds I could then I could be sure that my results do not differ from the original simply due to randomness.\nLim et al. (2020): Not met. The results obtained looked very similar to the original article, with minimal differences that I felt to be within the expected variation from the model stochasticity. However, if seeds had been present, we would have been able to say with certainty. I did not feel I needed to add seeds during the reproduction to get the same results.\nKim et al. (2021): Fully met. Included a seed, although I don’t get identical results as I had to reduce number of people in simulation.\nAnagnostou et al. (2022): Fully met. The authors included a random seed so the results I got were identical to the original (so no need for any subjectivity in deciding whether its similar enough, as I could perfectly reproduce).\nJohnson et al. (2021): Fully met. At start of script, authors set.seed(333).\nHernandez et al. (2015): Fully met. This ensured consistent results between runs of the script, which was really helpful.\nWood et al. (2021): Fully met. Sets seed based on replication number.\nReflections:\n\nDepending on your model and the outputs/type of output you are looking at, the lack of seeds can have varying impacts on the appearance of your results, and can make the subjective judgement of whether results are consistent harder (if discrepancies could be attributed to not having consistent seeds or not).\nIt can be really quite simple to include seeds.\nOver half of the studies did include seed control in their code.\nTom: There seems little argument against doing this. worth noting that commerical software does this for you and possibly explains why authors didn’t do this themselves if that was their background (lack of knowledge?).\nTom: Note simpy is independent of any sampling mechanism. We could just use python’s random module and set a single seed if needed (although you lose CRN) and we can setup our models so that we only need to set a single seed.\nTom: A key part of STARS 2.0 for reproducibility",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#code-to-produce-article-results",
    "href": "pages/reflections.html#code-to-produce-article-results",
    "title": "5  Reflections from reproductions",
    "section": "5.8 Code to produce article results",
    "text": "5.8 Code to produce article results\nThis is a common problem across all item types, and a key part of STARS 2.0 for reproducibility.\n\n\n\n\n\n\nProvide code to process results into tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\nN/A\n🟡\n❌\nN/A\n❌\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met.\nHuang et al. (2019): Not applicable. No tables in scope.\nLim et al. (2020): Partially met. It outputs the results in a similar structure to the paper (like a section of a table). However, it doesn’t have the full code to produce a table outright, for any of the tables, so additional processing still required.\nKim et al. (2021): Not met. Had to write code to generate tables, which included correctly implementing calculation of excess e.g. deaths, scaling to population size, and identify which columns provide the operation outcomes.\nAnagnostou et al. (2022): Not applicable. No tables in scope.\nJohnson et al. (2021): Not met. Had to write code to generate tables, which took me a while as I got confused over thinks like which tables / columns / scenarios to use.\nHernandez et al. (2015): Not met.\nWood et al. (2021): Fully met.\nReflections:\n\nIt can take a bit of time to do this processing, and it can be tricky/confusing to do correctly, so very handy for it to be provided.\nCommon issue.\n\n\n\n\n\n\n\n\n\n\nProvide code to process results into figures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\n❌\n🟡\n🟡\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met.\nHuang et al. (2019): Not met. Had to write code from scratch. For one of the figures, it would have been handy if informed that plot was produced by a simmer function (as didn’t initially realise this). It also took a bit of time for me to work out how to transform the figure axes as this was not mentioned in the paper (and no code was provided for these). It was also unclear and a bit tricky to work out how to standardise the density in the figures (since it is only described in the text and no formula/calculations are provided there or in the code).\nLim et al. (2020), Kim et al. (2021) and Anagnostou et al. (2022): Not met. However, the simplicity and repetition of the figures was handy.\nJohnson et al. (2021): Partially met. For Figure 3, most of the required code for the figure was provided, which was super helpful. However, this wasn’t complete, and for all others figures, I had to start from scratch writing the code.\nHernandez et al. (2015): Partially met. Provides a few example ggplots, but these are not all the plots, nor exactly matching article, nor including any of the pre-processing required before the plots, and so could only serve as a starting point (though that was still really handy).\nWood et al. (2021): Fully met. Figures match article, with one minor exception that I had to add smoothing to the lines on one of the figures.\nReflections:\n\nIt can take a bit of time to do this processing, particularly if the figure involves any transformations (and less so if the figure is simple), so very handy for it to be provided.\nAlso, handy if the full code can be provided for all figures (although partial code is more helpful than none at all).\nCommon issue.\n\n\n\n\n\n\n\n\n\n\nProvide code to calculate in-text results\n\n\n\n\n\nBy “in-text results”, I am referred to results that are mentioned in the text but not included in/cannot be deduced from any of the tables or figures.\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\nN/A\n❌\nN/A\nN/A\nN/A\nN/A\n\n\n\nShoaib and Ramamohan (2021), Huang et al. (2019), Kim et al. (2021): Not met.\nLim et al. (2020), Anagnostou et al. (2022), Johnson et al. (2021), Hernandez et al. (2015), Wood et al. (2021): Not applicable (no in-text results).\nReflections:\n\nProvide code to calculate in-text results\nUniversal issue, for those with in-text results not otherwise captured in tables and figures",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#documentation",
    "href": "pages/reflections.html#documentation",
    "title": "5  Reflections from reproductions",
    "section": "5.9 Documentation",
    "text": "5.9 Documentation\n\n\n\n\n\n\nInclude instructions on how to run the model/script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n🟡\n✅\n✅\n❌\n❌\n\n\n\nShoaib and Ramamohan (2021): Not met. No instructions, although is just a single script that you run.\nHuang et al. (2019): Not met. Not provided in runnable form but, regardless, no instructions for running it as it is provided (as a web application - i.e. no info on how to get that running).\nLim et al. (2020): Not met. No instructions, although is just a single script that you run.\nKim et al. (2021): Partially met. README tells you which folder has the scripts you need, although nothing further. Although all you need to do is run them.\nAnagnostou et al. (2022): Fully met. Clear README with instructions on how to run the model was really helpful.\nJohnson et al. (2021): Fully met. README has mini description of model and clear instructions on how to install and run the model.\nHernandez et al. (2015): Not met.\nWood et al. (2021): Not met. No instructions, although it was fairly self explanatory (single script master.R to run, then processing scripts named after items in article e.g. fig7.R).\nReflections:\n\nEven if as simple as running a script, include instructions on how to do so\nIn simpler projects (e.g. single script), this can be less of a problem.\nCommon issue\nTom: Evidence for STARS essential component of minimum documentation.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#other",
    "href": "pages/reflections.html#other",
    "title": "5  Reflections from reproductions",
    "section": "5.10 Other",
    "text": "5.10 Other\nGrid lines. Include tick marks/grid lines on figures, so it is easier to read across and judge whether a result is above or below a certain Y value.\nData dictionaries. Anagnostou et al. (2022): Included data dictionary for input parameters. Although I didn’t need this, this would have been great if I needed to change the input parameters at all.\nUnsupported versions. Hernandez et al. (2015): Due to the age of the work:\n\nSome packages were no longer available on Conda and had to be installed from PyPI\nThe version of python was no longer supported which meant:\n\nNot supported by Jupyter Lab and Jupyter Notebook (so no .ipynb files, or Jupyter Lab on Docker)\nNot supported by VSCode (so had to use “tricks” to run it, involving using a pre-release version of Python on VSCode)\nHad to create Docker image from scratch (i.e. couldn’t start from e.g. miniconda3)\n\n\nHowever, this is a slightly unavoidable problem unless you continue to maintain your code (which is ideal but not always feasible in a research environment). Realistically, if reusing this code for a new purpose, you would upgrade it to supported versions.\nTom: This is interesting - and you wonder if it would still be possible (given the “tricks” I followed) in another 10 years time.\nOriginal results files. Hernandez et al. (2015): Included some original results files, which was invaluable in identifying some of the parameters in the code that needed to be fixed.\nExcessive number of files. For Hernandez et al. (2015) the default behaviour of the script was to output lots of files from each round (so you could easily have 90, 100, 200+ files), which were then not used in analysis (as it just depended on an aggregate results file). Although these individual files might be useful during quality control, as a default behaviour of the script, it could easily make the repository quite busy/littered with files.\n\nTom reflected that this is more of a general housekeeping issue. He agrees and says its ok to do this, but perhaps they need a run mode that does not produce these verification files\n\nClasses. Hernandez et al. (2015): Structured code into classes, which was nice to work with/amend.\nVersion history and releases. Johnson et al. (2021) had commits to their GitHub repository after the publication date. It wasn’t clear which version aligned with the publication. However, the most recent commits add clear README instructions to the repository. We decided to use the latest version of the repository, but it would have beneficial to have releases/versions/a change log that would help to outline the commit history in relation to the publication and any subsequent changes.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#references",
    "href": "pages/reflections.html#references",
    "title": "5  Reflections from reproductions",
    "section": "5.11 References",
    "text": "5.11 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html",
    "href": "pages/repo_evaluation.html",
    "title": "6  Evaluation of the repository",
    "section": "",
    "text": "6.1 Summary\nUnique badge criteria:\nBadges:\nEssential components of STARS framework:\nOptional components of STARS framework:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#summary",
    "href": "pages/repo_evaluation.html#summary",
    "title": "6  Evaluation of the repository",
    "section": "",
    "text": "Reflections\n\n\n\n\n\nFour studies met 25% of criteria, and ranged from 12.5% to 100% reproduced.\nTwo studies met 33% of criteria, and these were 80% and 100% reproduced.\nThe remaining two studies were fully reproduced and met 41.7% and 83.3% of criteria.\nHowever, I think it is more meaningful to actually look at what criteria were and were not met.\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nNot certain how meaningful these numbers are, as we have imbalanced numbers of different types of badge, and meeting certain popular criteria will weight what is met v.s. not.\nFeel that looking at the criteria met is a bit more meaningful? And then specific examples of how that translates into badges - e.g.\n\nNone meeting ACM “Artifacts Evaluated - Functional” as requires xyz and these are commonly not met.\nFor several, they meet three badges, but those three badges have one criteria: reproducing results.\n\nAlso, it’s important to remember here that the criteria used for these were based on what could be found about each badge online, but we likely differences in our procedure (e.g. allowed troubleshooting for execution and reproduction, not under tight time pressure to complete). Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals (and, in fact, know likely not for many).\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nSimilar to journal criteria (unsurprisingly, as looking at similar things) - most studies meet very few and have wide range of reproduction success, from 12.5% to 100%. Three met more, and these were 80% to 100% reproduced.\nI think, if we were to draw anything from this, it would be to reflect on exactly what criteria were and were not met, and why/how that impacted reproduction, in any way (either success or time).\nNote: Just considers those fully met, in plot\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nThis highlights how Huang meets the most criteria, but is only partially reproduced - but I think it is most interesting to consider why this is.\nNote: Just considers those fully met, in plot",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#journal-badges",
    "href": "pages/repo_evaluation.html#journal-badges",
    "title": "6  Evaluation of the repository",
    "section": "6.2 Journal badges",
    "text": "6.2 Journal badges\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\nIn this section and below, the criteria for each study are marked as either being fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A).\nUnique criteria:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nCriteria related to how artefacts are shared\n\n\n\n\n\n\n\n\n\n\n\nStored in a permanent archive that is publicly and openly accessible\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nHas a persistent identifier\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nIncludes an open licence\n❌\n✅\n❌\n✅\n✅\n✅\n❌\n❌\n\n\n\nCriteria related to what artefacts are shared\n\n\n\n\n\n\n\n\n\n\n\nArtefacts are relevant to and contribute to the article’s results\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nComplete set of materials shared (as would be needed to fully reproduce article)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\nCriteria related to the structure and documentation of the artefacts\n\n\n\n\n\n\n\n\n\n\n\nArtefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n❌\n❌\n❌\n✅\n✅\n❌\n✅\n❌\n\n\n\nArtefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nArtefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nArtefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nCriteria related to running and reproducing results\n\n\n\n\n\n\n\n\n\n\n\nScripts can be successfully executed\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nIndependent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nReproduced within approximately one hour (excluding compute time)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nStored in a permanent archive that is publicly and openly accessible: Fulfillment doesn’t impact reproduction as I was able to get everything needed from the remote code repository (GitHub or GitLab). However, if these had been deleted from GitHub, it would have become invaluable.\nHas a persistent identifier: Fulfillment doesn’t impact reproduction.\nIncludes an open licence: This had a big impact on our ability to complete reproductions, as we had to ask authors to add an open licence to their work, to enable us to use it. Gladly, all authors we contacted kindly add these on request. However, it’s worth noting that this was a relatively common issue, and one of the most important, since it completely prevents reuse if excluded.\nArtefacts are relevant to and contribute to the article’s results: All met (if not met, this would be a massive hindrance).\nComplete set of materials shared (as would be needed to fully reproduce article): This had a really big impact on the reproduction. The main reason for longer times in reproduction was (a) code for scenarios not provided, and (b) code to process results into figures and tables not provided.\nArtefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community): In cases where this was not met, this was often related to hard-coding of parameters, or set up of code in a way that - alike hard-coding - made it much harder to reuse the model (in this context, to reuse them between different scenarios) - or quite a busy/cluttered repository which was confusing to navigate.\nArtefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions): Only three studies had any documentation - READMEs for Kim et al. (2021), Anagnostou et al. (2022) and Johnson et al. (2021) - but this criteria also required “package versions” which were uncommon. However, focusing on the READMEs, in each of these cases it was great to have these, guiding on how to run the scripts, or on what each folder/file in the repository is.\nArtefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose): In Anagnostou et al. (2022), they include a file CHARM_INFO.md alongside their README which walks through the input parameters for the model. I didn’t need to change any of these for the reproduction, but would imagine this is to be very helpful if someone were to reuse the model.\nArtefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript: Whilst this was true for Anagnostou et al. (2022), it should be noted that this was a very simple example, just requiring to run one script which quickly reproduces everything! I had been a bit uncertain on it, since the README doesn’t explicitly say how to make the figure, but it does provide instructions that lead you to regenerate the exact model results from the paper, and so I feel that it does provide instructions to reproduce results sufficiently (although would be more complete to include instructions for figure too - so if it weren’t a yes/no decision for badges, I would’ve said this was partially met). Ideally, studies would clearly outline how to reproduce results in full.\nScripts can be successfully executed: This is true, though I did allow troubleshooting, which sometimes took a long while. Hence, the importance of e.g. environments and scripts being provided in a runnable format (both covered on the reflections page), since these are the hurdles to successfully executing scripts.\nIndependent party regenerated results using the authors research artefacts: On the reproduction page, I reflected (where possible) on what I thought the primary reasons were, for cases where I didn’t manage to reproduce results despite troubleshooting.\nReproduced within approximately one hour (excluding compute time): In this study, this was pretty much impossible, since I followed a protocol of first setting up, reading the article, and so on. It is worth noting however that there were two studies that were quite quick to run, which I reflect about on the reproduction page.\n\n\n\nBadges:\nThe badges are grouped into three categories:\n\n“Open objects” badges: These badges relate to research artefacts being made openly available.\n“Object review” badges: These badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n“Reproduced” badges: These badges relate to an independent party regenerating the reuslts of the article using the author objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\n“Open objects” badges\n\n\n\n\n\n\n\n\n\n\n\nNISO “Open Research Objects (ORO)”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier• Includes an open licence\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nNISO “Open Research Objects - All (ORO-A)”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier• Includes an open licence• Complete set of materials shared (as would be needed to fully reproduce article)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nACM “Artifacts Available”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nCOS “Open Code”• Stored in a permanent archive that is publicly and openly accessible• Has a persistent identifier• Includes an open licence• Complete set of materials shared (as would be needed to fully reproduce article)• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nIEEE “Code Available”• Complete set of materials shared (as would be needed to fully reproduce article)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n“Object review” badges\n\n\n\n\n\n\n\n\n\n\n\nACM “Artifacts Evaluated - Functional”• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)• Artefacts are relevant to and contribute to the article’s results• Complete set of materials shared (as would be needed to fully reproduce article)• Scripts can be successfully executed\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nACM “Artifacts Evaluated - Reusable”• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)• Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)• Artefacts are relevant to and contribute to the article’s results• Complete set of materials shared (as would be needed to fully reproduce article)• Scripts can be successfully executed• Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nIEEE “Code Reviewed”• Complete set of materials shared (as would be needed to fully reproduce article)• Scripts can be successfully executed\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n“Reproduced” badges\n\n\n\n\n\n\n\n\n\n\n\nNISO “Results Reproduced (ROR-R)”• Independent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nACM “Results Reproduced”• Independent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nIEEE “Code Reproducible”• Independent party regenerated results using the authors research artefacts\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nPsychological Science “Computational Reproducibility”• Independent party regenerated results using the authors research artefacts• Reproduced within approximately one hour (excluding compute time)• Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)• Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nOnly one study had permanent archive (with persistent identifier), hence one being awarded NISO “Open Research Objects (ORO)” and ACM “Artifacts Available”. This was required by two other badges as well as sufficient documentation and/or complete set of materials. Since the one permanently archived study didn’t meet these criteria, none were awarded the two badges: NISO “Open Research Objects - All (ORO-A)” or COS “Open Code”.\nA complete set of materials was required by IEEE “Code Available” and IEEE “Code Reviewed” - but this was only met by one study, as studies commonly did not include code for scenarios or creation of figures and tables. It was also required by ACM “Artifacts Evaluated - Functional and Reusable” badges, but since that one study didn’t meet their documentation requirements, none were awarded those badges.\nThree badges had one criteria: reproduction of results - and hence, several studies received these (NISO “Results Reproduced (ROR-R)”, ACM “Results Reproduced”, IEEE “Code Reproducible”). However, it’s worth noting that we allowed troubleshooting (since that was how we approached the reproductions), and that some of these studies might not have received these badges, if they have any requirements that exclude troubleshooting (which is likely).\nThe final badge Psychological Science “Computational Reproducibility” had several criteria, but of importance, one of those was to complete the reproduction within an hour. This was somewhat impossible in our procedures, since we read the article and set-up etc beforehand, and so none were awarded this badge.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#stars-framework",
    "href": "pages/repo_evaluation.html#stars-framework",
    "title": "6  Evaluation of the repository",
    "section": "6.3 STARS framework",
    "text": "6.3 STARS framework\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nEssential components\n\n\n\n\n\n\n\n\n\n\n\nOpen licenceFree and open-source software (FOSS) licence (e.g. MIT, GNU Public Licence (GPL))\n❌\n✅\n❌\n✅\n✅\n✅\n❌\n❌\n\n\n\nDependency managementSpecify software libraries, version numbers and sources (e.g. dependency management tools like virtualenv, conda, poetry)\n❌\n❌\n❌\n🟡\n✅\n🟡\n❌\n❌\n\n\n\nFOSS modelCoded in FOSS language (e.g. R, Julia, Python)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nMinimum documentationMinimal instructions (e.g. in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments\n❌\n❌\n❌\n✅\n✅\n🟡\n❌\n❌\n\n\n\nORCIDORCID for each study author\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nCitation informationInstructions on how to cite the research artefact (e.g. CITATION.cff file)\n❌\n❌\n❌\n❌\n✅\n✅\n❌\n❌\n\n\n\nRemote code repositoryCode available in a remote code repository (e.g. GitHub, GitLab, BitBucket)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nOpen science archiveCode stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g. Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net))\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nOptional components\n\n\n\n\n\n\n\n\n\n\n\nEnhanced documentationOpen and high quality documentation on how the model is implemented and works (e.g. via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:• Plain english summary of project and model• Clarifying licence• Citation instructions• Contribution instructions• Model installation instructions• Structured code walk through of model• Documentation of modelling cycle using TRACE• Annotated simulation reporting guidelines• Clear description of model validation including its intended purpose\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nDocumentation hostingHost documentation (e.g. with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nOnline coding environmentProvide an online environment where users can run and change code (e.g. BinderHub, Google Colaboratory, Deepnote)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nModel interfaceProvide web application interface to the model so it is accessible to less technical simulation users\n❌\n✅\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nWeb app hostingHost web app online (e.g. Streamlit Community Cloud, ShinyApps hosting)\n❌\n✅\n❌\n❌\n🟡\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nThese topics were covered in the badge criteria reflections: open licence, minimum documentation, and open science archive.\nDependency management: This was pretty uncommon, and often took some troubleshooting at the start, to figure out which packages were needed, and certain versions.\nFOSS model: All met as requirement of our reproduction.\nORCID and citation information: Doesn’t impact reproduction in this case - but:\n\nWe do go to these from having found an article. I was choosing repositories that I had found from papers, so I already at least knew who the paper authors were.\nIn all cases, I emailed the authors, which requires finding contact information (generally via paper, sometimes from googling them to find new emails).\nAny attempted citation of the repository itself would’ve necessarily been correct, depending on whether the author list would be the same as in the paper, if you relied on the paper without citation information.\n\nRemote code repository: All met, most common way to share code.\nEnhanced documentation: Only three studies had any documentation, and neither met these extensive requirements. I anticipate - if any had met this - it would’ve made the reproduction very quick and easy!\nDocumentation hosting: Not applicable, given only basic documentation.\nOnline coding environment: None provided. I always intended to run on my own machine, so this might not have had much bearing in my case if provided, but would moreso for people who perhaps didn’t have Python or R installed, and hopefully would have bypassed environment troubleshooting issues.\nModel interface: Two studies had applications, although in both cases, these weren’t “outcomes” in scope of reproduction, nor did they produce them.\nWeb app hosting: This was quite important. Both apps had been hosted, but one was hosted with a site that is no longer operational. In both cases, the app wasn’t in “scope” although I did still view it and look into it for one as it was hosted and so could very easily - but for the other, I didn’t view it, as I didn’t go through the steps of running it locally, since it wasn’t the focus.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#timings",
    "href": "pages/repo_evaluation.html#timings",
    "title": "6  Evaluation of the repository",
    "section": "6.4 Timings",
    "text": "6.4 Timings\n\nShoaib and Ramamohan (2021) - 30m\nHuang et al. (2019) - 17m\nLim et al. (2020) - 18m\nKim et al. (2021) - 18m\nAnagnostou et al. (2022) - 19m\nJohnson et al. (2021) - 20m\nHernandez et al. (2015) - 13m\nWood et al. (2021) - 14m\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nNo particular comments, don’t think we learn much from the timings here.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#badge-sources",
    "href": "pages/repo_evaluation.html#badge-sources",
    "title": "6  Evaluation of the repository",
    "section": "6.5 Badge sources",
    "text": "6.5 Badge sources\nNational Information Standards Organisation (NISO) (NISO Reproducibility Badging and Definitions Working Group (2021))\n\n“Open Research Objects (ORO)”\n“Open Research Objects - All (ORO-A)”\n“Results Reproduced (ROR-R)”\n\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\n\n“Artifacts Available”\n“Artifacts Evaluated - Functional”\n“Artifacts Evaluated - Resuable”\n“Results Reproduced”\n\nCenter for Open Science (COS) (Blohowiak et al. (2023))\n\n“Open Code”\n\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\n\n“Code Available”\n“Code Reviewed”\n“Code Reproducible”\n\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\n“Computational Reproducibility”",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#references",
    "href": "pages/repo_evaluation.html#references",
    "title": "6  Evaluation of the repository",
    "section": "6.6 References",
    "text": "6.6 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nAssociation for Computing Machinery (ACM). 2020. “Artifact Review and Badging Version 1.1.” ACM. https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nAssociation for Psychological Science (APS). 2023. “Psychological Science Submission Guidelines.” APS. https://www.psychologicalscience.org/publications/psychological_science/ps-submissions.\n\n\nBlohowiak, Ben B., Johanna Cohoon, Lee de-Wit, Eric Eich, Frank J. Farach, Fred Hasselman, Alex O. Holcombe, Macartan Humphreys, Melissa Lewis, and Brian A. Nosek. 2023. “Badges to Acknowledge Open Practices.” https://osf.io/tvyxz/.\n\n\nHardwicke, Tom E., and Simine Vazire. 2023. “Transparency Is Now the Default at Psychological Science.” Psychological Science 0 (0). https://doi.org/https://doi.org/10.1177/09567976231221573.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nInstitute of Electrical and Electronics Engineers (IEEE). n.d. “About Content in IEEE Xplore.” IEEE Explore. Accessed May 20, 2024. https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-content.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.\n\n\nNISO Reproducibility Badging and Definitions Working Group. 2021. “Reproducibility Badging and Definitions.” https://doi.org/10.3789/niso-rp-31-2021.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html",
    "href": "pages/paper_evaluation.html",
    "title": "7  Evaluation of the article",
    "section": "",
    "text": "7.1 Summary\nSTRESS-DES:\nDES checklist derived from ISPOR-SDM:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#summary",
    "href": "pages/paper_evaluation.html#summary",
    "title": "7  Evaluation of the article",
    "section": "",
    "text": "Reflections\n\n\n\n\n\nOf the applicable criteria, all studies fully met at least 60% (and many others still partially met).\nI don’t think we learn much in relation to items reproduced, and that it is more interesting to consider which criteria were met and whether this aided the reproduction or not.\nNote. This plots the proportion of applicable criteria that were fully met.\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nThe proportion of applicable criteria met was lower for this checklist.\nHowever, I again think it’s most relevant to consider which criteria were met, rather than draw conclusions from this chart.\nNote. This plots the proportion of applicable criteria that were fully met.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#stress-des",
    "href": "pages/paper_evaluation.html#stress-des",
    "title": "7  Evaluation of the article",
    "section": "7.2 STRESS-DES",
    "text": "7.2 STRESS-DES\n\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\nIn this section and below, the criteria for each study are marked as either being fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nObjectives\n\n\n\n\n\n\n\n\n\n\n\n1.1 Purpose of the modelExplain the background and objectives for the model\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n1.2 Model outputsDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n🟡\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n1.3 Experimentation aimsIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis – Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments – Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation – (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\nLogic\n\n\n\n\n\n\n\n\n\n\n\n2.1 Base model overview diagramDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.2 Base model logicGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.3 Scenario logicGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\n2.4 AlgorithmsProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e. scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n✅\n🟡\n✅\n🟡\n✅\n✅\n✅\n✅\n\n\n\n2.5.1 Components - entitiesGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.5.2 Components - activitiesDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.5.3 Components - resourcesList all the resources included within the model and which activities make use of them.\n✅\n✅\nN/A\nN/A\n✅\nN/A\n✅\n✅\n\n\n\n2.5.4 Components - queuesGive details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n✅\n✅\nN/A\nN/A\n✅\nN/A\n✅\n✅\n\n\n\n2.5.5 Components - entry/exit pointsGive details of the model boundaries i.e. all arrival and exit points of entities. Detail the arrival mechanism (e.g. ‘thinning’ to mimic a non-homogenous Poisson process or balking)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nData\n\n\n\n\n\n\n\n\n\n\n\n3.1 Data sourcesList and detail all data sources. Sources may include:• Interviews with stakeholders,• Samples of routinely collected data,• Prospectively collected samples for the purpose of the simulation study,• Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n3.2 Pre-processingProvide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers.\n✅\nN/A\nN/A\n✅\nN/A\nN/A\nN/A\n✅\n\n\n\n3.3 Input parametersList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:• Base case data• Data use in experimentation, where different from the base case.• Where optimisation or design of experiments has been used, state the range of values that parameters can take.• Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n🟡\n🟡\n✅\n🟡\n✅\n✅\n✅\n✅\n\n\n\n3.4 AssumptionsWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n✅\n❌\n✅\n✅\n❌\n✅\n✅\n✅\n\n\n\nExperimentation\n\n\n\n\n\n\n\n\n\n\n\n4.1 InitialisationReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n🟡\n❌\n❌\n🟡\n❌\n✅\n❌\n✅\n\n\n\n4.2 Run lengthDetail the run length of the simulation model and time units.\n✅\n✅\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\n4.3 Estimation approachState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n🟡\n🟡\n🟡\n✅\n✅\nN/A\n✅\n✅\n\n\n\nImplementation\n\n\n\n\n\n\n\n\n\n\n\n5.1 Software or programming languageState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g. Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n\n\n\n5.2 Random samplingState the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n❌\n❌\n❌\n❌\n❌\nN/A\n❌\n✅\n\n\n\n5.3 Model executionState the event processing mechanism used e.g. three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n🟡\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n5.4 System specificationState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n✅\n❌\n🟡\n🟡\n❌\n❌\n🟡\n🟡\n\n\n\nCode access\n\n\n\n\n\n\n\n\n\n\n\n6.1 Computer model sharing statementDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nI find this chart a really helpful visualisation, to see what is commonly being met, things that are often not applicable, and things often not met.\nBelow, I’ve reflected on the impact of each criteria being fulfilled, on the reproduction. I’ve also identified where I feel I’ve been a bit more lax in my evaluation - indeed, the “subjective” nature of this evaluation is a limitation - although did (a) get consensus on uncertainties and unmet, and (b) revisit alongside other evaluations when writing this section, which helped me identify a few inconsistent decisions between studies within the evaluation, which I then addressed.\n1.1 Purpose of the modelExplain the background and objectives for the model.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n1.2 Model outputsDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n\nOne paper was rated as “partially met” as many of the outcomes used were not defined.\nFor those marked fully met, this was mainly based on them mentioning or being pretty clear about what the outcomes were, but wasn’t super strict - ie. didn’t require equations, or to specify how and when they were calculated.\nWhilst this is important, the raw/basic outcomes themselves were generally pretty straightforward, and issues with outputs more-so related to:\n\nWhich output to use from the results table (if unclear names, or similar names)\nHow to apply any required transformations\n\n\n1.3 Experimentation aimsIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis – Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments – Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation – (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n\nIn all seven papers with scenarios, these were described. Didn’t necessarily provide a “name” for each scenario, nor a “rationale” for the choice of scenarios, but did describe them.\nAlthough the description of the scenario can feel clear from the paper, when the code was not provided, it could be quick tricky and time-consuming to work out how to appropriately change the code, in order to implement the scenario.\n\n2.1 Base model overview diagramDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n\nAll papers included a diagram, which was great.\n\n2.2 Base model logicGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n2.3 Scenario logicGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n\nOverlap with 1.3 (implicit in describing 1.3, that would describe this).\n\n2.4 AlgorithmsProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e. scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n\nThose partially met are for describing some but not all of the algorithms. However, it is worth noting that it could be hard to actually ensure all relevant algorithms were described, if I hadn’t identified them in the more complex models.\n\n2.5.1 Components - entitiesGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n\nPretty basic requirement that all meet (unsurprisingly, as implicit in description of model / logic)\n\n2.5.2 Components - activitiesDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n\nPretty basic requirement that all meet (unsurprisingly, as implicit in description of model / logic)\nDidn’t necessarily require explicit description of routing.\n\n2.5.3 Components - resourcesList all the resources included within the model and which activities make use of them.\n\nGenerally seem to be mentioned when included, particularly as often form part of output (e.g. resource utilisation)\nWhen not mentioned (and based on known structure of model), assume not relevant.\n\n2.5.4 Components - queuesGive details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n\nAs for 2.5.3\n\n2.5.5 Components - entry/exit pointsGive details of the model boundaries i.e. all arrival and exit points of entities. Detail the arrival mechanism (e.g. ‘thinning’ to mimic a non-homogenous Poisson process or balking).\n\nGenerally fairly implicit\nOverlap with 2.4 algorithms, and hence didn’t necessarily mark this down if not full detail of arrival mechanism.\n\n3.1 Data sourcesList and detail all data sources. Sources may include:• Interviews with stakeholders,• Samples of routinely collected data,• Prospectively collected samples for the purpose of the simulation study,• Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n\nAll meet.\n\n3.2 Pre-processingProvide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers.\n\nOften had to assume that none occurred if none described.\n\n3.3 Input parametersList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:• Base case data• Data use in experimentation, where different from the base case.• Where optimisation or design of experiments has been used, state the range of values that parameters can take.• Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n\nThis was very important as it allows to check that the code parameters were correct as - in several cases - the provided code did not include the base case parameters as described. When missing from the paper, it was not possible to check them.\n\n3.4 AssumptionsWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n\nAlthough not relevant for reproduction, would be very relevant for reuse and validity\n\n4.1 InitialisationReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n\nOften not reported, and would be handy as it’s a pretty basic/fundamental aspect to the model that would help readers to have better understanding of how the model is working.\n\n4.2 Run lengthDetail the run length of the simulation model and time units.\n\nImportant to mention for same reasons as 3.3\n\n4.3 Estimation approachState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n\nThe description of this criteria is quite confusing - generally just focussed on identifying whether it was multiple replications or a big run, and then if the numbers used were justified.\nPartially met cases are those with replications that are not justified.\n\n5.1 Software or programming languageState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g. Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n\nWill often mentioned the programming language, but not operating system or version numbers\nThis was pretty handy, when versions were given, for cases where no versions were given in the repository itself - although ideally, versions could just simply be there.\n\n5.2 Random samplingState the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n\nFrequently not described, despite some of the studies having implemented and used seeds.\nInitially incorrectly addressed this in one of my evaluations, accidentally adding notes on the sampling of arrivals, rather than specifically on random samples. Have noted this to be clear that it was possible to misinterpret (although that may be more a mistake of my own! and not one others would necessarily make.)\n\n5.3 Model executionState the event processing mechanism used e.g. three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n\nOften not mentioned event processing machnism\nThis is a very long list of requirements in one category, and I found a bit difficult to evaluate, e.g. to check against all criteria\n\n5.4 System specificationState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n\nModel run time was really important to mention, but often not given. Particularly important for the models with longer run times, to know what to expect - including in light of reuse, and perhaps short times being necessary in certain contexts - or here, when troubleshooting, to know I might need to try lower numbers first while getting it working\nOverlaps with 5.3 parallel and 5.1 operating system.\n\n6.1 Computer model sharing statementDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n\nThis is inevitable/selection bias, given we chose papers that had links to code.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#des-checklist-derived-from-ispor-sdm",
    "href": "pages/paper_evaluation.html#des-checklist-derived-from-ispor-sdm",
    "title": "7  Evaluation of the article",
    "section": "7.3 DES checklist derived from ISPOR-SDM",
    "text": "7.3 DES checklist derived from ISPOR-SDM\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nModel conceptualisation\n\n\n\n\n\n\n\n\n\n\n\n1 Is the focused health-related decision problem clarified?…the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2 Is the modeled healthcare setting/health condition clarified?…the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n3 Is the model structure described?…the model’s conceptual structure was described in the form of either graphical or text presentation.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n4 Is the time horizon given?…the time period covered by the simulation was reported.\n✅\n✅\n✅\n✅\n❌\n✅\n✅\n✅\n\n\n\n5 Are all simulated strategies/scenarios specified?…the comparators under test were described in terms of their components, corresponding variations, etc\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\n6 Is the target population described?…the entities simulated and their main attributes were characterized.\n✅\n❌\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\nParamaterisation and uncertainty assessment\n\n\n\n\n\n\n\n\n\n\n\n7 Are data sources informing parameter estimations provided?…the sources of all data used to inform model inputs were reported.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n8 Are the parameters used to populate model frameworks specified?…all relevant parameters fed into model frameworks were disclosed.\n🟡\n🟡\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n9 Are model uncertainties discussed?…the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n🟡\n❌\n❌\n❌\n✅\nN/A\n✅\n✅\n\n\n\n10 Are sensitivity analyses performed and reported?…the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters’ plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n✅\n❌\n✅\n❌\nN/A\n✅\n❌\n✅\n\n\n\nValidation\n\n\n\n\n\n\n\n\n\n\n\n11 Is face validity evaluated and reported?…it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n❌\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n\n\n\n12 Is cross validation performed and reported…comparison across similar modeling studies which deal with the same decision problem was undertaken.\nN/A\n❌\n❌\n✅\n❌\n✅\n❌\n❌\n\n\n\n13 Is external validation performed and reported?…the modeler(s) examined how well the model’s results match the empirical data of an actual event modeled.\nN/A\nN/A\nN/A\n✅\n❌\n✅\n❌\n❌\n\n\n\n14 Is predictive validation performed or attempted? …the modeler(s) examined the consistency of a model’s predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\nN/A\nN/A\nN/A\nN/A\nN/A\n❌\nN/A\nN/A\n\n\n\nGeneralisability and stakeholder involvement\n\n\n\n\n\n\n\n\n\n\n\n15 Is the model generalizability issue discussed?…the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n✅\n✅\n✅\n❌\n🟡\n✅\n❌\n✅\n\n\n\n16 Are decision makers or other stakeholders involved in modeling?…the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\n17 Is the source of funding stated?…the sponsorship of the study was indicated.\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\n18 Are model limitations discussed?…limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\n✅\n🟡\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nThese guidelines have quite a different focus from STRESS-DES. STRESS-DES is very focused on what and how modelling work is performed. These guidelines do cover that, but have quite alot of focus on validity of the work, and good practice (e.g. stating funding, involving stakeholders). This is important, as different checklists can be important - and although practice is typically to use one practice, referring to more than one could also be beneficial.\n1 Is the focused health-related decision problem clarified?…the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n2 Is the modeled healthcare setting/health condition clarified?…the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n3 Is the model structure described?…the model’s conceptual structure was described in the form of either graphical or text presentation.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n4 Is the time horizon given?…the time period covered by the simulation was reported.\n\nThere was only one study where this was not stated (Anagnostou et al. (2022)), though this didn’t impact the reproduction itself, for this study I was simply able to run the code and pretty much get the result required with minimal troubleshooting.\n\n5 Are all simulated strategies/scenarios specified?…the comparators under test were described in terms of their components, corresponding variations, etc\n\nAll papers with scenarios described them. As reflected for STRESS-DES, though the description of the scenario can feel clear from the paper, when the code was not provided, it could be quick tricky and time-consuming to work out how to appropriately change the code, in order to implement the scenario.\n\n6 Is the target population described?…the entities simulated and their main attributes were characterized.\n\nFulfilment didn’t impact the reproduction as this is more about interpretation/validity/etc.\n\n7 Are data sources informing parameter estimations provided?…the sources of all data used to inform model inputs were reported.\n\nAll met, although it’s worth noting that I didn’t check that every single parameter’s data source was stated (and indeed, its likely some were not) - simply whether I could identified that at least some were.\n\n8 Are the parameters used to populate model frameworks specified?…all relevant parameters fed into model frameworks were disclosed.\n\nThis was quite important for the reproduction as it allows us to check the parameters in the code are correct. When I identified that there were some discrepancies between the article and code, then in cases where a parameter was not given in the article, I couldn’t be sure if it was correct or not in the code, as there was nothing to compare against.\n\n9 Are model uncertainties discussed?…the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n\nSome don’t (but worth noting this doesn’t have impact on reproduction - beyond just these being additional values/lines in plot that we are trying to reproduce)\n\n10 Are sensitivity analyses performed and reported?…the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters’ plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n\nSome performed, some mentioned but no results presented, and some didn’t mention at all. One explained it to not be relevant.\n\n11 Is face validity evaluated and reported?…it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n\nRare (only one completed) - but didn’t impact reproduction as this is more related to validity\n\n12 Is cross validation performed and reported…comparison across similar modeling studies which deal with the same decision problem was undertaken.\n\nRare (only two completed) - but didn’t impact reproduction as this is more related to validity\n\n13 Is external validation performed and reported?…the modeler(s) examined how well the model’s results match the empirical data of an actual event modeled.\n\nRare (only two completed) - although some argue not applicable - but didn’t impact reproduction as this is more related to validity\n\n14 Is predictive validation performed or attempted? …the modeler(s) examined the consistency of a model’s predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\n\nGenerally not applicable - but didn’t impact reproduction as this is more related to validity\n\n15 Is the model generalizability issue discussed?…the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n\nMore than half did discuss this - but didn’t impact reproduction as this is more related to reuse\n\n16 Are decision makers or other stakeholders involved in modeling?…the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n\nRare (only one mentioned) - but didn’t impact reproduction as this is more related to validity\n\n17 Is the source of funding stated?…the sponsorship of the study was indicated.\n\nGood practice, usually a journal requirement, although some did not have - but didn’t impact reproduction.\n\n18 Are model limitations discussed?…limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\n\nMost discuss limitations (didn’t really assess how comprehensive these were though, except in two cases where they only had a very general or hint towards limitations) - but didn’t impact reproduction.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#timings",
    "href": "pages/paper_evaluation.html#timings",
    "title": "7  Evaluation of the article",
    "section": "7.4 Timings",
    "text": "7.4 Timings\n\nShoaib and Ramamohan (2021) - 1h 56m\nHuang et al. (2019) - 1h 28m\nLim et al. (2020) - 1h 12m\nKim et al. (2021) - 2h 12m\nAnagnostou et al. (2022) - 53m\nJohnson et al. (2021) - 1h 32m\nHernandez et al. (2015) - 1h 11m\nWood et al. (2021) - 1h 24m\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nIt sometimes took quite a while to find all this information from the articles - and we acknowledge there’s a chance that it is provided somewhere in the article but I missed it. For both of these reasons, we see the value in actually attaching a completed reporting checklist, clearly laying out key information about the model and study.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#use-of-reporting-guidelines",
    "href": "pages/paper_evaluation.html#use-of-reporting-guidelines",
    "title": "7  Evaluation of the article",
    "section": "7.5 Use of reporting guidelines",
    "text": "7.5 Use of reporting guidelines\nRegarding whether each study mentioned using reporting guidelines:\n\nShoaib and Ramamohan (2021) - ❌\nHuang et al. (2019) - ❌\nLim et al. (2020) - ❌\nKim et al. (2021) - ❌\nAnagnostou et al. (2022) - ❌\nJohnson et al. (2021) - ✅ Consolidated Health Economic Evaluation Reporting Standards (CHEERS) - Husereau et al. (2013)\nHernandez et al. (2015) - ❌\nWood et al. (2021) - ✅ STRESS-DES: Strengthening The Reporting of Empirical Simulation Studies (Discrete-Event Simulation) - Monks et al. (2019)\n\nAlthough this is only a small sample, its interesting to note that the two studies that used reporting guidelines both had the highest proportion of fully met criteria in either reporting guideline.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#uses-a-previously-reported-model",
    "href": "pages/paper_evaluation.html#uses-a-previously-reported-model",
    "title": "7  Evaluation of the article",
    "section": "7.6 Uses a previously reported model",
    "text": "7.6 Uses a previously reported model\nRegarding whether each study was using a previously reported model:\n\nShoaib and Ramamohan (2021) - No\nHuang et al. (2019) - No\nLim et al. (2020) - No\nKim et al. (2021) - Yes - previously described by Glover et al. (2018) and Thompson et al. (2018)\nAnagnostou et al. (2022) - No\nJohnson et al. (2021) - Yes - EPIC model previously described by Sadatsafavi et al. (2019)\nHernandez et al. (2015) - No\nWood et al. (2021) - Yes - previously described by Wood et al. (2020)\n\nAgain, a small sample, but this time a weaker pattern. We note that the two studies that used reporting guidelines are the same that are previously reported models here, alongside one other study which was previously reported but did not use reporting guidelines in this instance, and has a lower proportion of criteria that were fully met.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#references",
    "href": "pages/paper_evaluation.html#references",
    "title": "7  Evaluation of the article",
    "section": "7.7 References",
    "text": "7.7 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nGlover, Matthew J., Edmund Jones, Katya L. Masconi, Michael J. Sweeting, Simon G. Thompson, Janet T. Powell, Pinar Ulug, and Matthew J. Bown. 2018. “Discrete Event Simulation for Decision Modeling in Health Care: Lessons from Abdominal Aortic Aneurysm Screening.” Medical Decision Making 38 (4): 439–51. https://doi.org/10.1177/0272989X17753380.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nHusereau, Don, Michael Drummond, Stavros Petrou, Chris Carswell, David Moher, Dan Greenberg, Federico Augustovski, Andrew H. Briggs, Josephine Mauskopf, and Elizabeth Loder. 2013. “Consolidated Health Economic Evaluation Reporting Standards (CHEERS) Statement.” Value in Health 16 (2): e1–5. https://doi.org/10.1016/j.jval.2013.02.010.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nSadatsafavi, Mohsen, Shahzad Ghanbarian, Amin Adibi, Kate Johnson, J. Mark FitzGerald, William Flanagan, Stirling Bryan, and Don Sin. 2019. “Development and Validation of the Evaluation Platform in COPD (EPIC): A Population-Based Outcomes Model of COPD for Canada.” Medical Decision Making 39 (2): 152–67. https://doi.org/10.1177/0272989X18824098.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nThompson, Simon G, Matthew J Bown, Matthew J Glover, Edmund Jones, Katya L Masconi, Jonathan A Michaels, Janet T Powell, Pinar Ulug, and Michael J Sweeting. 2018. “Screening Women Aged 65 Years or over for Abdominal Aortic Aneurysm: A Modelling Study and Health Economic Evaluation.” Health Technology Assessment 22 (43): 1–142. https://doi.org/10.3310/hta22430.\n\n\nWood, Richard M., Christopher J. McWilliams, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2020. “COVID-19 Scenario Modelling for the Mitigation of Capacity-Dependent Deaths in Intensive Care.” Health Care Management Science 23 (3): 315–24. https://doi.org/10.1007/s10729-020-09511-7.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/compendium.html",
    "href": "pages/compendium.html",
    "title": "8  Reflections from research compendium and test-run",
    "section": "",
    "text": "8.1 Reflections related to compendium\nOrganisation:\nTests:\nGitHub actions:\nDocker:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reflections from research compendium and test-run</span>"
    ]
  },
  {
    "objectID": "pages/compendium.html#reflections-related-to-compendium",
    "href": "pages/compendium.html#reflections-related-to-compendium",
    "title": "8  Reflections from research compendium and test-run",
    "section": "",
    "text": "No one right way to organise. If it was already pretty good, I left it (e.g. Kim et al. 2021). However, for lots of them, I did change organise to e.g. scripts/, inputs/, outputs/\n\n\n\nWhen creating the tests, since many of the models had long run times, my intention was often just to create a “mini-run” of the model that allows someone to quickly and easily check if they can run some aspect of the model (without needing to try the whole one with long run time).\nThe tests created provide examples of how can set up tests in Python and R, doing something basic like comparing dataframes or CSV files between runs. Indeed, it took a little bit of time to get set up initially with tests working as I want, but once figured for one, I could reuse like a template for the next.\nSometimes had to amend model to be able to run tests - e.g. for Wood et al. 2021, test wouldn’t work if model was run with parallel processing\nAlso, an example of Johnson et al. 2021 where I couldn’t get the model to work correctly within a test (despite no visible errors), and so went with the solution of making a normal .R script that operates like a test but without the testthat package\n\n\n\nWasn’t able to successfully build a quarto book using GitHub actions if it required both Python and R (in which case, had to push from local)\n\n\n\nThis sometimes took rather alot of troubleshooting, and could sometimes be quite tricky. However, once resolved (Python, R, and later a older Python), I could then just reuse previous dockerfiles like a template (although sometimes still some troubleshooting still needed).",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reflections from research compendium and test-run</span>"
    ]
  },
  {
    "objectID": "pages/compendium.html#reflections-related-to-test-run",
    "href": "pages/compendium.html#reflections-related-to-test-run",
    "title": "8  Reflections from research compendium and test-run",
    "section": "8.2 Reflections related to test-run",
    "text": "8.2 Reflections related to test-run\nThe test-run stage after making compendium helped us to spot run issues that I hadn’t noticed, mainly from having not completely tested absolutely everything before pushing. For example:\n\nShoaib and Ramamohan 2022 - there was an authentication error when trying to pull the docker image from GitHub Container Registry, as I was missing some of the steps related to personal access tokens\nHernandez et al. 2015 - though the test worked locally, it did not work on docker. This was related to imports, and resolved by adding __init__.py files, but I hadn’t noticed as hadn’t checked the test on docker.\nLim et al. 2020 - one of the files failed to run as I hadn’t uploaded some of the required data\n\nShows importance of having someone else check things - easy to leave in small mistakes that mean it doesn’t run. In general, having something like this to check your code is a really handy practice, if possible.\nThe test-run stage also helped highlight things that weren’t clear. For example, I had a test that took a few minutes to run, and whilst running your console is blank. He was unsure if this was an error - so I then add a section to the README that explained what to expect to see when running those tests.\nAlso, likely errors people will encounter - for example, by not having a particular environment active or not being in the right folder when trying to run something - and so I then made sure to clarify this in the README.\nWhen running test-runs on a fresh machine, Tom often found there were operating system dependencies he had to install for R. Also, examples like Huang et al. 2019 were Tom looked to run on his virtual machine but that only allocates 4GB RAM and the model used 8GB RAM so he had to use a different machine - hence, also helping highlight memory requirements.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reflections from research compendium and test-run</span>"
    ]
  },
  {
    "objectID": "pages/results_discussion.html",
    "href": "pages/results_discussion.html",
    "title": "9  Results discussion",
    "section": "",
    "text": "9.1 Reflections\n(Addressing Tom’s suggestion - “Maybe there are some high level groupings of your findings? e.g. Study reporting; Model structuring and functionality; Analysis pipeline”)",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Results discussion</span>"
    ]
  },
  {
    "objectID": "pages/results_discussion.html#reflections",
    "href": "pages/results_discussion.html#reflections",
    "title": "9  Results discussion",
    "section": "",
    "text": "9.1.1 Analysis pipeline\nThese are important for reproduction. If need to troubleshoot, then model structure and functionality are particularly useful - e.g.:\n\nWriting code for scenarios is impacted by: programmatic, don’t hard code, sufficient comments\nWriting code for results is impacted by: understandable output\n\nNotes:\n\nTom: Key for STARS 2.0: seeds\n\n\n\n\n“Reflections” in the group “Analysis Pipeline”\n\n\n\n\n\n\n\n\nLinks to evaluation\n\n\n\n\n\nHence, these are included in relationship between reproduction and evaluation below\n\nPackages - STARS: dependency management\nVersions - Badges: documentation, and STRESS: software\nScenario - ISPOR and STRESS: scenario, and Badges: complete set of materials\nRequired parameters - ISPOR and STRESS: input parameters\n\n\n\n\n\n\n9.1.2 Model structure and functionality\nThese are important for reuse - but also pipeline stuff! As they (a) make reproduction easier and (b) are important when troubleshooting.\nNotes:\n\nData dictionaries are relevant to inputs and outputs.\nSome of these can become “less of an issue” for reproduction if there is a reproducible analytical pipeline, as don’t need to delve into it and troubleshoot (but are still relevant for reuse).\n\n\n\n\n“Reflections” in the group “Model structure and functionality”\n\n\n\n\n\n\n\n\nLinks to evaluation\n\n\n\n\n\nHence, these are included in relationship between reproduction and evaluation below\n\nData dictionaries - Badges: careful documentation\nInstructions on how to run - STARS and Badges: documentation (sufficient, minimal and step by step)\nRun time - STRESS: run time\n\n\n\n\n\n\n9.1.3 Other\nThese didn’t fit into the two categories above:\n\nOpen license\n\nOn ALL repositories required\n\nUnsupported versions\n\n\n\n\n\n\n\nLinks to evaluation\n\n\n\n\n\nHence, these are included in relationship between reproduction and evaluation below\n\nBadges and STARS: open license",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Results discussion</span>"
    ]
  },
  {
    "objectID": "pages/results_discussion.html#headline-findings",
    "href": "pages/results_discussion.html#headline-findings",
    "title": "9  Results discussion",
    "section": "9.2 Headline findings",
    "text": "9.2 Headline findings\nWhilst above captures the reflections, I’ve also pulled out a few headline findings.\nThe four most important things:\n\nUsing correct parameters in code\nIncluding code for all scenarios\nInclude code to create tables, figures and in-text results\nOpen license\n\nMany of these headlines link to the importance of reproducible analytical pipelines (RAP) for simulation.\nWhy were these the most fundamental?\n\nUsing the code. Open license (4) is fundamentally important - can’t reuse code without it. In several studies, had to ask authors to add license (all kindly agreed).\n\n\nTiming. Timings varied alot, from 2h11 to 28h14\n\nSix slower studies (&gt;12h)\n\nI think main reasons for time were: troubleshooting wrong parameters (1), writing code for scenarios (2), and creating tables/figures/in-text results (3) (including appropriate transformations, which columns to use, and so on).\nScope could be larger (5 to 17 items)\n\nTwo quicker studies (&lt;4h)\n\nBoth required very little troubleshooting (e.g. only small amount of coding)\nScope was also simple (1 or 5 items)\n\n\n\n\nReproduction success. I think main reasons were having wrong parameters (1), not having code for scenarios (2) and not having code for results (3).",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Results discussion</span>"
    ]
  },
  {
    "objectID": "pages/results_discussion.html#relationship-between-reproduction-and-evaluation",
    "href": "pages/results_discussion.html#relationship-between-reproduction-and-evaluation",
    "title": "9  Results discussion",
    "section": "9.3 Relationship between reproduction and evaluation",
    "text": "9.3 Relationship between reproduction and evaluation\nRegarding relationship between reproduction and evaluation, in terms of article reporting, these feel like the key things that impacted reproduction…\n(Also addressing Tom’s suggestion: “Evidence supporting the use of reporting guidelines… as in - reporting guidelines state this should info be included and you found it v.difficult or impossible to reproduce without it”)\n\n9.3.1 Article\nReally important to be in article:\n\nSTRESS: Run time\n\nFound this very useful when mentioned, and a bit frustrating when not and later realise its a long run time. Want in both article and repository as whether someone finds repository from article or vice versa, want them to have right expectations up front of run requirements for this model\n\nISPOR and STRESS: Input parameters\n\nThis was VERY important to be reported as it allows us to check code parameters are correct. When missing from paper, not possible to check\n\n\nHandy to be in article but should ideally be in repository:\n\nSTRESS: software\nScenario code ISPOR and STRESS\n\nMay describe in paper but if code not provided can still be tricky and time consuming to implement\n\n\n\n\n9.3.2 Repository\n\nSTARS and badges: documentation (sufficient, minimal, step by step, careful)\n\nInstructions on how to run\nUnderstandable output e.g. data dictionaries\nVersions\n\nSTARS: dependency management",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Results discussion</span>"
    ]
  },
  {
    "objectID": "pages/results_discussion.html#evidence-for-framework-components",
    "href": "pages/results_discussion.html#evidence-for-framework-components",
    "title": "9  Results discussion",
    "section": "9.4 Evidence for framework components",
    "text": "9.4 Evidence for framework components\nReflections:\n\n“Include instructions on how to run the model/script” –&gt; Evidence for STARS essential component minimum documentation\n“For slow models, state the expected run time” –&gt; Evidence for STRESS-DES 5.4\n“For computationally expensive models, state memory usage and provide alternatives for lower spec machines” –&gt; Evidence for STRESS-DES\n“Provide all the required parameters” and “If not provided in the script, then clearly present all parameters in the paper” –&gt; Evidence for STRESS-DES 3.3",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Results discussion</span>"
    ]
  },
  {
    "objectID": "pages/results_discussion.html#other-things-i-wanted-to-mention",
    "href": "pages/results_discussion.html#other-things-i-wanted-to-mention",
    "title": "9  Results discussion",
    "section": "9.5 Other things I wanted to mention",
    "text": "9.5 Other things I wanted to mention\nDifferent checklists have different focuses… STRESS-DES focuses on what and how modelling work is performed… derived from ISPOR-SDM checklist is more related to validity type stuff…\nWhether or not a study was fully reproduced or not doesn’t necessarily mean it was “good” or “bad”. Did lots of troubleshooting for some before fully reproduced. And sometimes, couldn’t reproduce for simple reasons, like likely parameter being wrong impacting lal results.\nBadges not awarded for not having combination of things required e.g. archive + docs, or complete + docs. Many got basic one of reproducing results, but I allowed troubleshooting - yet they may not - particularly HOURS of troubleshooting.\nArticle evaluation sometimes took quite a while… limitation that may not be perfect… those that used guidelines did better… emphasis importance of (a) using guidelines and (b) attaching completed guidelines\nMeeting STARS optional wasn’t helpful for reproduction (but would be for reuse)",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Results discussion</span>"
    ]
  },
  {
    "objectID": "pages/other_studies.html",
    "href": "pages/other_studies.html",
    "title": "10  Comparison to other studies",
    "section": "",
    "text": "In-progress\nTODO: Consider, would there be any relevant literature related to time spent to try and get someone elses code mentioned - like how Andy mentioned there would be a max amount of time that someone in the NHS would reasonably spend troubleshooting\nTODO: Add page comparing our findings to other studies. This could include:\n\nStudies that have gone through this same process of finding barriers in reproductions\nStudies that recommend how to share code (although, it might be simpler to just start with that first bullet and then decide whether to do both)\n\nDo we also compare the evaluation results? Or do we just use that in context of barriers? (e.g. Schwander et al. (2021) and Zhang, Lhachimi, and Rogowski (2020) for reporting… Laurinavichyute, Yadav, and Vasishth (2022) for code…)\nnot sure if want to be comparing the actual proportions reproduced… or maybe do, but important to bare in mind what is being compared and what define as success… e.g. some include studies that haven’t shared the code\nStarting points:\n\nKrafczyk et al. (2021) - results are recommendations based on experiences, which were:\n\ncan see completion in figure 1\nbeing clear on links between article, code and data (e.g. which code uses which data, and which parts of code made each bit of article)\ninclude scripts for each aspect of article, with it easy to locate the scripts needed, and the scripts including the parameters needed and clearly label\nbe clear about hardware needed, e.g. if large amount of computing resources would be required. at least report hardware used. ideally include “small test case that can be run by users with conventional hardware”\nlist software dependencies and versions\nuse seeds, and report seed you used\nmake all code and data available with an appropriate licence\ninclude master script that runs in computations in publication\nuse same terminology in code and article\nuse version control and specify the e.g. commit hash that identifies the version used\navoid hard coding parameters\ndesign scripts in a way that allows people to easily change parameters and run again\navoid hard coding file paths\nprovide script that checks whether users results match original (within expected deviation)\nif compare against competing methods, include info on how those were implemented and tested\nuse build system for C/C++ code\nprovide scripts to make the figures and tables\n\nWood, Müller, and Brown (2018)\n\ncomplete data: 27 comparable results, 5 minor differences, 0 major differences\nincomplete data: 10 comparable, 4 minor differences, 1 major differences\nmain issue was the code and data not being shared\n\nSchwander et al. (2021)\n\nreproduction success for 3 out of 4 models\nfacilitators:\n\n“Model structure and possible state transitions were presented in a state transition diagram”\n“Overview of input parameters was provided in table format”\n\nhurdles:\n\n“PSAs were performed” (probablistic sensitivity analysis)\n“Relevant PSA values for PSA result reproduction were provided (type of distribution and either mean and standard deviation or distribution parameters were provided)”\n“Clinical event simulation results were provided (which are very helpful to guide potential assumptions to be made for rebuilding the model and which provide an additional means of testing the fit of the replication)”\n“Relevant details on the underlying life tables were provided (including year of data)”\n“Several self-created regression equations were introduced but without details on how to apply/solve the provided regressions correctly”\n\n\nLaurinavichyute, Yadav, and Vasishth (2022)\nKonkol, Kray, and Pfeiffer (2019)\nHardwicke et al. (2021)\nMonks and Harper (2023) (although overlap - look at how many of mine are from the review - some were identified from that, but some were not)\nHenderson et al. (2024)\nSamuel and Mietchen (2024)\n\nReproduction success:\n\n27,271 jupyter notebooks (2660 GitHub repositories, 3467 publications, majority python)\n15,817 included dependencies in standard requirement files\nOf those, 10,388 could be installed successfully\nOf those, 1203 ran without error\nOf those, 879 produced results identical to those reported in the original notebook\n\nReferences several other studies that have attempted to re-run jupyter notebooks\nModuleNotFoundError, ImportError and FileNotFounderError were the top 3 common exceptions\nAlso used flakenb to look for code styling errors\nImplications:\n\nLow reproducibility as in prior studies\nReview processes don’t pay attention to journal reproducibiltiy\nCommon errors around dependencies… Use exiting approaches like requirements, conda and poetry\nSome journals, article types, levels of documentation and research fields had more reproduced notebooks than others. Worth considering procedures at those journals with most reproduced (iScience). Also, notebooks combining computation and narrative.\nThings go out of data… importance of pre-prints…\nReproducibility badges, reproducibility platforms like REScience, nanopublications\n\n\n\nAnd some relevant articles/discussions:\n\nHrynaszkiewicz, Harney, and Cadwallader (2021)\n\nSurvey in Autumn 2020 of previous authors and other registered users with PLOS Computational Biology\n214 complete responses\nLots had papers where code was not shared - reasons inc (1) takes too much time to prepare code for sharing, (2) software and systems dependencies (3) concerns with my ability to prepare the code for sharing (4) I needed to protect intellectual property, etc…\nMore likely to submit to journal if mandatory code sharing policy (moreso for ECR)\nLow mean satisfication score for accessing others code (44.0)\n\nCadwallader and Hrynaszkiewicz (2022)\n\nSurvey in Spring 2021 of previous authors and other registers used with PLOS computational biology related disciplines\n188 complete responses\nQuestions on\n\nHow often look at code associated with research articles\nWhether have encountered different methods of code sharing\nHow useful found different methods of accessing code - link to code repository was rated most useful - with code “available upon request” least useful. When explaining why link was most useful, common reasons were: (a) ability to see new versions of code (b) quick access to code (c) good documentation/README (d) practicality (e) reproduction of results (f) established. For sharing via notebooks, like ability to explore code and reproduction of results. For archived code, like that it gives long term access.\nFeatures of code notebooks that are useful when accessing/reading code, highest ranked: (1) having all code, data and figures in one place, (2) knowing code is running in right environment (3) ability to interact inline with code in browser (4) ability to uncover data point by hovering over point in graph …etc\nSatisifcation and importance scores of factors associated with sharing code\nTime spent preparing and sharing code\nExtra time willing to spend to make code easier to read and run\n\n\n“Revisiting Code Reusability” (2022)\n\nInitiative from Nature Machine Intelligence to have new article type “Reusability Report” where researchers try code from article in journal and try running it, applying to new data or tweaking or extending it\n\nMejba et al. (2023)\n\nLists benefits of code reuse: increase productivity, improved code quality, reduced errors, cost efficiency, faster time-to-market, knowledge sharing, scalability, sustainability, innovation\nLists challenges of code reuse: understanding reused code, integration issues, incompatability, maintaining and updating reused code, licensing and ownership, security risks, quality control, overhead, dependency management\nGives solutions to changes e.g. understanding reused code –&gt; comprehensive documentation\nStrategies for effective code reuse… modular design, use of libraries and frameworks, design patterns, object oriented programming, apis and microservices, code documentation, automated testing, refactoring, code reviews, ci/cd\nLists security implications of code reuse, and best practices for security\nImpacts of code reuse on software development lifecycle:\n\nPlanning and analysis - reuse code can reduce time and resources required, although crucial to check code compatability, quality and security to meet needs\nDesign - design of code can facilitate reuse\nImplementation - reuse snippets, lirbaries, etc.\nTesting - reused code can have benefit of being previously tests so less bugs, more stable software, but will need rigorous re test\nDeployment - if code has depenedencies, need to manage properly eg docker\nMaintenance - if reused code well understood and well documentation can make maintence easier\n\n\nDuPre et al. (2022)\n\nhttps://theplosblog.plos.org/2022/05/uphold-the-code/ describes the article, saying ” DuPree and colleagues draw a distinction between the scholarly article, which they argue functions as a kind of preview or entrypoint to the research, and the real scholarly work. That is, the methods researchers develop (in this case software and code), and the data they gather and analyze in order to produce a result. It’s this data and code which forms the core of the research, the fulcrum on which everything else turns. For that reason, code not only deserves a place in the literature, it is required in order to fully understand and appreciate the scholarship.”\nArticle is about “hybrid research objects” - i.e. multiple content types, so narrative text plus at least one of: code, data, and/or computation. Can link in data/code availability statements. Publisher may require those linked objects to meet specific standards and include as part of review process. “Hope to see more hybrid research objects where each linked object is formatted with domain-relevant standards… and bi-directionally linked using persistent identifiers”\nArticle then talks about interactive research objects, with everything all in one place, like “executable research articles”, RMArkdown, Jupyter Book, Binder, Pangeo, NeuroLibre\n\nBenureau and Rougier (2018)\n\nhttps://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2017.00069/full#B22\n“In conclusion, the R3 (reproducible) form should be accepted as the minimum scientific standard (Wilson et al., 2017).”\n\nWilson et al. (2017)\n\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510\n\nGomes et al. (2022)\n\nhttps://royalsocietypublishing.org/doi/full/10.1098/rspb.2022.1113\nFigure 1 is a nice visualisation\n\nConnolly et al. (2023)\n\nhttps://par.nsf.gov/servlets/purl/10510561\n\nhttps://www.nature.com/articles/nn.4550\n\n\n\n\n\nBenureau, Fabien C. Y., and Nicolas P. Rougier. 2018. “Re-Run, Repeat, Reproduce, Reuse, Replicate: Transforming Code into Scientific Contributions.” Frontiers in Neuroinformatics 11 (January). https://doi.org/10.3389/fninf.2017.00069.\n\n\nCadwallader, Lauren, and Iain Hrynaszkiewicz. 2022. “A Survey of Researchers’ Code Sharing and Code Reuse Practices, and Assessment of Interactive Notebook Prototypes.” PeerJ 10 (August): e13933. https://doi.org/10.7717/peerj.13933.\n\n\nConnolly, Andrew, Joseph Hellerstein, Naomi Alterman, David Beck, Rob Fatland, Ed Lazowska, Vani Mandava, and Sarah Stone. 2023. “Software Engineering Practices in Academia: Promoting the 3Rs—Readability, Resilience, and Reuse.” Harvard Data Science Review 5 (2). https://doi.org/10.1162/99608f92.018bf012.\n\n\nDuPre, Elizabeth, Chris Holdgraf, Agah Karakuzu, Loïc Tetrel, Pierre Bellec, Nikola Stikov, and Jean-Baptiste Poline. 2022. “Beyond Advertising: New Infrastructures for Publishing Integrated Research Objects.” PLOS Computational Biology 18 (1): e1009651. https://doi.org/10.1371/journal.pcbi.1009651.\n\n\nGomes, Dylan G. E., Patrice Pottier, Robert Crystal-Ornelas, Emma J. Hudgins, Vivienne Foroughirad, Luna L. Sánchez-Reyes, Rachel Turba, et al. 2022. “Why Don’t We Share Data and Code? Perceived Barriers and Benefits to Public Archiving Practices.” Proceedings of the Royal Society B: Biological Sciences 289 (1987): 20221113. https://doi.org/10.1098/rspb.2022.1113.\n\n\nHardwicke, Tom E., Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michèle B. Nuijten, Benjamin N. Peloquin, Benjamin E. deMayo, Bria Long, Erica J. Yoon, and Michael C. Frank. 2021. “Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal Psychological Science: An Observational Study.” Royal Society Open Science 8 (1): 201494. https://doi.org/10.1098/rsos.201494.\n\n\nHenderson, Alec S., Roslyn I. Hickson, Morgan Furlong, Emma S. McBryde, and Michael T. Meehan. 2024. “Reproducibility of COVID-Era Infectious Disease Models.” Epidemics 46 (March): 100743. https://doi.org/10.1016/j.epidem.2024.100743.\n\n\nHrynaszkiewicz, Iain, James Harney, and Lauren Cadwallader. 2021. “A Survey of Code Sharing Practice and Policy in Computational Biology.” OSF. https://doi.org/10.31219/osf.io/f73a6.\n\n\nKonkol, Markus, Christian Kray, and Max Pfeiffer. 2019. “Computational Reproducibility in Geoscientific Papers: Insights from a Series of Studies with Geoscientists and a Reproduction Study.” International Journal of Geographical Information Science 33 (2): 408–29. https://doi.org/10.1080/13658816.2018.1508687.\n\n\nKrafczyk, M. S., A. Shi, A. Bhaskar, D. Marinov, and V. Stodden. 2021. “Learning from Reproducing Computational Results: Introducing Three Principles and the Reproduction Package.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 379 (2197): 20200069. https://doi.org/10.1098/rsta.2020.0069.\n\n\nLaurinavichyute, Anna, Himanshu Yadav, and Shravan Vasishth. 2022. “Share the Code, Not Just the Data: A Case Study of the Reproducibility of Articles Published in the Journal of Memory and Language Under the Open Data Policy.” Journal of Memory and Language 125 (August): 104332. https://doi.org/10.1016/j.jml.2022.104332.\n\n\nMejba, Raoha Bin, Sabbir Morshed Miazi, Ashikur Rahaman Palash, Touhidur Rahman Sobuz, and Ranasinghe Arachchige Tharindu Harshana Ranasinghe. 2023. “The Evolution and Impact of Code Reuse: A Deep Dive into Challenges, Reuse Strategies and Security,” November. https://doi.org/10.5281/ZENODO.10141558.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\n“Revisiting Code Reusability.” 2022. Nature Machine Intelligence 4 (10): 801–1. https://doi.org/10.1038/s42256-022-00554-9.\n\n\nSamuel, Sheeba, and Daniel Mietchen. 2024. “Computational Reproducibility of Jupyter Notebooks from Biomedical Publications.” GigaScience 13 (January): giad113. https://doi.org/10.1093/gigascience/giad113.\n\n\nSchwander, Björn, Mark Nuijten, Silvia Evers, and Mickaël Hiligsmann. 2021. “Replication of Published Health Economic Obesity Models: Assessment of Facilitators, Hurdles and Reproduction Success.” Pharmacoeconomics 39 (4): 433–46. https://doi.org/10.1007/s40273-021-01008-7.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nWood, Benjamin D. K., Rui Müller, and Annette N. Brown. 2018. “Push Button Replication: Is Impact Evaluation Evidence for International Development Verifiable?” PLOS ONE 13 (12): e0209416. https://doi.org/10.1371/journal.pone.0209416.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparison to other studies</span>"
    ]
  },
  {
    "objectID": "pages/framework.html",
    "href": "pages/framework.html",
    "title": "11  Modifying the framework",
    "section": "",
    "text": "11.1 Contents of the STARS framework",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/framework.html#contents-of-the-stars-framework",
    "href": "pages/framework.html#contents-of-the-stars-framework",
    "title": "11  Modifying the framework",
    "section": "",
    "text": "Section from reflections\nConsideration for framework\n\n\n\n\nList required packages\nThe difference between listing all packages, and listing some that then have other dependencies. The various options there are for listing packages, and the downside of just assuming its obvious from imports. The options for guiding dependency management (e.g. providing scripts that download packages, and then environment management tools)\n\n\nProvide version for Python/R and for packages\nImportant to do so for both (not just packages). Challenge of implementing in R. Ease of implementing in Python. Importance of listing in repository itself. Options for doing this in R and Python. Suggestions. Need to explore more with R. Is it a matter of providing renv but suggesting someone could try with latest versions in first instance? As not realistic to guarantee maintenance of research projects. But know at least when it worked - as being clear, it won’t always, even in R\n\n\nModel in a “runnable” format\nThis was an unusual case, but I think could merge more under the example of providing code that aligns with the paper, as in that case, it was the code for a web app when the paper wasn’t about that\n\n\nModel designed to run programmatically, and don’t hard code parameters you want to change.\nConsider whether framework will given guidance on model structure. If so, it should be suggesting that (A) parameters are seperate from model and (B) run scenarios by changing elsewhere (and not by directly modifying the model code). Why do this? Because its simpler to run multiple versions of model with same script, and reduces likelihood of missing errors of inputing wrong parameters\n\n\nAvoid large amounts of code duplciation\nThis would also be a code structure thing (does it fit in framework?). Good to do in general - fairly standard coding practice. Here as it makes code more readable, makes it easier when want to change all scenarios, and reduces likelihood of introducing mistakes (which did see)\n\n\nSufficient comments in code\nLikewise, code structure. Standard practice like docstrings (which only one or two had). The standard structures that are available for docstrings (e.g. roxygen in R). And then also just general comments in code itself.\n\n\nQuicker models\nWorth considering for framework, as this was practically one of the big things for me when it came to using the models. Think about the options that are available for reducing model run time. And from the start, principles of keeping it simple and small. Things to avoid that make it slow. Alternatives to those things. The value of a quicker model (e.g. in being able to make an app, to rerun it all easily and spot mistakes, to just be easier to work with).\n\n\nState run time\nCould include in minimum documentation\n\n\nState memory usage and alterantives for lower spec machines\nCould include in enhanced documentation? Or minimum? Need to consider that this can seem a daunting thing if not familiar and wouldn’t know how to find this out or what this means\n\n\nProvide code for all scenarios\n\n\n\nInclude correct parameters in script\nSuggestion of default model parameters matching up to baseline in paper, and explaining that if not, issue becomes that its hard to check if model is all correct without code or paper listing every single parameter\n\n\nProvide all required parameters\n\n\n\nClearly present parameters in paper\nThis would not be relevant for STARS framework, but could feed into STRESS-DES work? How it being in a table was so much easier than it being described in the text, and how it being comprehensive of all parameters that get varied (or clear if those are the only things changed from baseline, and so on)\n\n\nProvide calculations\nAgain, might not quite fit into STARS? But if you are going to be mentioning the “pre-processed” values at all, then its important to include the calculation (ideally in the code, as that is the clearest demonstration of exactly what you did)\n\n\nSaves output to a file\nCode structure type thing. This was really handy, particularly for long run times. In doing this, set up in way that is easy to change name and location of output file, and not hard coded to one thing\n\n\nUnderstandable output tables\nNot sure where/if this might fit. But suggestion is to not provide alternative results for same metrics. And if a table or output might be unclear on what need or how to calculate, then providing some docs or something that supports\n\n\nUsing seeds\nExplain how, how it can be quite simple, and what benefit of this is. Could be in enhanced\n\n\nProvide code to produce (a) tables (b) figures (c) in-text results\nExplaining why this is handy (being able to reproduce gives confidence model is running right and also important in science) (plus useful for self too if need to make changes in review process and so on). Noting importance of not forgetting about “in-text” results\n\n\nInstructions on how to run model\nAlready in minimal documentation, but be aware, even as simple as “run this script and here is an example of how” is helpful\n\n\nGrid lines\nMinor, not sure if fits/to include\n\n\nData dictionaries\nMinor and intersects with good commenting, might not be relevant for all, but also worth considering, if people are uploading data as part of model, what the principles and practices and recommendations are for shairng of data and linking to those (as this is likely part of those, along with other things). If there is overlap, make it clear the synergies (e.g. if talks about archiving data, how could do alongside code, and so on).",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/framework.html#presentation-of-the-framework",
    "href": "pages/framework.html#presentation-of-the-framework",
    "title": "11  Modifying the framework",
    "section": "11.2 Presentation of the framework",
    "text": "11.2 Presentation of the framework\nChecklist table with categories, description and space to complete. But in multiple synced formats (perhaps auto conversion github action between them) - e.g. markdown, latex, docx\nDiagram: Think about any ways could adapt this and if would benefit from that. E.g. gradual reveal with GIF. more icons. alt layout. etc.\nWebsite: Could share like https://joss.readthedocs.io/en/latest/paper.html but would want to make sure we are adding sufficient detail on top of checklist.\nInteractive web app: Could make a web app that serves as a “form” for people to complete and then downloaded their completed framework like there is set up for the PRISMA checklist.\n\nTom: Agree this is a good idea",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/framework.html#reflections-from-the-evaluations",
    "href": "pages/framework.html#reflections-from-the-evaluations",
    "title": "11  Modifying the framework",
    "section": "11.3 Reflections from the evaluations",
    "text": "11.3 Reflections from the evaluations\n\nTBC - in progress below…\nAlthough we can comment on how commonly or uncommonly something met a criteria, it is only a small sample (n=6), so when covered in the DES best practice audit, that might be more informative, as baesd on far more studies.\nFrom evaluation of repository:\n\nOpen licence - available on about half, added on request for rest, a very important one as without it you technically cannot reuse someones code\nComplete materials - rarely, so alot of time spent on writing code and figuring out how best to do that\nDependency management - rarely done, and although not an issue in simpler cases, a big impact in more complex\nModel interface - provided for two although not helpful as (1) host no longer operating, and as not focus, didn’t try to run locally, and (2) other app was viewable but didn’t produce the figures and outcomes from paper (was more general, whilst paper delved into scenarios beyond app)\n\nFrom evaluation of paper:\n\nIt often took a long time (up to 2 hours). It would be valuable if the completed checklist were provided along with the paper, like how people often do with equator network reporting guidelines like PRISMA. Consider varying levels detail its possible to complete (yes/no, linking to sections, full detail). Linking to sections is minimum needed to be really useful. Yes/no doesn’t impact time to find information really.\nInput parameters - very handy, as if uncertain in code, use this to check or code\nSome of my evaluations were more light handed (e.g. STRESS 1.2 and 1.3) whilst some were more nit-picky\nRun length - important for knowing when its slow\nScenarios - important to be clear, felt provided in a table was the clearest\n\nReflections on STRESS-DES from having worked on it:\n\n1.2 “how and when calculated” might feel like excess detail\n1.3A providing name for each scenario doesn’t always seem necessary\n2.4 initially found confusing/irrelevant (e.g. “operation of conveyor”)\n3.2 would be helpful to have more examples\n5.1 listing all packages and their versions is never particularly achievable or feasible. More feasible is specifying main package used for model (e.g. simmer, simpy, base R, numpy/pandas) and providing finer details in code (although it was helpful to have version of this package in the paper, when no versions were mentioned in the GitHub)\n5.2 Algorithm used is tricky/uncommon knowledge\n5.3 Event-processing machanism is tricky/uncommon knowledge\nToo many different things in several (could benefit from being clearer, seperated, or into components like 2.5). This was the case for 1.3, 3.3, 4.1, 4.3, 5.1, 5.2, 5.3, 5.4\n\nReflections from comparing what is in STRESS-DES v.s. ISPOR-derived:\n\nScenarios - ISPOR has a simpler description\nModel uncertainities - nice that ISPOR requires this\nISPOR has lots of items that relate to validity and how “good” it is (9 to 18) - whilst STRESS doesn’t touch on this",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modifying the framework</span>"
    ]
  },
  {
    "objectID": "pages/limitations.html",
    "href": "pages/limitations.html",
    "title": "12  Limitations",
    "section": "",
    "text": "12.1 Subjective reproduction success\nAs noted in the research protocol, the assessment of reproduction success involves subjective judgment.\nWe sometimes looked at absolute and percentage difference betweeen our reproduction and the original results. However, these could be significantly influenced by the scale used. For instance, comparing values like 0.1, 0.2, and 0.3 results in larger percentage differences than comparing values such as 10, 15, and 20, even though the latter comparison might hold more practical significance depending on the context.\nThe nature of the metrics being analyzed also affected outcomes. For example, in the health economic study, costs and Quality-Adjusted Life Years (QALYs) appeared similar in value. However, when using them to calculating the Incremental Cost-Effectiveness Ratio (ICER) and the Incremental Net Monetary Benefit (INMB), even minor variations in the costs and QALYs resulted in substantial differences in the ICER and INMB.",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Limitations</span>"
    ]
  },
  {
    "objectID": "pages/limitations.html#reuse-v.s.-reproduction",
    "href": "pages/limitations.html#reuse-v.s.-reproduction",
    "title": "12  Limitations",
    "section": "12.2 Reuse v.s. reproduction",
    "text": "12.2 Reuse v.s. reproduction\nThis study did not attempt to reuse the model/s in new contexts. Despite this, for some studies, I did have to examine the underlying code extensively to understand how to implement the scenarios or process the results. This troubleshooting process was similar to reuse, as I was attempting to adapt the model code to run a new scenario. However, for other studies, I could quite easily run the model and didn’t need to delve into the code.\nAs such, I would not have been able to consistently assess whether the repository provided sufficient information for a comprehensive understanding of the model, necessary for reuse. As such, the focus is moreso on reproduction and what was necessary/helpful/unnecessary for that.\nIt’s important to note that features I didn’t as important for reproduction were indeed specific to reproduction, not to reuse. As such, certain elements that weren’t essential for reproduction might still be highly valuable for reuse (and, we know, often would be), such as:\n\nValidity information from the article\nWeb applications",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Limitations</span>"
    ]
  },
  {
    "objectID": "pages/limitations.html#human-error-in-evaluation",
    "href": "pages/limitations.html#human-error-in-evaluation",
    "title": "12  Limitations",
    "section": "12.3 Human error in evaluation",
    "text": "12.3 Human error in evaluation\nThe evaluations (in particular, of the articles) are likely to be imperfect. It can be difficult to locate information in the papers and errors may have occurred. To help mitigate this we:\n\nHad a second person look at uncertain or unmet criteria\nRevisited all the evaluations side-by-side once complete to check for any inconsistent decisions between them\n\nHowever, this potential for error - as well as the time taken to complete these evaluations - does highlight how it can be so helpful to actually provide a completed reporting guidelines framework, which highlights key reporting points for each study and indicates where to find specific information. This can become especially helpful as studies become more complex with longer articles, or details often spread across several appendices or earlier papers.",
    "crumbs": [
      "Discussion",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Limitations</span>"
    ]
  },
  {
    "objectID": "pages/acknowledgements.html",
    "href": "pages/acknowledgements.html",
    "title": "13  Acknowledgements",
    "section": "",
    "text": "We would like to thank the authors who made their code available under open licences, facilitating our research. We are especially grateful to the following authors for their helpful communication and support throughout the project:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Acknowledgements</span>"
    ]
  }
]