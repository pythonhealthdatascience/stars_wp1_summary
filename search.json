[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Reproducibility Assessments: Summary",
    "section": "",
    "text": "Overview\nThis book describes the findings from work package 1 of the project STARS: Sharing Tools and Artefacts for Reproducible Simulations in healthcare.\nUse the sidebar to navigate through:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Computational Reproducibility Assessments: Summary",
    "section": "",
    "text": "Introduction - background on STARS, pilot work, and the aim of work package 1\nMethods - summarises the methods which are described in detail in our protocol\nResults - describes the results of the reproductions and evaluations, and reflections from the process\nDiscussion - considers and applies findings",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "Computational Reproducibility Assessments: Summary",
    "section": "Funding",
    "text": "Funding\nThis work is supported by the Medical Research Council [grant number MR/Z503915/1].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Computational Reproducibility Assessments: Summary",
    "section": "Licence",
    "text": "Licence\nThis book is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "pages/background.html",
    "href": "pages/background.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Pilot work\nThis book describes work from work package 1 of the project STARS: Sharing Tools and Artefacts for Reproducible Simulations in healthcare.\nThe project was funded for two years by the Medical Research Council, from April 2024 to April 2026. Prior to this funding, some pilot work was carried out by Tom Monks, Alison Harper and Nav Mustafee. This included a review of code sharing practices and the development of the STARS framework.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#pilot-work",
    "href": "pages/background.html#pilot-work",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 Review of healthcare simulation sharing practices\nMonks and Harper (2023) explored how discrete-event simulation (DES) models used in a health context (e.g. health services, health economics) were shared and whether this sharing adhered to best practice. The full study can be viewed at:\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\nIn summary, they identified 564 papers describing a DES model from a health context published from 2019 to 2022. Of these, only 8.3% (47/564) had available model code (either sharing the code themselves, or citing an openly available model). Looking by year, this rose from 4.0% for studies published in 2019, to 9.0% for 2022.\nFurther findings:\n\nMore likely that code was shared if model was:\n\nCreated using free and open source software (FOSS) (28.7%, 29/101)\nA COVID-19 model (24.6%, 17/69)\n\nOf the papers that did share a model:\n\nMost were written in a programming language (66%, 31/47), the rest in a commercial off the shelf Visual Interactive Modelling (VIM) software\nThese were evaluated in a best practice audit, with the results as follows: “In general, computer models and artefacts were published without a DOI (n = 7); rarely included ORCIDs for authors (n = 6); rarely included an open licence (n = 21); were mostly supported by a README file (n = 28); rarely included documentation detailing how to run the model (n = 15); provided no form of formal or informal dependency management (n = 21); did not include any evidence of model testing (n = 3); were almost all downloadable (n = 38); and rarely executable via a cloud-based platform (n = 10).” (Monks and Harper (2023))\n\nFew studies used a reporting guideline (12.8%, 72/564) - mostly using:\n\nOne of the International Society for Pharmacoeconomics and Outcomes Research (ISPOR) publications (n=37), or\nThe Strengthening the Reporting of Empirical Simulation Studies (STRESS) guidelines from Monks et al. (2019) (n=22)\n\n\nThe review concludes that “there are many (simple) best practices the community can adopt, such as the use of trusted archives, and documentation, to improve its sharing”. (Monks and Harper (2023))\n\n\n2.1.2 Pilot STARS framework\nMonks, Harper, and Mustafee (2024) introduces a pilot framework for sharing DES models called STARS: Sharing Tools and Artefacts for Reusable Simulations. Note that this “reusable” is changed into “reproducible” for the STARS project as we build on this work in the current project.\nThe pilot STARS framework consists of essential components (minimum to make models “long-term, citable, functional, appropriately licenced”) and optional components (enhance model “accessibility, understanding, and maintainability”). (Monks, Harper, and Mustafee (2024))\nThe essential components are:\n\nOpen licence\nDependency management\nFOSS model\nMinimum documentation\nOpen Researcher and Contributor IDs (ORCID)\nCitation information\nRemote code repository\nOpen science archive\n\nThe optional components are:\n\nEnhanced documentation\nDocumentation hosting\nOnline coding environment\nModel interface\nWeb app hosting\n\nThis is summarised in the diagram below…\n\n\n\nSTARS framework overview\n\n\nThis was supported by example implementations in Python:\n\nExample 1 - stars-treat-sim\nExample 2 - stars-streamlit-example and stars-simpy-example-docs - with web app and hosted docs\nExample 3 - stars-ciw-example - with web app and hosted docs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#stars-project",
    "href": "pages/background.html#stars-project",
    "title": "2  Introduction",
    "section": "2.2 STARS project",
    "text": "2.2 STARS project\nThe MRC-funded STARS project builds on this pilot work. As stated in the funding application, the objective of this project is “to improve the quality and quantity of shared discrete-event simulation models, tools and other research artefacts:\n\nIdentify barriers, and good practices for sharing simulation models;\nDevelop a new framework for sharing computer models applicable the most common free and open-source languages;\nTest the framework in both retrospective and prospective case studies;\nDevelop online interactive training materials;\nTransfer knowledge of our STARS framework to health data science researchers;\nEnsure sustainability of materials;\nSupport our partner archival journals adopt open science principles and our findings;”\n\nThis objectives will be achieved through four work packages:\nWork package 1: Reproducibility of computational results\n\nAssess the computational reproducibility of eight published DES models created in Python and R.\nEvaluate the publication, code and associated artefacts against reporting guidelines, best practice for code sharing, and criteria from journal artefact badges.\n\nWork package 2: R and Python framework for sharing DES models\n\nImprove the pilot framework (e.g. extend baesd on barriers and enablers to reproduction observed in work package 1, and making it relevant to R models)\nProvide time-saving measures for researchers (e.g. automated support for STRESS, use of large language models (LLM) to support creating of summaries, automated testing of models, continuous integration tools)\n\nWork package 3: Prospective and retrospective application of the framework\n\nApply STARS framework within two case studies (one prospective and one retrospective)\n\nWork package 4: Training\n\nDevelop online interactive training materials to support researchers in using the STARS framework\n\nFrom this point onwards, this site/book summarises the findings from work package 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/background.html#references",
    "href": "pages/background.html#references",
    "title": "2  Introduction",
    "section": "2.3 References",
    "text": "2.3 References\n\n\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html",
    "href": "pages/protocol.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Protocol summary\nFor work package 1, the methods were described in our protocol, Heather et al. (2024).\nThese are summarised below, and modifications from the protocol are also described.\nFor this work, eight published healthcare DES models were selected. These were models with publicly available code under an open licence (either already available or add on request from the STARS team). For each model, the follow stages of work were conducted:\nStage 1: Reproduction - assessing the computational reproducibility of each study\nStage 2: Evaluation - evaluating the publication, code and associated artefacts against sharing and reporting standards\nStage 3: Report and research compendium - summary report and organised repository\nFor each study, a quarto site was produced which shared the results from the reproduction and evaluation and the summary report. Throughout the work, a detailed logbook was kept to keep track of timings and to record work on each stage, such as detailing troubleshooting steps during the reproduction, or uncertainities discussed with another STARS team member during the evaluation.\nSummary diagram\nThis process is captured in the diagram below:",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#protocol-summary",
    "href": "pages/protocol.html#protocol-summary",
    "title": "3  Methods",
    "section": "",
    "text": "Informed authors about study and, if not available, asked if they would be happy to add an open licence to their code\nSet up repository for reproduction with the article and code\nRead the article and defined the scope of the reproduction, archiving the scope (and repository) on Zenodo before proceeding\nLooked over the model code, created a suitable environment with the software and packages required, and then ran the model. For each study, with any issues faced in running the model, troubleshooting was performed such as modifying or writing code. If troubleshooting was exhaused and there were still issues or discrepancies in the results, the original study authors were informed and provided the opportunity to advice on the reason for this (although with no pressure or requirement to do so)\nFor each item in the scope, a decision was made as to whether this had been successfully reproduced or not. This was a subjective decision which allowed some expected deviation due to model stochasticity (for example, lack of seed control).\nThis is timed (including time to produce each item in the scope), and limited to a maximum of 40 horus\n\n\n\nThe publication was evaluated against reporting guidelines for DES:\n\nMonks et al. (2019) - STRESS-DES: Strengthening The Reporting of Empirical Simulation Studies (Discrete-Event Simulation) (Version 1.0).\nZhang, Lhachimi, and Rogowski (2020) - The generic reporting checklist for healthcare-related discrete event simulation studies derived from the the International Society for Pharmacoeconomics and Outcomes Research Society for Medical Decision Making (ISPOR-SDM) Modeling Good Research Practices Task Force reports.\n\nThe model code and associated artefacts (e.g. the GitHub repository shared by the authors) was evaluated against:\n\nThe criteria of badges related to reproducibility from various organisations and journals - namely:\n\nNational Information Standards Organisation (NISO)(NISO Reproducibility Badging and Definitions Working Group (2021))\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\nCenter for Open Science (COS) (Blohowiak et al. (2023))\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (2024))\nPsychological Science (Hardwicke and Vazire (2024) and Association for Psychological Science (APS) (2024))\n\nRecommendations from the pilot STARS framework for the sharing of code and associated materials from discrete-event simulation models (Monks, Harper, and Mustafee (2024)).\n\nThis is timed\n\n\n\nWrote a report summarising the computational reproducibility assessment and evaluation\nRestructed the reposuitory into a “research compendium”, which essentially consisted of organising the repository to ensure it is easy and clear for someone else to re-run. Steps included:\n\nAdding run times to the model notebooks\nWrite a README for the reproduction folder\nMoving data, methods and outputs into seperate folders\nCreating tests which check if a user can get the same results from the model as we did during the reproduction\nA Dockerfile and Docker image published on the GitHub container registry\n\nA second researcher from the STARS team then attempted to use the repository and confirm whether they were able to reproduce the results of the first researcher\nFinally, the repository was archived on Zenodo, and the authors were informed.\n\n\n\n\n\n\n\nWorkflow for STARS work package 1",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#inspiration",
    "href": "pages/protocol.html#inspiration",
    "title": "3  Methods",
    "section": "3.2 Inspiration",
    "text": "3.2 Inspiration\nAs cited in Heather et al. (2024), the protocol was informed by several prior studies assessing computational reproducibility:\n\nKrafczyk et al. (2021): Assessed the reproducibility of seven articles published in the Journal of Computational Physics.\nB. D. K. Wood, Müller, and Brown (2018) and B. Wood et al. (2018): Assessed the reproducibility of 109 published impact evaluations in low- and middle-income countries. Conducted in association with the replication programme of the International Initiative for Impact Evaluation (3ie).\nSchwander et al. (2021): Assessed the reproducibility of four health economic obesity models.\nLaurinavichyute, Yadav, and Vasishth (2022): Assessed the reproducibility of 118 articles published in the Journal of Memory and Language.\nKonkol, Kray, and Pfeiffer (2019): Assessed the reproducibility of 41 geoscientific articles from Copernicus Publications and the Journal of Statistical Software.\n\nIt was also informed by:\n\nAyllón et al. (2021): Article with recommendations on keeping modelling notebooks to support completion of TRACE (TRAnsparent and Comprehensive model Evaluation) documents.\nBerkeley Initiative for Transparency in the Social Sciences (2022): Guidelines on conducting reproductions of published social science research.\nMcManus, Turner, and Sach (2019): Article proposing several possible definitions for success in reproducing or replicating models in health economics.\nMarwick, Boettiger, and Mullen (2018): Article recommending how to structure data analytical work as research compendiums using the R package structure.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#modifications-from-the-protocol",
    "href": "pages/protocol.html#modifications-from-the-protocol",
    "title": "3  Methods",
    "section": "3.3 Modifications from the protocol",
    "text": "3.3 Modifications from the protocol\nThere were two main changes from the protocol…\n\n\n\n\n\n\n\nModification\nDescription and reason for the change\n\n\n\n\nExpanding from 6 to 8 studies\nWhilst partway through the 6th study, we reflected on what we had found so far, and felt it would be beneficial to do a few more studies. The primary motivation for this was to try and include a selection of studies that reflect the range of studies in the literature. So far, we had a few studies where the model code/run times were very large or very small, and we felt it beneficial to try and include some more “medium-sized” studies, as these are more typical of the literature. We also thought it would be good to include an older study (for example, about 10 years ago, if we can find one), as all those included so far were within the last few years, but working with more outdated code will be/can be a problem people encounter.\n\n\nUsing the latest software packages\nIn the protocol, I had planned that - if no versions were provided we select a version of the software and each package that is closest to but still prior to the date of the code archive or paper publication. I kept to this for the Python models (easily set using a conda/mamba environment). However, I had great difficulties attempting to do this in R, and could not successfully backdate both. As such, I used the latest versions of R and each package for those studies\n\n\n\nThere were also some very minor changes, which are explained below…\n\n\n\n\n\n\n\nModification\nDescription and reason for the change\n\n\n\n\nUsing percentage difference in results to help decided reproduction success\nI did explore this, but I ultimately found it very unhelpful, as the percentage difference could be greatly impacted by scale (for example, 0.1 vs 0.2 would appear much greater than 3 vs 4, but the actual meaning of these differences might be similar (e.g. both might be considered a small difference) depending on the scale used and what is being compared - whilst in another context with a different scale, 0.1 vs 0.2 might actually reflect a huge difference!).\n\n\nMoving onto evaluation stage before receive author response regarding reproduction discrepancy or before getting consensus on reproduction\nIn the protocol, we required that authors are contacted if there are any remaining difficulties in running the code or items in the scope that were not reproduced. The authors were given a total of four weeks to respond if they chose to. We had implied that we must wait for this time to pass before continuing to the evaluation stage (since the three stages were presented as being completed one after another). The rationale for this was that the timings for the reproduction would be influenced by whether the evaluation had been completed or not, and vice versa. However, given the many possible influences on the timings, this was considered negligble.We also required consensus on whether items had been succesfully reproduced or not before moving on. In some cases, this was not done, as other team members were not available (e.g. busy or on annual leave) and so could not yet give a second opinion, and so I progressed with the evaluation and got consensus on reproductions afterwards, once they were available.\n\n\nOrganisation of the repository for the research compendium\nIn the protocol, we had planned that seperate folders were created for data, methods and outputs. This was generally followed but, if an alternative structure seemed more suitable. For example, if the original study already divided items well, but perhaps with different folder names or with multiple scripts folders or so on, we might have used that original structure, as it still served the purpose of being clear and easy to re-run, whilst reducing the number of differences compared with the original study.\n\n\nRetrospective archiving\nFor Johnson et al. 2021, I forgot to create a release to archive this repository after defining scope and before proceeding with reproduction. However, I was easily able to resolve this by setting the release to a prior commit, choosing the commit from that timepoint (between scope and reproduction), so it was as if it had been done at the time\n\n\nIdentification of papers\nAlthough 7 papers were identified from the review by Monks and Harper (2023), 1 was identified from additional searches (informal, not systematic) - this was so we could find an older paper.\n\n\nReflections\nI effectivelly created an “additional” stage, which was to create a page of reflections on the reproduction (barriers and faciliators). This was not formally part of the original protocol, but implicit.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#timings",
    "href": "pages/protocol.html#timings",
    "title": "3  Methods",
    "section": "3.4 Timings",
    "text": "3.4 Timings\nAs per the protocol, the reproduction and evaluation stages were timed. Although this was conducted carefully and thoroughly, it will not be perfect, and we recognise some of the potential sources of variation in timings between studies, such as:\n\nWhether we consistently included amendments to the quarto site and repository and time spent on GitHub commits etc. within the timings for the reproduction.\nFor the first R study (Huang et al. (2019)), I initially tried to create an environment with R and package versions prior to the article publication date, although had great difficulties with this and ended up using the latest versions. This contributed to the set-up time during this reproduction, but on later R models (Kim et al. (2021) and Johnson et al. (2021)), based on that experience, I did not attempt to backdate them when getting started.\nAny estimated times (for example, if I were partway through working but someone in the office came to talk to me and I forgot to note the time of that, I might estimate if that were about five or ten minutes of conversation, and set the time accordingly).\nTimings from consensus discussions regarding uncertainities in the evaluation or reproduction (as these might be longer if done in person rather than over email - or vice versa - and I sometimes spent longer on sorting/tidying these for some studies than others, which I would have included in the time)\nWhether subjectively feel that need to add random seeds during reproduction stage, if results vary considerably between each run, and so a certain seed could get a much more similar result than another\n\nAt an estimate, this uncertainty between study timings would lead me to conclude that the timings are approximately correct, give or take up to about four hours. However, this is just an estimate, and it is worth noting that Krafczyk et al. (2021), who also conducted computational reproducibility assessments in a different context, estimated that human error introduced a maximum of 8 hours ambiguity in the timings, due to the “non-precise nature of starting and stopping the watch consistently”.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "pages/protocol.html#references",
    "href": "pages/protocol.html#references",
    "title": "3  Methods",
    "section": "3.5 References",
    "text": "3.5 References\n\n\n\n\nAssociation for Computing Machinery (ACM). 2020. “Artifact Review and Badging Version 1.1.” ACM. https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nAssociation for Psychological Science (APS). 2024. “Psychological Science Submission Guidelines.” APS. https://www.psychologicalscience.org/publications/psychological_science/ps-submissions.\n\n\nAyllón, Daniel, Steven F. Railsback, Cara Gallagher, Jacqueline Augusiak, Hans Baveco, Uta Berger, Sandrine Charles, et al. 2021. “Keeping Modelling Notebooks with TRACE: Good for You and Good for Environmental Research and Management Support.” Environmental Modelling & Software 136 (February): 104932. https://doi.org/10.1016/j.envsoft.2020.104932.\n\n\nBerkeley Initiative for Transparency in the Social Sciences. 2022. “Guide for Advancing Computational Reproducibility in the Social Sciences.” https://bitss.github.io/ACRE/.\n\n\nBlohowiak, Ben B., Johanna Cohoon, Lee de-Wit, Eric Eich, Frank J. Farach, Fred Hasselman, Alex O. Holcombe, Macartan Humphreys, Melissa Lewis, and Brian A. Nosek. 2023. “Badges to Acknowledge Open Practices.” https://osf.io/tvyxz/.\n\n\nHardwicke, Tom E., and Simine Vazire. 2024. “Transparency Is Now the Default at Psychological Science.” Psychological Science 35 (7): 708–11. https://doi.org/10.1177/09567976231221573.\n\n\nHeather, Amy, Thomas Monks, Alison Harper, Navonil Mustafee, and Andrew Mayne. 2024. “Protocol for Assessing the Computational Reproducibility of Discrete-Event Simulation Models on STARS.” Zenodo. https://doi.org/10.5281/zenodo.12179846.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nInstitute of Electrical and Electronics Engineers (IEEE). 2024. “About Content in IEEE Xplore.” IEEE Explore. https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-content.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nKonkol, Markus, Christian Kray, and Max Pfeiffer. 2019. “Computational Reproducibility in Geoscientific Papers: Insights from a Series of Studies with Geoscientists and a Reproduction Study.” International Journal of Geographical Information Science 33 (2): 408–29. https://doi.org/10.1080/13658816.2018.1508687.\n\n\nKrafczyk, M. S., A. Shi, A. Bhaskar, D. Marinov, and V. Stodden. 2021. “Learning from Reproducing Computational Results: Introducing Three Principles and the Reproduction Package.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 379 (2197): 20200069. https://doi.org/10.1098/rsta.2020.0069.\n\n\nLaurinavichyute, Anna, Himanshu Yadav, and Shravan Vasishth. 2022. “Share the Code, Not Just the Data: A Case Study of the Reproducibility of Articles Published in the Journal of Memory and Language Under the Open Data Policy.” Journal of Memory and Language 125 (August): 104332. https://doi.org/10.1016/j.jml.2022.104332.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using R (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\nMcManus, Emma, David Turner, and Tracey Sach. 2019. “Can You Repeat That? Exploring the Definition of a Successful Model Replication in Health Economics.” PharmacoEconomics 37 (11): 1371–81. https://doi.org/10.1007/s40273-019-00836-y.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nMonks, Thomas, and Alison Harper. 2023. “Computer Model and Code Sharing Practices in Healthcare Discrete-Event Simulation: A Systematic Scoping Review.” Journal of Simulation 0 (0): 1–16. https://doi.org/10.1080/17477778.2023.2260772.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.\n\n\nNISO Reproducibility Badging and Definitions Working Group. 2021. “Reproducibility Badging and Definitions.” https://doi.org/10.3789/niso-rp-31-2021.\n\n\nSchwander, Björn, Mark Nuijten, Silvia Evers, and Mickaël Hiligsmann. 2021. “Replication of Published Health Economic Obesity Models: Assessment of Facilitators, Hurdles and Reproduction Success.” Pharmacoeconomics 39 (4): 433–46. https://doi.org/10.1007/s40273-021-01008-7.\n\n\nWood, Benjamin D. K., Rui Müller, and Annette N. Brown. 2018. “Push Button Replication: Is Impact Evaluation Evidence for International Development Verifiable?” PLOS ONE 13 (12): e0209416. https://doi.org/10.1371/journal.pone.0209416.\n\n\nWood, Benjamin, Annette Brown, Eric Djimeu, Maria Vasquez, Semi Yoon, and Jane Burke. 2018. “Replication Protocol for Push Button Replication (PBR).” OSF, January. https://doi.org/https://doi.org/10.17605/OSF.IO/YFBR8.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "pages/badges.html",
    "href": "pages/badges.html",
    "title": "4  Badges",
    "section": "",
    "text": "4.1 Summary\nThere are several different badges with similar criteria. For clarity, a summary provides a comparison in the key criteria between these badges, and then the quoted criteria for each are here laid out in full.\nA set of criteria was outlined in the protocol and used for the evaluation, but this was since revisited, revised and re-evaluated, to ensure the evaluation is more specific to the unique criteria of each badge (and not over-simplified).",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Badges</span>"
    ]
  },
  {
    "objectID": "pages/badges.html#summary",
    "href": "pages/badges.html#summary",
    "title": "4  Badges",
    "section": "",
    "text": "4.1.1 “Open objects” badges\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nACM Artifacts Available\nNISO ORO\nNISO ORO-A\nCOS Open Code\nIEEE Code Available\n\n\n\n\nArtefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)\n✅\n✅\n✅\n✅\n❌\n\n\nOpen licence\n❌\n✅\n✅\n✅\n❌\n\n\nComplete (all artefacts that would be needed for reproduction)\n❌\n❌\n✅\n❌\n✅\n\n\nDocuments (a) how code is used (b) how it relates to article (c) software, systems, packages and versions\n❌\n❌\n❌\n✅\n❌\n\n\n\n\n\n4.1.2 “Object review” badges\n\n\n\n\n\n\n\n\n\nCriteria\nACM Artifacts Evaluated - Functional\nACM Artifacts Evaluated - Reusable\nIEEE Code Reviewed\n\n\n\n\nDocuments (a) inventory of artefacts (b) sufficient description for artefacts to be exercised\n✅\n✅\n❌\n\n\nArtefacts relevant to paper\n✅\n✅\n❌\n\n\nComplete (all artefacts that would be needed for reproduction)\n✅\n✅\n✅\n\n\nScripts can be successfully executed\n✅\n✅\n✅\n\n\nArtefacts are “carefully documented and well-structured to the extent that reuse and repurposing is facilitated”, adhering to norms and standards\n❌\n✅\n❌\n\n\n\n\n\n4.1.3 “Reproduced” badges\nAcross all badges, whether explicitly stated or not, assuming that:\n\nReproduced meaning agreement within tolerance deemed acceptable given context (e.g. not change main claims)\nReproduction is within a reasonable time frame (assuming this to mean no more than a few hours, at the very most)\nMinor troubleshooting is reasonable (e.g. set up of right environment), but nothing extensive\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nACM Results Reproduced\nNISO ROR-R\nIEEE Code Reproducible\nPsychological Science Computational Reproducibility\n\n\n\n\nReproduced results (assuming (a) acceptably similar (b) reasonabe time frame (c) only minor troubleshooting)\n✅\n✅\n✅\n✅\n\n\nREADME file with step-by-step instructions to run analysis\n❌\n❌\n❌\n✅\n\n\nDependencies (e.g. package versions) stated\n❌\n❌\n❌\n✅\n\n\nClear how output of analysis corresponds to article\n❌\n❌\n❌\n✅",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Badges</span>"
    ]
  },
  {
    "objectID": "pages/badges.html#full-criteria",
    "href": "pages/badges.html#full-criteria",
    "title": "4  Badges",
    "section": "4.2 Full criteria",
    "text": "4.2 Full criteria\n\n4.2.1 Association for Computing Machinery (ACM)\n\n4.2.1.1 Artifacts Available\n\n“Author-created artifacts relevant to this paper have been placed on a publically accessible archival repository. A DOI or link to this repository along with a unique identifier for the object is provided.\n\nWe do not mandate the use of specific repositories. Publisher repositories (such as the ACM Digital Library), institutional repositories, or open commercial repositories (e.g., figshare or Dryad) are acceptable. In all cases, repositories used to archive data should have a declared plan to enable permanent accessibility. Personal web pages are not acceptable for this purpose.\nArtifacts do not need to have been formally evaluated in order for an article to receive this badge. In addition, they need not be complete in the sense described above. They simply need to be relevant to the study and add value beyond the text in the article. Such artifacts could be something as simple as the data from which the figures are drawn, or as complex as a complete software system under study.”\n\nAssociation for Computing Machinery (ACM) (2020)\n\n\n\n4.2.1.2 Artifacts Evaluated - Functional\n\n“The artifacts associated with the research are found to be documented, consistent, complete, exercisable, and include appropriate evidence of verification and validation.\n\nDocumented: At minimum, an inventory of artifacts is included, and sufficient description provided to enable the artifacts to be exercised.\nConsistent: The artifacts are relevant to the associated paper, and contribute in some inherent way to the generation of its main results.\nComplete: To the extent possible, all components relevant to the paper in question are included. (Proprietary artifacts need not be included. If they are required to exercise the package then this should be documented, along with instructions on how to obtain them. Proxies for proprietary data should be included so as to demonstrate the analysis.)\nExercisable: Included scripts and/or software used to generate the results in the associated paper can be successfully executed, and included data can be accessed and appropriately manipulated.”\n\n\n\n\n4.2.1.3 Artifacts Evaluated - Reusable\n\n“The artifacts associated with the paper are of a quality that significantly exceeds minimal functionality. That is, they have all the qualities of the Artifacts Evaluated – Functional level, but, in addition, they are very carefully documented and well-structured to the extent that reuse and repurposing is facilitated. In particular, norms and standards of the research community for artifacts of this type are strictly adhered to.”\n\n\n\n4.2.1.4 Results Reproduced\n\n“The main results of the paper have been obtained in a subsequent study by a person or team other than the authors, using, in part, artifacts provided by the author.”\n“…exact…reproduction of results is not required, or even expected. Instead, the results must be in agreement to within a tolerance deemed acceptable for experiments of the given type. In particular, differences in the results should not change the main claims made in the paper.”\nFor reproduced (or replicated) badges, “…a peer-reviewed publication which reports the replication or reproduction must be submitted as evidence, and if awarded, the badge will contain a link to this paper.”\n\n\n\n\n4.2.2 National Information Standards Organisation (NISO)\n\n4.2.2.1 Open Research Objects (ORO) and Open Research Objects - All (ORO-A)\n\n“This badge signals that author-created digital objects used in the research (including data and code) are permanently archived in a public repository that assigns a global identifier and guarantees persistence, and are made available via standard open licenses that maximize artifact availability.\nIf all relevant research objects are made available, the badge is designated by a modifier, e.g., ORO-A. This badge signals that the research publication is reproducible (as described in the NASEM Report).\nNotes:\n\nThis is akin to author-supplied supplemental materials, shared under a standard public license such as an Open Science Initiative (OSI)–approved license for software or a Creative Commons license or public domain dedication for data and other materials.\nThis definition corresponds to the Association for Computing Machinery (ACM) “Artifacts Available” badge, and to the combined Center for Open Science (COS) “Open Data” and “Open Materials” (pertaining to digital objects) badges. (See Appendix A.)\nThe determination of what objects are “relevant” to a research publication is in the hands of the editorial board or leadership members of the community, in addition to the authors themselves.\nFor physical objects relevant to the research, the metadata about the object should be made available.”\n\nNISO Reproducibility Badging and Definitions Working Group (2021)\n\n\n\n4.2.2.2 Research Objects Reviewed (ROR)\nThis badge was not included, as it doesn’t have specific criteria to review against, and simply states that artefacts should be “reviewed according to the criteria provided by the badge issuer” (NISO Reproducibility Badging and Definitions Working Group (2021)).\n\n\n4.2.2.3 Results Reproduced (ROR-R)\n\n“This badge signals that an additional step was taken or facilitated by the badge issuer (e.g., publisher, trusted third-party certifier) to certify that an independent party has regenerated computational results using the author-created research objects, methods, code, and conditions of analysis. Results Reproduced assumes that the research objects were also reviewed. For this reason, a possible distinction for this badge could be as a tag on the ROR badge: ROR-R. This Recommended Practice has adopted the National Academies of Sciences Engineering Medicine, definition of Reproducibility: “We define reproducibility to mean computational reproducibility— obtaining consistent results using the same input data, computational steps, methods, code, and conditions of analysis.””\nNISO Reproducibility Badging and Definitions Working Group (2021)\n\n\n\n\n4.2.3 Center for Open Science (COS)\n\n4.2.3.1 Open Code\n\n“The Open Analytic Code badge is earned by making publicly available the analytical code needed to reproduce the reported analyses.\nCriteria:\n\nDigitally-shareable code is publicly available on an open-access repository (e.g., university repository, a registration on the Open Science Framework, or an independent repository at www.re3data.org). The code must be provided in a format that is time-stamped, immutable, and permanent. It must also have an associated persistent identifier such as a DOI. Note that the code itself may not have a distinct identifier, but rather be included in a repository with related files that is under one distinct identifier.\nThe code has an open license allowing others to copy, distribute, and make use of the code while allowing the licensor to retain credit and copyright as applicable.\nSufficient explanation is present for an independent researcher to understand how the code is used and relates to the reported methodology, including information about versions of software, systems, and packages.”\n\nBlohowiak et al. (2023)\n\n\n\n\n4.2.4 Institute of Electrical and Electronics Engineers (IEEE)\n\n4.2.4.1 Code Available\n\n“The code and/or datasets, including any associated data and documentation, provided by the authors is reasonable and complete and can potentially be used to support reproducibility of the published results.”\nInstitute of Electrical and Electronics Engineers (IEEE) (2024)\n\n\n\n4.2.4.2 Code Reviewed\n\n“The code and/or datasets, including any associated data and documentation, provided by the authors is reasonable and complete, runs to produce the outputs described, and can support reproducibility of the published results.”\nInstitute of Electrical and Electronics Engineers (IEEE) (2024)\n\n\n\n4.2.4.3 Code Reproducible\n\n“This badge signals that an additional step was taken or facilitated to certify that an independent party has regenerated computational results using the author-created research objects, methods, code, and conditions of analysis. Reproducible assumes that the research objects were also reviewed.”\nInstitute of Electrical and Electronics Engineers (IEEE) (2024)\n\n\n\n\n4.2.5 Psychological Science\n\n4.2.5.1 Computational Reproducibility\n\n“All manuscripts submitted to Psychological Science are expected to be computationally reproducible. That is, a reader should be able to run the authors’ analysis scripts on the authors’ data and reproduce the results reported in the manuscript, tables, and figures. As part of the STAR review after conditional acceptance, we will conduct computational reproducibility checks.\nTo ensure their work is computationally reproducible, authors should accompany analysis scripts with a README file that provides clear step-by-step instructions for repeating the analyses. Any software dependencies (e.g., package versions) should be identified. Authors should make clear how the output of the analysis scripts corresponds to the results reported in the manuscript. We strongly encourage authors to perform a reproducibility check within their own team before submitting a manuscript, as this will reduce delays at the conditional acceptance stage.”\nAssociation for Psychological Science (APS) (2024)\n“The Computational Reproducibility Badge will be awarded when authors take the necessary steps to ensure that reported results can be independently reproduced, within a reasonable time frame, by the STAR team. This is a standard that we expect from all articles submitted to Psychological Science; however, we appreciate that the community may currently lack the resources and skills required to write reproducible manuscripts. With that in mind, we are identifying and collating resources (see Educational Resources section) to help the community build the capacity to meet this standard. Eventually, we plan to make verified computational reproducibility a blanket requirement for all empirical articles.”\nHardwicke and Vazire (2024)",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Badges</span>"
    ]
  },
  {
    "objectID": "pages/badges.html#references",
    "href": "pages/badges.html#references",
    "title": "4  Badges",
    "section": "4.3 References",
    "text": "4.3 References\n\n\n\n\nAssociation for Computing Machinery (ACM). 2020. “Artifact Review and Badging Version 1.1.” ACM. https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nAssociation for Psychological Science (APS). 2024. “Psychological Science Submission Guidelines.” APS. https://www.psychologicalscience.org/publications/psychological_science/ps-submissions.\n\n\nBlohowiak, Ben B., Johanna Cohoon, Lee de-Wit, Eric Eich, Frank J. Farach, Fred Hasselman, Alex O. Holcombe, Macartan Humphreys, Melissa Lewis, and Brian A. Nosek. 2023. “Badges to Acknowledge Open Practices.” https://osf.io/tvyxz/.\n\n\nHardwicke, Tom E., and Simine Vazire. 2024. “Transparency Is Now the Default at Psychological Science.” Psychological Science 35 (7): 708–11. https://doi.org/10.1177/09567976231221573.\n\n\nInstitute of Electrical and Electronics Engineers (IEEE). 2024. “About Content in IEEE Xplore.” IEEE Explore. https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-content.\n\n\nNISO Reproducibility Badging and Definitions Working Group. 2021. “Reproducibility Badging and Definitions.” https://doi.org/10.3789/niso-rp-31-2021.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Badges</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html",
    "href": "pages/reproduction.html",
    "title": "5  Reproduction",
    "section": "",
    "text": "5.1 Studies\nShoaib and Ramamohan (2021): Uses python (salabim) to model primary health centres (PHCs) in India. The model has four patient types: outpatients, inpatients, childbirth cases and antenatal care patients. Four model configurations are developed based on observed PHC practices or government-mandated operational guidelines. The paper explores different operational patterns for scenarios where very high utilisation was observed, to explore what might help reduce utilisation of these resources. Note: The article was as Shoaib and Ramamohan (2022), but we used the green open access pre-print Shoaib and Ramamohan (2021). Link to reproduction.\nHuang et al. (2019): Uses R (simmer) to model an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway. The paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR. Link to reproduction.\nLim et al. (2020): Uses python (NumPy and pandas) to model the transmission of COVID-19 in a laboratory. It examines the proportion of staff infected in scenarios varying the: number of shifts per day; number of staff per shift; overall staff pool; shift patterns; secondary attack rate of the virus; introduction of protective measures (social distancing and personal protective equipment). Link to reproduction.\nKim et al. (2021): Adapts a previously developed R (Rcpp, expm, msm, foreach, iterators, doParallel) model for abdominal aortic aneurysm (AAA) screening of men in England. The model is adapted/used to explore different approaches to resuming screening and surgical repair for AAA, as these services were paused or substantially reduced during COVID-19 due to concerns about virus transmission. Link to reproduction.\nAnagnostou et al. (2022): This paper includes two models - we have focussed just on the dynamiC Hospital wARd Management (CHARM) model. CHARM uses python (SimPy) to model intensive care units (ICU) in the COVID-19 pandemic (as well as subsequent stays in a recovery bed). It includes three types of admission to the ICU (emergency, elective or COVID-19). COVID-19 patients are kept seperate, and if they run out of capacity due to a surge in COVID-19 admissions, additional capacity can be pooled from the elective and emergency capacity. Link to reproduction.\nJohnson et al. (2021): This study uses a previously validated discrete-event simulation model, EPIC: Evaluation Platform in chronic obstructive pulmonary disease (COPD). The model is written in C++ with an R interface, using R scripts for execution. The model is adapted to evaluate the cost-effectiveness of 16 COPD case detection strategies in primary care, comparing costs, quality-adjusted life years (QALYs), incremental cost-effectiveness ratios (ICER), and incremental net monetary benefits (INMB) across scenarios. Sensitivity analyses are also conducted. Link to reproduction.\nHernandez et al. (2015): This study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense counter-measures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput. They use python for most of the analysis (with SimPy for the simulation component), but R to produce the plots and tables for the paper. Link to reproduction.\nWood et al. (2021): This study uses discrete-event simulation (R (base R, dplyr, tidyr)) to explore the deaths and life years lost under different triage strategies for an intensive care unit, relative to a baseline strategy. The unit is modelled with 20 beds (varied from 10 to 200 in sensitivity analyses). Three different triage strategies are explored, under three different projected demand trajectories. Link to reproduction.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#scope-and-reproduction",
    "href": "pages/reproduction.html#scope-and-reproduction",
    "title": "5  Reproduction",
    "section": "5.2 Scope and reproduction",
    "text": "5.2 Scope and reproduction\n\n\n\n\n\n\n\n\n\nStudy\nScope\nSuccess\nTime\n\n\n\n\nShoaib and Ramamohan 2022\n17 items:• 1 table• 9 figures• 7 in-text results\n16 out of 17 (94.1%)\n28h 14m\n\n\nHuang et al. 2019\n8 items:• 5 figures• 3 in-text results\n3 out of 8 (37.5%)\n24h 10m\n\n\nLim et al. 2020\n9 items:• 5 tables• 4 figures\n9 out of 9 (100%)\n12h 27m\n\n\nKim et al. 2021\n10 items:• 3 tables• 6 figures• 1 in-text result\n10 out of 10 (100%)\n14h 42m\n\n\nAnagnostou et al. 2022\n1 item:• 1 figure\n1 out of 1 (100%)\n2h 11m\n\n\nJohnson et al. 2021\n5 items:• 1 table• 4 figures\n4 out of 5 (80%)\n19h 49m\n\n\nHernandez et al. 2015\n8 items:• 6 figures• 2 tables\n1 out of 8 (12.5%)\n17h 56m\n\n\nWood et al. 2021\n5 items:• 4 figures• 1 table\n5 out of 5 (100%)\n3h 50m\n\n\n\n\n\n\n\n\n\nReproduction reflections\n\n\n\n\n\nFor the studies where I didn’t manage to fully reproduce results despite troubleshooting, my reflections on what I think to be the primary reason for not managing to reproduce results in each case were:\nShoaib and Ramamohan (2021): No specific suggestions - had to write code to run scenarios and process results, so it could be that I hadn’t done this all exactly the same as in the original study.\nHuang et al. (2019): No specific suggestions - had to write code to run scenarios and process results, so it could be that I hadn’t done this all exactly the same as in the original study.\nJohnson et al. (2021): No specific suggestions - although note that their results on GitHub appeared to likewise have the same discrepancy compared with the article, so it appears there might have been a minor change to the code/parameters made from that used to produce the article figure that might explain this.\nHernandez et al. (2015): This appears likely to be due to a parameter somewhere being not quite right. Ivan Hernandez suggested that it could be a random seed or that the optimisation doesn’t have enough runs - although was using the same seed and played around with run number. The other suggestion was that the version on GitHub might not have exactly the right version of inputs - and interestingly, he had an earlier version of the paper that had more similar results (our discrepancy was in range on Y axis, and he had an earlier version with range of 0 to 15000). This reaffirms that a minor parameter difference is the likely reason for the discrepancy, with that having impacted across nearly all the results.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#time-to-complete-reproductions",
    "href": "pages/reproduction.html#time-to-complete-reproductions",
    "title": "5  Reproduction",
    "section": "5.3 Time to complete reproductions",
    "text": "5.3 Time to complete reproductions\n\nNon-interactive figure:\n\n\n\n\n\n\n\n\n\nVersion of non-interactive figure used in article:\n\n\n\n\n\n\n\n\n\nInteractive figure:\n\n\n                                                \n\n\n\n\n\n\n\n\nTiming reflections\n\n\n\n\n\nThere was a large amount of variation in the time each reproduction took. For each study, I have also reflected on what I think were the primary reasons behind things being quicker or slower:\nShoaib and Ramamohan (2021) - 28h 14m:\n\nThis had lots of items to reproduce (17)\nTook me longer than usual to set up environment as needed specific package versions and had some confusion around package statistics which is base but I had mixed up with one that can be imported from conda/pypi\nMost of the time was dedicated to identifying parameters for each scenario, writing code to run the scenarios (and run these programmatically), and writing code to process the results into figures and tables.\n\nHuang et al. (2019) - 24h 10m:\n\nTime-consuming aspects were modifying the code from the app so I could run it, writing code for each scenario, and writing code for figures (it took me quite a whiole to work out transformations for axes and how to standardise density)\n\nLim et al. (2020) - 12h 27m:\n\nLots of time was spent setting up model so could run programmatically, writing code for scenarios, and writing code to produce figures.\n\nKim et al. (2021) - 14h 42m:\n\nTime-consuming aspect was largely writing code to produce tables and figures (including figuring out how to process).\n\nAnagnostou et al. (2022) - 2h 11m:\n\nThis only had one item to reproduce.\nIt required very little troubleshooting - just had to write code to produce figure, although it was relatively simple.\n\nJohnson et al. (2021) - 19h 49m:\n\nMost time was spent on writing code for sensitivity analysis, and to produce tables and figures (which included working out which results tables / columns / scenarios to use, how to transform columns, and how to calculate features like efficiency frontier)\n\nHernandez et al. (2015) - 17h 56m:\n\nTime-consuming aspects were writing code for scenarios and for figures and tables, and troubleshooting parameter discrepancies.\n\nWood et al. (2021) - 3h 50m:\n\nRan quick as code for model didn’t require troubleshooting (included all correct parameters and scenarios), and as they provided code to produce the figures and tables.\n\nRegarding run time of the models themselves, for longer models, I sometimes experimented with parameters to run it quicker, in order to more easily troubleshoot, else I would run for a long time and then discover XYZ is wrong. Although the model run time itself is not included in the times, it is important to note, as some models took several days, and so in practice, this would impact on someone if they were trying to run a model within a short amount of time.\nRegarding the reproduction run times, I think the main reflection from above is that including code with correct parameters and scenarios, and code to make the figures and tables, has a really big impact.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#model-run-times",
    "href": "pages/reproduction.html#model-run-times",
    "title": "5  Reproduction",
    "section": "5.4 Model run times",
    "text": "5.4 Model run times\nFor reference, the run times for the models are detailed below. It’s worth being aware that these are not compared on a “level playing field” as they were run on different machines, and with/without parallel processing or parallel terminals.\n\n\n\n\n\n\n\n\n\nStudy\nMachine specs\nModel run time\nAdditional details\n\n\n\n\nShoaib and Ramamohan (2021)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\n22 minutes 26 seconds\nThis is based on the combined runtime of the notebooks which use 10 replications (rather than 100) and parallel processing, but with each notebook run seperately.\n\n\nHuang et al. (2019)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\n29 minutes 10 seconds\nCombined time from notebooks run seperately.\n\n\nLim et al. (2020)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\n49 minutes and 17 seconds\n\n\n\nKim et al. (2021)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\n6 hours 53 minutes\nWe reduced the number of patients in the simulation from 10 million to 1 million, to improve run times. Note, you can expect the runtime to be notably longer on machines with lower specs than this. For example, I ran surveillance scenario 0 on an Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux. The runtime increased from 4 minutes 28 seconds up to 21 minutes 59 seconds.\n\n\nAnagnostou et al. (2022)\nIntel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux\nOnly a few seconds.\n\n\n\nJohnson et al. (2021)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\nAt least 1 day 11 hours\nIt was run from the command line via multiple terminals simultaneously - hence, the exact times are impacted by the number being ran at once. The base case scenarios were ran at the same time, and took an overall total time of 20 hours 40 minutes. The sensitivity analysis scenario were all ran at the same time, and took an overall total of 1 day 10 hours 57 minutes. Four files (scenarios 2 and 3) had to be re-run seperately after fixing a mistake, and when just those were run, the total was 21 hours 26 minutes (quicker than when they were run as part of the full set of scenarios, when they took up to 1.3 days). Based on this, if you were to run all scenarios at once in parallel in seperate terminals, you could expect a run time of at least 1 day 11 hours, but would be higher than that (since the more run at once, the lower it takes, as you can see from the scenario 2 and 3 times above).\n\n\nHernandez et al. (2015)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\n9 hours 16 minutes\nThis involved running the models within each experiment in parallel, but each of the experiment files seperately. If these are run at the same time (which I could do without issue), then you will be able to run them all within 4 hours 28 minutes (the longest experiment) (or a little longer, due to slowing speeds from running at once). Also, this has excluded one of the variants for Experiment 3, which I did not run as it had a very long run time (quoted to be 27 hours in the article) and as, regardless, I had not managed to reproduce the other sub-plots in the figure for that experiment.\n\n\nWood et al. (2021)\nIntel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux\n48 hours 25 minutes\nIt ran in a single loop, so required the computer to remain on for that time. This time includes all scenarios - but, for just the base scenario, the run time was 2 hours 3 minutes.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reproduction.html#references",
    "href": "pages/reproduction.html#references",
    "title": "5  Reproduction",
    "section": "5.5 References",
    "text": "5.5 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\n———. 2022. “Simulation Modeling and Analysis of Primary Health Center Operations.” SIMULATION 98 (3): 183–208. https://doi.org/10.1177/00375497211030931.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reproduction</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html",
    "href": "pages/reflections.html",
    "title": "6  Reflections from reproductions",
    "section": "",
    "text": "6.1 Summary\nThis page describes the facilitators and barriers encountered in each reproduction, presented as a series of recommendations. These are grouped into two themes:\nWith each section, I have created a table which evaluates whether the facilitators were fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A) for each study.\nLinks to each study:\nThis is a summary of all the items that were evaluated (with ✅🟡❌) below.\nIt excludes the “Other section”. It also excludes a few criteria which weren’t felt particularly relevant/suitable to mark as met or not. These were “optimise model run time” and “avoid large file sizes”.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#recommendations-to-support-reproduction.",
    "href": "pages/reflections.html#recommendations-to-support-reproduction.",
    "title": "6  Reflections from reproductions",
    "section": "6.2 Recommendations to support reproduction.",
    "text": "6.2 Recommendations to support reproduction.\n\n\n6.2.1 Set-up\n\n\n\n\n\n\nShare code with an open licence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n❌\n✅\n✅\n✅\n❌\n❌\n\n\n\nAbsence of licence prevents legal reuse of code for reproduction.\n\n\n\n\n\n\n\n\n\nLink publication to a specific version of the code\n\n\n\n\n\nWhether or not they did it:\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nThen, considering how “applicable” it feels… although you could argue those where I have marked “non-applicable” would become applicable with the addition of any new commits to the repository.\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\nN/A\nN/A\nN/A\n❌\nN/A\n❌\nN/A\nN/A\n\n\n\nShoaib and Ramamohan (2021), Hernandez et al. (2015): N/A. Not met but not an issue as last commit (with exception of any related to licence from our communication) is prior to publication, so marked as not applicable.\nHuang et al. (2019), Lim et al. (2020), Anagnostou et al. (2022), Wood et al. (2021): N/A. Only 1 commit (excluding repo creation), or only commits on one day.\nKim et al. (2021): Not met. Had commits to their GitHub repository after the publication date - though these largely appear related to a different model in the same repository, or for a fix.\nJohnson et al. (2021): Not met. Had commits to their GitHub repository after the publication date. It wasn’t clear which version aligned with the publication. However, the most recent commits add clear README instructions to the repository. We decided to use the latest version of the repository, but it would have beneficial to have releases/versions/a change log that would help to outline the commit history in relation to the publication and any subsequent changes.\n\n\n\n\n\n\n\n\n\nList required packages and versions\n\n\n\n\n\nPackages and versions\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n\n\n\nPackages\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n✅\n✅\n🟡\n🟡\n❌\n\n\n\nShoaib and Ramamohan (2021): Not met. This became a time-consuming issue as it took a while to identify a dependency that was needed for the code to work (greenlet) (based on reading the documentation for salabim), and a while longer to realise I had installed another package when the package I needed was in base (statistics).\nHuang et al. (2019): Not met. However, this was fairly easily resolved based on imports to .R script, and then on extra imports suggested by RStudio when I tried and failed to run the script.\nLim et al. (2020): Partially met. The only packages needed (numpy and pandas) are mentioned in the paper (although only listed as imports in the script).\nKim et al. (2021): Fully met. Provides commands to install packages required at the start of scripts, which I could then easily base renv on automatically (as it detects them).\nAnagnostou et al. (2022): Fully met. Provides requirements.txt\nJohnson et al. (2021): Partially met. DESCRIPTION file accompanying epicR package contained some but not all dependencies.\nHernandez et al. (2015): Partially met. Some (but not all) of the required packages were listed in the paper. Of particular note, this depended on having a local package myutils/, which was another GitHub repository from the author. This was not mentioned anywhere, and so required to notice this was needed.\nWood et al. (2021): Not met. However, easily resolved based on imports to .R script.\nReflections:\n\nThe import statements can be sufficient in indicating all the packages required but this is not always the case if there are “hidden”/unmentioned dependencies that don’t get imported\n\nTom: Given that import statements are not always enough, then I would argue more is needed.\nTom: Useful to know that renv “detects” dependencies listed as imports in scripts.\n\nThere are various options for listing the packages (e.g. comprehensive import statements, installation lines in the script, environment files, package DESCRIPTION file).\nIdeally mention this in repository, not just the paper.\nIf there are local dependencies (e.g. other GitHub repositories), make sure to (a) mention and link to these repositories, so it is clear they are also required, and (b) include licence/s in those repositories also, so they can be used.\nThis was a common issue.\n\nVersions\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n\n\n\nShoaib and Ramamohan (2021): Not met. I had to backdate the package versions as the model didn’t work with the latest.\nHuang et al. (2019): Not met. I initially tried to create an environment with R and package versions that were prior to the article publication date. However, I had great difficulties implementing this with R, and never managed to successfully do this. This was related to:\n\nThe difficulty of switching between R versions\nProblems in finding available/a source for specific package versions for specific versions or R\n\nLim et al. (2020): Partially met. Provides major Python version, but chose minor and the package versions based on article publication date.\nKim et al. (2021): Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.\nAnagnostou et al. (2022): Partially met (depending on how strict you are being). The Python version was stated in the paper, and the SimPy version was stated in the complementary app repository (although neither were mentioned in the model repository itself). Requirements file just contains one thing - simpy - with no version.\nJohnson et al. (2021): Partially met. R version given in paper. DESCRIPTION file contains minimum versions for some but not all packages.\nHernandez et al. (2015): Partially. Versions of Python, R and some (but not all) packages given in the paper. Some versions weren’t very specific (e.g. Python 2.7 v.s. something specific like 2.7.12)\nWood et al. (2021): Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.\nReflections:\n\nModels will sometimes work with the latest versions of packages, but likewise, you will sometimes need to backdate as it no longer works with the latest\nFor Python, it was very easy to “backdate” the python and package versions. However, I found this very difficult to in R, and ended up always using the latest versions.\nVersions are sometimes provided elsewhere (e.g. in paper, in other repositories), but would be handy to be in model repository itself.\nHandy to provide specific versions too, particularly when there can be reasonably large changes between minor versions.\nThis was a very common issue.\nTom: For R, we are sort of moving towards a pre-built container for an R reproducible pipeline\n\nResponse: Though being aware that it is possible to successfully reproduce without backdating - didn’t run into issues with it for the R models - though that doesn’t mean you wouldn’t - but it is a sort of “characteristic” of R, that it is supposed to be less changeable than Python in this regard. Though obviously no guarantees. And not a fair comparison, as I didn’t try to run the Python ones without backdating.\n\n\n\n\n\n\n\n6.2.2 Running the model\n\n\n\n\n\n\nProvide code for all scenarios and/or sensitivity analyses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\nN/A\n🟡\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. There were several instances where it took quite a while to understand how and where to modify the code in order to run scenarios (e.g. no arrivals, transferring admin work, reducing doctor intervention in deliveries).\nHuang et al. (2019): Not met. Set up a notebook to programmatically run the model scenarios. It took alot of work to modify and write code that could run the scenarios, and I often made mistakes in my interpretation for the implementation of scenarios, which could be avoided if code for those scenarios was provided.\nLim et al. (2020): Not met. Several parameters or scenarios were not incorporated in the code, and had to be added (e.g. with conditional logic to skip or change code run, removing hard-coding, adding parameters to existing).\nKim et al. (2021): Not met. Took alot of work to change model from for loop to function, to set all parameters as inputs (some were hard coded), and add conditional logic of scenarios when required.\nAnagnostou et al. (2022): Not applicable. No scenarios.\nJohnson et al. (2021): Partially met. Has all base case scenarios, but not sensitivity analysis.\nHernandez et al. (2015): Not met. Took a while to figure out how to implement scenarios.\nWood et al. (2021): Fully met.\nReflections:\n\nCommon issue\nTime consuming and tricky to resolve\nTom: This is a headline. Also, links to importance of reproducible analytical pipelines (RAP) for simulation.\n\n\n\n\n\n\n\n\n\n\nEnsure model parameters are correct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n🟡\n❌\n🟡\n✅\n✅\n✅\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Partially met. Script is set with parameters for base configuration 1, with the exception of number of replications.\nHuang et al. (2019): Not met. The baseline model in the script did not match the baseline model (or any scenario) in the paper, so had to modify parameters.\nLim et al. (2020): Partially met. The included parameters were corrected, but the baseline scenario included varying staff strength to 2, and the provided code only varied 4 and 6. I had to add some code that enabled it to run with staff strength 2 (as there were an error that occured if you tried to set that).\nKim et al. (2021): Fully met.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): Fully met. Base case parameters all correct.\nHernandez et al. (2015): Not met. As agreed with the author, this is likely the primary reason for the discrepancy in these results - they are very close, and we see similar patterns, but not reproduced. Unfortunately, several parameters were wrong, and although we changed those we spotted, we anticipate there could be others we hadn’t spotted that might explain the remaining discrepancies.\nWood et al. (2021): Fully met.\nReflections:\n\nAt least provide a script that can run the baseline model as in the paper (even if not providing the scenarios)\nThis can introduce difficulties - when some parameters are wrong, you rely on the paper to check which parameters are correct or not, but if the paper doesn’t mention every single parameter (which is reasonably likely, as this includes those not varied by scenarios), then you aren’t able to be sure that the model you are running is correct.\nThis can make a really big difference, and be likely cause of managing to reproduce everything v.s. nothing, if it impacts all aspects of the results.\nTom: I think this comes back to minimum verification as well. I think the “at least for one scenario” idea of yours is excellent.\n\n\n\n\n\n\n\n\n\n\nControl randomness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n✅\n✅\n✅\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The lack of seeds wasn’t actually a barrier to the reproduction though due to the replication number. I later add seeds so my results could be reproduced, and found that the ease of setting seeds with salabim was a greater facilitator to the work. I only had to change one or two lines of code to then get consistent results between runs (unlike other simulation software like SimPy where you have to consider the use of seeds by different sampling functions). Moreover, by default, salabim would have set a seed (although overridden by original authors to enable them to run replications).\nHuang et al. (2019): Not met. It would have been beneficial to include seeds, as there was a fair amount of variability, so with seeds I could then I could be sure that my results do not differ from the original simply due to randomness.\nLim et al. (2020): Not met. The results obtained looked very similar to the original article, with minimal differences that I felt to be within the expected variation from the model stochasticity. However, if seeds had been present, we would have been able to say with certainty. I did not feel I needed to add seeds during the reproduction to get the same results.\nKim et al. (2021): Fully met. Included a seed, although I don’t get identical results as I had to reduce number of people in simulation.\nAnagnostou et al. (2022): Fully met. The authors included a random seed so the results I got were identical to the original (so no need for any subjectivity in deciding whether its similar enough, as I could perfectly reproduce).\nJohnson et al. (2021): Fully met. At start of script, authors set.seed(333).\nHernandez et al. (2015): Fully met. This ensured consistent results between runs of the script, which was really helpful.\nWood et al. (2021): Fully met. Sets seed based on replication number.\nReflections:\n\nDepending on your model and the outputs/type of output you are looking at, the lack of seeds can have varying impacts on the appearance of your results, and can make the subjective judgement of whether results are consistent harder (if discrepancies could be attributed to not having consistent seeds or not).\nIt can be really quite simple to include seeds.\nOver half of the studies did include seed control in their code.\nTom: There seems little argument against doing this. worth noting that commerical software does this for you and possibly explains why authors didn’t do this themselves if that was their background (lack of knowledge?).\nTom: Note simpy is independent of any sampling mechanism. We could just use python’s random module and set a single seed if needed (although you lose CRN) and we can setup our models so that we only need to set a single seed.\nTom: A key part of STARS 2.0 for reproducibility\n\n\n\n\n\n\n6.2.3 Outputs\n\n\n\n\n\n\nInclude code to calculate all required model outputs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. Had to add some outputs and calculations (e.g. proportion of childbirth cases referred, standard deviation)\nHuang et al. (2019): Not met. It has a complicated output (standardised density of patient in queue) that I was never certain on whether I correctly calculated. Although it outputs the columns required to calculate it, due its complexity, I feel this was not met, as it feels like a whole new output in its own right (and not just something simple like a mean).\nLim et al. (2020): Not met. The model script provided was only set up to provide results from days 7, 14 and 21. The figures require daily results, so I needed to modify the code to output that.\nKim et al. (2021): Not met. Had to write code to find aorta sizes of people with AAA-related deaths.\nAnagnostou et al. (2022): Fully met. Although worth noting this only had one scenario/version of model and one output to reproduce.\nJohnson et al. (2021): Note met. It has an output that is in “per 1000” and, although it outputs the columns required to calculate this, I found it very tricky to work out which columns to use and how to transform them to get this output, and so feel this is not met (as feels like a seperate output, and not something simple like a mean, and as it felt so tricky to work out).\nHernandez et al. (2015): Fully met.\nWood et al. (2021): Fully met.\nReflections:\n\nCalculate and provide all the outputs required\nAppreicate this can be a bit “ambiguous” (e.g. if its just plotting a mean or simple calculation, then didn’t consider that here) (however, combined with other criteria, we do want them to provide code to calculate outputs, so we would want them to provide that anyway)\nTom: This is a headline. I suspect we can find supporting citations elsewhere from other fields. Its a reporting guideline thing too, but in natural language things can get very ambiguous still! Would be good to make that point as well I think.\n\n\n\n\n\n\n\n\n\n\nInclude code to generate the tables, figures and other reported results\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\n❌\n❌\n🟡\n🟡\n✅\n\n\n\nProvide code to process results into tables\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\nN/A\n🟡\n❌\nN/A\n❌\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met.\nHuang et al. (2019): Not applicable. No tables in scope.\nLim et al. (2020): Partially met. It outputs the results in a similar structure to the paper (like a section of a table). However, it doesn’t have the full code to produce a table outright, for any of the tables, so additional processing still required.\nKim et al. (2021): Not met. Had to write code to generate tables, which included correctly implementing calculation of excess e.g. deaths, scaling to population size, and identify which columns provide the operation outcomes.\nAnagnostou et al. (2022): Not applicable. No tables in scope.\nJohnson et al. (2021): Not met. Had to write code to generate tables, which took me a while as I got confused over thinks like which tables / columns / scenarios to use.\nHernandez et al. (2015): Not met.\nWood et al. (2021): Fully met.\nReflections:\n\nIt can take a bit of time to do this processing, and it can be tricky/confusing to do correctly, so very handy for it to be provided.\nCommon issue.\n\nProvide code to process results into figures\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n❌\n❌\n🟡\n🟡\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met.\nHuang et al. (2019): Not met. Had to write code from scratch. For one of the figures, it would have been handy if informed that plot was produced by a simmer function (as didn’t initially realise this). It also took a bit of time for me to work out how to transform the figure axes as this was not mentioned in the paper (and no code was provided for these). It was also unclear and a bit tricky to work out how to standardise the density in the figures (since it is only described in the text and no formula/calculations are provided there or in the code).\nLim et al. (2020), Kim et al. (2021) and Anagnostou et al. (2022): Not met. However, the simplicity and repetition of the figures was handy.\nJohnson et al. (2021): Partially met. For Figure 3, most of the required code for the figure was provided, which was super helpful. However, this wasn’t complete, and for all others figures, I had to start from scratch writing the code.\nHernandez et al. (2015): Partially met. Provides a few example ggplots, but these are not all the plots, nor exactly matching article, nor including any of the pre-processing required before the plots, and so could only serve as a starting point (though that was still really handy).\nWood et al. (2021): Fully met. Figures match article, with one minor exception that I had to add smoothing to the lines on one of the figures.\nReflections:\n\nIt can take a bit of time to do this processing, particularly if the figure involves any transformations (and less so if the figure is simple), so very handy for it to be provided.\nAlso, handy if the full code can be provided for all figures (although partial code is more helpful than none at all).\nCommon issue.\n\nProvide code to calculate in-text results\nBy “in-text results”, I am referred to results that are mentioned in the text but not included in/cannot be deduced from any of the tables or figures.\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\nN/A\n❌\nN/A\nN/A\nN/A\nN/A\n\n\n\nShoaib and Ramamohan (2021), Huang et al. (2019), Kim et al. (2021): Not met.\nLim et al. (2020), Anagnostou et al. (2022), Johnson et al. (2021), Hernandez et al. (2015), Wood et al. (2021): Not applicable (no in-text results).\nReflections:\n\nProvide code to calculate in-text results\nUniversal issue, for those with in-text results not otherwise captured in tables and figures",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#recommendations-to-support-troubleshooting-and-reuse",
    "href": "pages/reflections.html#recommendations-to-support-troubleshooting-and-reuse",
    "title": "6  Reflections from reproductions",
    "section": "6.3 Recommendations to support troubleshooting and reuse",
    "text": "6.3 Recommendations to support troubleshooting and reuse\n\n\n6.3.1 Design\n\n\n\n\n\n\nSeparate model code from applications\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\nN/A\n❌\nN/A\nN/A\n✅\nN/A\nN/A\nN/A\n\n\n\nFor studies where this is relevant (i.e. with applications):\n\nHuang et al. (2019): Not met. The model code was provided within the code for a web application, but the paper was not focused on this application, and instead on specific model scenarios. I had to extract the model code and transform it into a format that was “runnable” as an R script/notebook.\nAnagnostou (2022): Fully met. Code for web app in a seperate repository to model code.\n\nFor other studies, regarding whether the model code was in a “runnable” format and not embedded within anything else…\n\nShoaib and Ramamohan (2021): Provided as a single .py file which ran model with function main().\nLim et al. (2020): Provided as a single .py file which ran the model with a for loop.\nKim et al. (2021): Has seperate .R scripts for each scenario which ran the model by calling functions from elsewhere in repository.\nJohnson et al. (2021): Model provided as a package (which is an R interface for the C++ model).\nHernandez et al. (2015): The model (python code) can be run from main.py.\nWood et al. (2021): Provided as a single .R file which ran the model with a for loop.\n\nReflections:\n\nIf you are presenting the results of a model, then provide the code for that model in a “runnable” format.\nThis was an uncommon issue.\n\n\n\n\n\n\n\n\n\n\nAvoid hard-coded parameters\n\n\n\n\n\nDon’t hard code parameters that you will want to change for scenario analyses\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n🟡\n❌\n✅\nN/A\n✅\n🟡\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. Although some parameters sat “outside” of the model within the main() function (and hence were more “changeable”, even if not “changeable” inputs to that function, but changed directly in script). However, many of the other parameters were hard-coded within the model itself. It took time to spot where these were and correctly adjust them to be modifiable inputs.\nHuang et al. (2019): Partially met. Pretty much all of the parameters that we wanted to change were not hard coded and were instead inputs to the model function simulate_nav(). However, I did need to add an exclusive_use scenario which conditionally changed ir_resources, but that is the only exception. I also add ed_triage as a changeable input but didn’t end up needing that to reproduce any results (was just part of troubleshooting). I also\nLim et al. (2020): Not met. Some parameters were not hard coded within the model, but lots of them were not.\nKim et al. (2021): Fully met. All model parameters could be varied from “outside” the model code itself, as they were provided as changeable inputs to the model.\nAnagnostou et al. (2022): N/A as no scenarios.\nJohnson et al. (2021): Fully met. All model parameters could be varied from “outside” the model code itself, as they were provided as changeable inputs to the model.\nHernandez et al. (2015): Partially met. Did not hard code runs, population, generations, and percent pre-screened. However, did hard code other parameters like bi-objective v.s tri-objective model and bounding. Also, it was pretty tricky to change percent pre-screened, as it assumed you provided a .txt file for each %.\nWood et al. (2021): Fully met. All model parameters for the scenarios/sensitivity analysis could be varied from “outside” the model code itself.\nReflections:\n\nIt can be quite difficult to change parameters that are hard coded into the model. Ideally, all the parameters that a user might want to change should be easily changeable and not hard coded.\nThis is a relatively common issue.\nThere is overlap between this and whether the code for scenarios is provided (as typically, the code for scenario is conditionally changing parameter values, although this can be facilitated by not hard coding the parameters, so you call need to change the values from “outside” the model code, rather than making changes to the model functions themselves). Hence, have included as two seperate reflections.\nImportant to note that we evaluate this in the context of reproduction - and have not checked for hard-coded parameters outside the specified scenario analyses, but that someone may wish to alter if reusing the model for a different analysis/context/purpose.\n\n\n\n\n\n\n\n\n\n\nMinimise code duplication\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The model often contained very similar blocks of code before or after warm-up.\nHuang et al. (2019): Fully met.\nLim et al. (2020): Fully met.\nKim et al. (2021): Not met. There was alot of duplication when running each scenario (e.g. repeated calls to Eventsandcosts, and repeatedly defining the same parameters). This meant, if changing a parameter that you want to be consistent between all the scripts (e.g. number of persons), you had to change each of the scripts one by one.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): Not met. There was alot of duplication when running each scenario. This meant, when amending these for the sensitivity analysis, I would need to change the same parameter 12 times within the script, and for changes to all, changing it 12 times in 14 duplicate scripts. Hence, it was simpler to write an R script to do this than change it directly, but for base case, I had to make sure I carefully changed everything in both files.\nHernandez et al. (2015): Fully met.\nWood et al. (2021): Fully met.\nReflections: Large amounts of code duplication are non-ideal as they can:\n\nMake code less readable\nMake it trickier to change universal parameters\nIncrease the likelihood of introducing mistakes\nMake it trickier to set up scenarios/sensitivity analyses\n\n\n\n\n\n\n6.3.2 Clarity\n\n\n\n\n\n\nComment sufficiently\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n✅\n🟡\n🟡\n❌\n🟡\n❌\n\n\n\nShoaib and Ramamohan (2021) and Huang et al. (2019): Not met. Would have benefitted from more comments, as it took some time to ensure I have correctly understood code, particularly if they used lots of abbreviations.\nLim et al. (2020): Fully met. There were lots of comments in the code (including doc-string-style comments at the start of functions) that aided understanding of how it worked.\nKim et al. (2021): Partially met. Didn’t have any particular issues in working out the code. There are sufficient comments in the scenario scripts and at the start of the model scripts, although within the model scripts, there were sometimes quite dense sections of code that would likely benefit from some additional comments.\nAnagnostou et al. (2022): Partially met. Didn’t have to delve into the code much, so can’t speak from experience as to whether the comments were sufficient. From looking through the model code, several scripts have lots of comments and docstrings for each function, but some do not.\nJohnson et al. (2021): Not met. Very few comments in the Case_Detection_Results...Rmd files, which were the code files provided.\nHernandez et al. (2015): Partially met. There are some comments and doc-strings, but not comprehensively.\nWood et al. (2021): Not met. Very few comments, so for the small bit of the code that I did delve into, took a bit of working out what different variables referred to.\nReflections:\n\nWith increasing code complexity, the inclusion of sufficient comments becomes increasingly important, as it can otherwise be quite time consuming to figure out how to fix and change sections of code\nDefine abbreviations used within the code\nGood to have consistent comments and docstrings throughout (i.e. on all scripts, on not just some of them)\nCommon issue\nTom: I guess this one isn’t strictly necessary for reproducibility. The main issue was that the studies required a fair bit of manual work to get them to reproduce the results from teh mixed issues you listed above. This is sort of a “failsafe option” for reproducibility or perhaps more relevant for reuse/adaptation.\n\n\n\n\n\n\n\n\n\n\nEnsure clarity and consistency in the model results tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. There were two alternative results spreadsheets with some duplicate metrics but sometimes differing results between them, which made it a bit confusing to work out what to use.\nHuang et al. (2019), Lim et al. (2020), and Anagnostou et al. (2022): Fully met. Didn’t experience issues interpreting the contents of the output table/s.\nKim et al. (2021): Not met. It took me a little while to work out what surgery columns I needed, and to realise I needed to combine two of them. This required looking at what inputs genreated this, and referring to a input data dictionary.\nJohnson et al. (2021): Not met. I had mistakes and confusion figuring out which results tables I needed, which columns to use, and which scenarios to use from the tables.\nHernandez et al. (2015): Fully met. Straightforward with key information provided.\nWood et al. (2021): Fully met. I didn’t need to work with the output tables, but from looking at them now, they make sense.\nReflections:\n\nDon’t provide alternative results for the same metrics\nMake it clear what each colum/category in the results table means, if it might not be immediately clear.\nMake differences between seperate results tables clear.\nTom: In a RAP for simulation world we have a env + model + script that gets you to the exact results table you see in the paper and this isn’t a problem (although more time consuiming to setup).\n\n\n\n\n\n\n\n\n\n\nInclude run instructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n🟡\n✅\n✅\n❌\n❌\n\n\n\nShoaib and Ramamohan (2021): Not met. No instructions, although is just a single script that you run.\nHuang et al. (2019): Not met. Not provided in runnable form but, regardless, no instructions for running it as it is provided (as a web application - i.e. no info on how to get that running).\nLim et al. (2020): Not met. No instructions, although is just a single script that you run.\nKim et al. (2021): Partially met. README tells you which folder has the scripts you need, although nothing further. Although all you need to do is run them.\nAnagnostou et al. (2022): Fully met. Clear README with instructions on how to run the model was really helpful.\nJohnson et al. (2021): Fully met. README has mini description of model and clear instructions on how to install and run the model.\nHernandez et al. (2015): Not met.\nWood et al. (2021): Not met. No instructions, although it was fairly self explanatory (single script master.R to run, then processing scripts named after items in article e.g. fig7.R).\nReflections:\n\nEven if as simple as running a script, include instructions on how to do so\nIn simpler projects (e.g. single script), this can be less of a problem.\nCommon issue\nTom: Evidence for STARS essential component of minimum documentation.\n\n\n\n\n\n\n\n\n\n\nState run times and machine specifications\n\n\n\n\n\nSummary (whether stated both)\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n🟡\n❌\n🟡\n❌\n❌\n🟡\n🟡\n🟡\n\n\n\nState run time\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n❌\n❌\n❌\n✅\n🟡\n🟡\n\n\n\nShoaib and Ramamohan (2021): Fully met. Run time stated in paper (but not repository).\nHuang et al. (2019): Not met.\nLim et al. (2020): Not met.\nKim et al. (2021): Not met. A prior paper describing the model development mentions the run time, but not the current paper or repository, so this is easily missed.\nAnagnostou et al. (2022): Not met! Although it only took seconds, you could argue that stating this is still important if there were some error that made it look like the model were running continuously (e.g. stuck in a loop) - and as it helps someone identify that they are able to run it on their machine\nJohnson et al. (2021): Fully met. In the README, they state the the run time with 100 million agents is 16 hours, which was very handy to know, as I then just got stuck in running with fewer agents while troubleshooting.\nHernandez et al. (2015): Partially met. Some of the run times are mentioned in the paper, but not all, although this did help indicate that we would anticipate other s scenarios to similarly take hours to run.\nWood et al. (2021): Partially met. In the paper, they state that it takes less than five minutes for each scenario, but this feels like half the picture, given the total run time was 48 hours.\nReflections:\n\nFor long models with no statement, it can take a while to realise that it’s not an error in the code or anything, but actually just a long run time! And hard to know how long to expect, and whether it is without the capacities of your machine and so on.\nIdeally include statement of run time in repository as well as paper.\nIdeally include run time of all components of analysis (e.g. all scenarios).\nCommon issue.\nTom: This supports the inclusion of section 5.4 in the STRESS-DES guidelines\n\nResponse: But think it is also important that this is in the repository itself, and not just the paper.\n\n\nState machine specification\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n✅\n❌\n❌\n❌\n❌\n❌\n\n\n\nRegarding the one that did describe it:\n\nLim et al. (2020): Describe in article (“desktop computer (Intel Core i5 3.5 GHz, 8 GB RAM)”)\n\nComputationally expensive models\nRegarding whether models were computationally expensive…\n\nShoaib and Ramamohan (2021), Huang et al. (2019), and Lim et al. (2020), Anagnostou et al. (2022): No issues\nKim et al. (2021): Unable to run on my machine (serial took too long to run (would have to leave laptop on for many many hours which isn’t feasible), and parallel was too computationally expensive and crashed the machine (with the original number of people)). This is not mentioned in the repository or paper, but only referred to in a prior publication. Would’ve been handy if it included suggestions like reducing number of people and so on (which is what I had to do to feasibly run it).\nJohnson et al. (2021): It becomes more computationally expensive if try to run lots at once in simultaneous terminals. Didn’t try running one on local machine with full parameter due to long run time making it infeasible, but knowing my system specs, it should have been able to if did.\nHernandez et al. (2015): This had long run times but I don’t know if it was computationally expensive or not - I just know that I didn’t run into any issues (but I didn’t record memory usage, so its possible a lower-specced machine might).\nWood et al. (2021): Not applicable. As stated in their prior paper, the model is constrained by processing time, not computer memory.\n\nReflections:\n\nSome models are so computationally expensive that it may be simply impossible to run it a feasible length of time without a high powered machine.\nHandy to mention memory requirements so someone with lower spec machine can ensure they would be able to run it.\nIf a model is computationally expensive, it would be good to provide suggested alternatives that allow it to be run on lower spec machines\nNot a common problem - only relevant to computationally expensive models\nTom: Agree it makes sense to report this, and is captured in reporting guidelines like STRESS-DES.\n\n\n\n\n\n\n6.3.3 Functionality\n\n\n\n\n\n\nOptimise model run time\n\n\n\n\n\nThe run time of models had a big impact on how easy it was to reproduce results as longer run times meant it was tricky (or even impossible) to run in the first place, or tricky to re-run. The studies where I made adjustments were:\n\nShoaib and Ramamohan (2021): Add parallel processing and ran fewer replications\nHuang et al. (2019): No changes made.\nLim et al. (2020): Add parallel processing\nKim et al. (2021): Reduced number of people in simulation, and switched from serial to the provided parallel option.\nAnagnostou et al. (2022): Model was super quick which made it really easy to run and re-run each time\nJohnson et al. (2021): Experimented with using a fewer number of agents for troubleshooting (although ultimately had to run with full number to reproduce results), and ran the scripts in parallel by opening seperate terminals simultaneously. Note: Long run time also meant it took a longer time to do this reproduction - although we excluded computation time in our timings, it just meant e.g. when I made a mistake in coding of scenario analysis and had to re-run, I had to wait another day or two for that to finish before I could resume.\nHernandez et al. (2015): Add parallel processing, did not run one of the scenarios (it was very long, and hadn’t managed to reproduce other parts of same figure regardless), and experimented with reducing parameters for evolutionary algorithm (but, in the end, ran with full parameters, though lower were helpful while working through and troubleshooting).\nWood et al. (2021): No changes made, but unlike other reproduction, didn’t try to run at smaller amounts - just set it to run as-is over the weekend.\n\nIn one of the studies, there was a minor error which needed fixing, which we anticipated to likely be present due to long run times meaning the model wasn’t all run in sequence at the end.\nReflections:\n\nReduce model run time if possible as it makes it easier to work with, and facilitates doing full re-runs of all scenarios (which can be important with code changes, etc).\n\nRelatedly, it is good practice to re-run all scripts before finishing up, as then you can spot any errors like the one mentioned for Kim et al. (2021)\n\nCommon issue (to varying degrees - i.e. taking 20 minutes, up to taking several hours or even day/s).\nTom: Long run times are inevitable for some models, but this does suggest that some extra work to build confidence the model is working is expected is beneficial, like one or a small set of verification scenarios that are quick to run.\n\n\n\n\n\n\n\n\n\n\nSaves output to a file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n❌\n❌\n❌\n✅\n❌\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Fully met. Outputs to .xlsx files\nHuang et al. (2019), Lim et al. (2020), and Kim et al. (2021): Not met. Outputs to dataframe/s.\nAnagnostou et al. (2022): Outputs to OUT_STATS.csv. Note: Although not needed for the reproduction itself, when I tried to amend the name and location of the csv file output the model for use in tests, this was very tricky to do as it was hard coded into the scripts and I found difficult to amend due to how the model is run and set up.\nJohnson et al. (2021): Not met. Outputs to dataframe/s that are not saved as files (although can see within the kept .md file from knitting).\nHernandez et al. (2015): Fully met. Outputs to .txt files.\nWood et al. (2021): Fully met. Outputs to .csv files.\nReflections:\n\nCommon issue\nParticularly important if model run time is even slightly long (even just minutes long, but even more so as becomes many minutes / hours), so don’t always have to re-run it each time to get results\nSet up this in such a way that it is easy to change the name and location of the output file.\n\n\n\n\n\n\n\n\n\n\nAvoid excessive output files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\n✅\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\nFor Hernandez et al. (2015) the default behaviour of the script was to output lots of files from each round (so you could easily have 90, 100, 200+ files), which were then not used in analysis (as it just depended on an aggregate results file). Although these individual files might be useful during quality control, as a default behaviour of the script, it could easily make the repository quite busy/littered with files.\n\nTom reflected that this is more of a general housekeeping issue. He agrees and says its ok to do this, but perhaps they need a run mode that does not produce these verification files\n\n\n\n\n\n\n\n\n\n\nAddress large file sizes\n\n\n\n\n\nI have not evaluated like as a criteria, as a large file size is not inherently a bad thing, and might be difficult to avoid. However, when files are very large, this can make things trickier, such as with requiring compression and use of GitHub Large File Storage (LFS) for tracking, which has limits on the free tier.\nRegarding file sizes in each study:\n\nShoaib and Ramamohan (2021): Not relevant (results files &lt;35 kB)\nHuang et al. (2019): Provided code didn’t save results to file. When I saved to file, these were large, so I compressed to .csv.gz, which made them small enough that GitHub was still happy (26 MB).\nLim et al. (2020): Provided code didn’t save results to file. When I saved to file, these were small, so not relevant (results files &lt;60 kB)\nKim et al. (2021): Provided code didn’t save results to file. When I saved to file, these were small, so not relevant (results files &lt;1 kB)\nAnagnostou et al. (2022): Not relevant (results file 34 kB)\nJohnson et al. (2021): Provided code didn’t save results to file. When I saved to .csv files, these were small, so not relevant (results files &lt;3.6kB)\nHernandez et al. (2015): Not relevant (aggregate results files &lt;10 kB).\nWood et al. (2021): Aggregate results files are small (327 kB), but raw results files are very large (2.38 GB), and even when compressed to .csv.gz (128 MB) require using of GitHub LFS\n\nReflections:\n\nFor most studies, this was not relevant, with outputs relatively small.\nI only really found this to be an issue when files exceeded GitHub threshold. GitHub give warning over 50 MB, blocks files over 100 MB, requiring you to use GitHub LFS. Recommends repositories ideally &lt;1 GB and for sure &lt; 5GB. GitHub LFS has limits on storage and bandwith use (1GB of each).\nReducing file size isn’t the only solution. In cases where you have large files, a good option can be storing it elsewhere and then pulling from there into your workflow - for example, storing in zenodo and fetching using pooch.\n\nTom: Approach with zenodo seems sensible. There’s a simple workflow for submitting to a journal with this approach as well. Plus scenarios/scripts can link to raw datafiles on zenodo for comparion.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#other-recommendations",
    "href": "pages/reflections.html#other-recommendations",
    "title": "6  Reflections from reproductions",
    "section": "6.4 Other recommendations",
    "text": "6.4 Other recommendations\n\n\n\n\n\n\nBe aware of potential system dependencies\n\n\n\n\n\nThere can also be system dependencies, which will vary between systems, and may not be obvious if researchers already have these installed. We identified these when setting up the docker environments (which act like “fresh installs”):\n\nShoaib and Ramamohan (2021), Lim et al. (2020), Anagnostou et al. (2022) - no dependencies\nHuang et al. (2019), Kim et al. (2021), Johnson et al. (2021) and Wood et al. (2021) - libcurl4-openssl-dev, libssl-dev, libxml2-dev, libglpk-dev, libicu-dev - as well as tk for Johnson et al. 2021\nHernandez et al. (2015) - wget, build-essential, libssl-dev, libffi-dev, libbz2-dev, libreadline-dev, libsqlite3-dev, zlib1g-dev, libncurses5-dev, libgdbm-de, libnss3-dev, tk-dev, liblzma-dev, libsqlite3-dev, lzma, ca-certificates, curl, git\n\nAlthough it would be unreasonable for authors to be aware of and list all system dependencies, given they may not be aware of them, this does show the benefit of creating something like docker in identifying them and making note of them within the docker files.\nThis issue was specific to (a) R studies, and (b) the study with an unsupported version of Python that required building it from source in the docker file.\n\n\n\n\n\n\n\n\n\nModel is designed to be run programmatically (i.e. can run model with different parameters without needing to change the model code)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Not met. The model is set up as classes and run using a function. However, it is not designed to allow any variation in inputs. Everything uses default inputs, and it designed in such a way that - if you wish to vary model parameters - you need to directly change these in the script itself.\nHuang et al. (2019): Fully met. Model was set up as a function, with many of the required parameters already set as “changeable” inputs to that function.\nLim et al. (2020): Fully met. The model is created from a series of functions and run with a for loop that iterates through different parameters. As such, the model is able to be run programmatically (within that for loop, which varied e.g. staff per shift and so on and re-ran the model).\nKim et al. (2021): Fully met. Each scenario is an R script which states different parameters and then calls functions to run model.\nAnagnostou et al. (2022): Fully met. Change inputs in input .csv files.\nJohnson et al. (2021): Fully met. Creates a list of input which are then used by a run() function.\nHernandez et al. (2015): Fully met. Model created from classes, which accept some inputs and can run the model.\nWood et al. (2021): Fully met. Changes inputs to run all scenarios from a single .R file.\nReflections:\n\nDesign model so that you can re-run it with different parameters without needing to make changes to the model code itself.\n\nThis allows you to run multiple versions of the model with the same script.\nIt also reduces the likelihood of missing errors (e.g. if miss changing an input parameter somewhere, or input the wrong parameters and don’t realise).\n\nThis was an uncommon issue.\nNote, this just refers to the basic set-up, with items below like hard coding parameters also being very important in this context.\n\n\n\n\n\n\n\n\n\n\nUse relative file paths\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n✅\nN/A\nN/A\n✅\n✅\nN/A\n✅\n✅\n\n\n\nShoaib and Ramamohan (2021): Fully met. Just provides file path, so file is saved into current/run directory.\nHuang et al. (2019): Not applicable. All inputs defined within script. Outputs were not saved to file/s.\nLim et al. (2020): Not applicable. All inputs defined within script. Outputs were not saved to file/s.\nKim et al. (2021): Fully met. Uses relative file paths for sourcing model and input parameters (gets current directory, then navigates from there).\nAnagnostou et al. (2022): Fully met. Uses relative imports of local code files.\nJohnson et al. (2021): Not applicable. All inputs defined within script. Outputs are not specifically saved to a file (just that the .md and image files were automatically saved when the .Rmd file was knit). EpicR is package import.\nHernandez et al. (2015): Fully met. Creates folder in current working directory based on date/time to store results.\nWood et al. (2021): Fully met. Although I then changed things a bit as reorganised repository and prefer not to work with setwd(), these were set up in such a way that it would be really easy to correct file path, just by setting working directory at start of script.\nReflections:\n\nThis was not an issue for any studies - but included to note this was a “facilitator”, as would have needed to amend if they weren’t (and Tom noted that this is a common problem that he runs into elsewhere).\n\n\n\n\n\n\n\n\n\n\nProvide all the required parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n❌\n✅\n✅\n✅\n❌\n✅\n\n\n\nShoaib and Ramamohan (2021): Some parameters that could not be calculated were not provided - ie. what consultation boundaries to use when mean length of doctor consultation was 2.5 minutes\nHuang et al. (2019): Not met. In this case, patient arrivals and resource numbers were listed in the paper, and there were several discprepancies between this and the provided code. However, for many of the model parameters like length of appointment, these were not mentioned in the paper, and so it was not possible to confirm whether or not those were correct. Hence, marked as not met, as the presence of discrepenancies for several other parameters puts these into doubt.\nLim et al. (2020): Not met. For Figure 5, had to guess the value for staff_per_shift.\nKim et al. (2021): Fully met.\nAnagnostou et al. (2022): Fully met.\nJohnson et al. (2021): Fully met. Could determine appropriate parameters for sensitivity analysis from figures in article.\nHernandez et al. (2015): Not met. The results have a large impact by the bounding set, but this was not mentioned in the paper or repository, and required me looking at the numbers in results and GitHub commit history to estimate the appropriate bounds to use.\nWood et al. (2021): Fully met.\nReflections:\n\nProvide all required parameters\nTom: Evidence to support STRESS-DES 3.3\n\n\n\n\n\n\n\n\n\n\nIf not provided in the script, then clearly present all parameters in the paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n❌\n🟡\nN/A\nN/A\n✅\n🟡\nN/A\n\n\n\nShoaib and Ramamohan (2021): Not met. Although there was a scenario table, this did not include all the parameters I would need to change. It was more challenging to identify parameters that were only described in the body of the article. There were also some discrepancies in parameters between the main text of the article, and the tables and figures. Some scenarios were quite ambiguous/unclear from their description in the text, and I initially misunderstood the required parameters for the scenarios.\nHuang et al. (2019): Not met. As described above, paper didn’t adequately describe all parameters.\nLim et al. (2020): Partially met. Nearly all parameters are in the paper table, and others are described in the article. However, didn’t provide information for the staff_per_shift for Figure 5.\nKim et al. (2021) and Anagnostou et al. (2022): Not applicable. All provided.\nJohnson et al. (2021): Fully met. All parameters clearly in the two figures presenting the sensitivity analysis, and didn’t have to look elsewhere beyond that.\nHernandez et al. (2015): Most parameters are relatively easily identified from the text or figure legends (though would be easier if provided in a table or similar). Parameter for bounding was not provided in paper.\nWood et al. (2021): Not applicable. All provided.\nReflections:\n\nProvide parameters in a table (including for each scenario), as it can be difficult/ambiguous to interpret them from the text, and hard to spot them too.\n\nTom: The ambiguity of the natural language for scenarios was an important finding.\n\nBe sure to mention every parameter that gets changed (e.g. for Lim et al. (2020), as there wasn’t a default staff_per_shift across all scenarios, but not stated for the scenario, had to guess it).\nTom: Evidence to support STRESS-DES 3.3\n\n\n\n\n\n\n\n\n\n\nIf will need to process parameters, provide required calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoaib and Ramamohan (2021)\nHuang et al. (2019)\nLim et al. (2020)\nKim et al. (2021)\nAnagnostou et al. (2022)\nJohnson et al. (2021)\nHernandez et al. (2015)\nWood et al. (2021)\n\n\n\n\n❌\n✅\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n\n\n\nShoaib and Ramamohan (2021): Not met. It was unclear how to estimate inter-arrival time.\nHuang et al. (2019): Fully met. The calculations for inter-arrival times were provided in the code, and the inputs to the code were the number of arrivals, as reported in the paper, and so making it easy to compare those parameters and check if numbers were correct or not.\nLim et al. (2020): Not applicable. The parameter not provided is not one that you would calculate.\nKim et al. (2021) and Anagnostou et al. (2022): Not applicable. All provided.\nJohnson et al. (2021): Not applicable. No processing of parameters required.\nHernandez et al. (2015): Not applicable.\nWood et al. (2021): Not applicable. All provided.\nReflections:\n\nIf you are going to be mentioning the “pre-processed” values at all, then its important to include the calculation (ideally in the code, as that is the clearest demonstration of exactly what you did)\nTom: This is a very good point for RAP.\n\n\n\n\nGrid lines. Include tick marks/grid lines on figures, so it is easier to read across and judge whether a result is above or below a certain Y value.\nData dictionaries. Anagnostou et al. (2022): Included data dictionary for input parameters. Although I didn’t need this, this would have been great if I needed to change the input parameters at all.\nUnsupported versions. Hernandez et al. (2015): Due to the age of the work:\n\nSome packages were no longer available on Conda and had to be installed from PyPI\nThe version of python was no longer supported which meant:\n\nNot supported by Jupyter Lab and Jupyter Notebook (so no .ipynb files, or Jupyter Lab on Docker)\nNot supported by VSCode (so had to use “tricks” to run it, involving using a pre-release version of Python on VSCode)\nHad to create Docker image from scratch (i.e. couldn’t start from e.g. miniconda3)\n\n\nHowever, this is a slightly unavoidable problem unless you continue to maintain your code (which is ideal but not always feasible in a research environment). Realistically, if reusing this code for a new purpose, you would upgrade it to supported versions.\nTom: This is interesting - and you wonder if it would still be possible (given the “tricks” I followed) in another 10 years time.\nOriginal results files. Hernandez et al. (2015): Included some original results files, which was invaluable in identifying some of the parameters in the code that needed to be fixed.\nClasses. Hernandez et al. (2015): Structured code into classes, which was nice to work with/amend.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/reflections.html#references",
    "href": "pages/reflections.html#references",
    "title": "6  Reflections from reproductions",
    "section": "6.5 References",
    "text": "6.5 References\n\n\n\n\nAnagnostou, Anastasia. 2022. “CHARM: dynamiC Hospital wARd Management.” Brunel University London. https://doi.org/10.17633/rd.brunel.18517892.v1.\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reflections from reproductions</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html",
    "href": "pages/repo_evaluation.html",
    "title": "7  Evaluation of the repository",
    "section": "",
    "text": "7.1 Summary\nThe code and related research artefacts in the original code repositories were evaluated against:\nBetween each journal badge, there was often alot of overlap in criteria. Hence, a list of unique criteria was produced. The repositories are evaluated against this criteria, and then depending on which criteria they met, against the badges themselves.\nCaveat: Please note that these criteria are based on available information about each badge online. Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals.\nConsider: What criteria are people struggling to meet from the guidelines?\nUnique badge criteria:\nBadges:\nEssential components of STARS framework:\nOptional components of STARS framework:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#summary",
    "href": "pages/repo_evaluation.html#summary",
    "title": "7  Evaluation of the repository",
    "section": "",
    "text": "Reflections\n\n\n\n\n\nNo clear relationship. I think it is more meaningful to actually look at what criteria were and were not met.\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nNot certain how meaningful these numbers are, as we have imbalanced numbers of different types of badge, and meeting certain popular criteria will weight what is met v.s. not.\nFeel that looking at the criteria met is a bit more meaningful? And then specific examples of how that translates into badges - e.g.\n\nNone meeting ACM “Artifacts Evaluated - Functional” as requires xyz and these are commonly not met.\nFor several, they meet three badges, but those three badges have one criteria: reproducing results.\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nSimilar to journal criteria (unsurprisingly, as looking at similar things) - most studies meet very few and have wide range of reproduction success, from 12.5% to 100%. Three met more, and these were 80% to 100% reproduced.\nI think, if we were to draw anything from this, it would be to reflect on exactly what criteria were and were not met, and why/how that impacted reproduction, in any way (either success or time).\nNote: Just considers those fully met, in plot\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nThis highlights how Huang meets the most criteria, but is only partially reproduced - but I think it is most interesting to consider why this is.\nNote: Just considers those fully met, in plot\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\nTable with proportion of applicable STARS criteria that were fully met\n\n\n\n\n\nThis is part of a table used in the journal article:\n\n\n\n\n\n\n\n\n\nreproduction\nstars_essential\nstars_optional\n\n\n\n\nKim et al. 2021 (10/10)\n100.0% (10/10)\n50%\n0%\n\n\nLim et al. 2020 (9/9)\n100.0% (9/9)\n25%\n0%\n\n\nWood et al. 2021 (5/5)\n100.0% (5/5)\n25%\n0%\n\n\nAnagnostou et al. 2022 (1/1)\n100.0% (1/1)\n88%\n20%\n\n\nShoaib and Ramamohan&lt;br&gt;2021 (16/17)\n94.1% (16/17)\n25%\n0%\n\n\nJohnson et al. 2021 (4/5)\n80.0% (4/5)\n50%\n0%\n\n\nHuang et al. 2019 (3/8)\n37.5% (3/8)\n25%\n40%\n\n\nHernandez et al. 2015 (1/8)\n12.5% (1/8)\n25%\n0%",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#journal-badges",
    "href": "pages/repo_evaluation.html#journal-badges",
    "title": "7  Evaluation of the repository",
    "section": "7.2 Journal badges",
    "text": "7.2 Journal badges\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\nIn this section and below, the criteria for each study are marked as either being fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A).\nUnique criteria:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nCriteria related to how artefacts are shared\n\n\n\n\n\n\n\n\n\n\n\nArtefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nOpen licence\n❌\n✅\n❌\n✅\n✅\n✅\n❌\n❌\n\n\n\nCriteria related to what artefacts are shared\n\n\n\n\n\n\n\n\n\n\n\nComplete (all relevant artefacts available)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\nArtefacts relevant to paper\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nCriteria related to the structure and documentation of the artefacts\n\n\n\n\n\n\n\n\n\n\n\nDocuments (a) how code is used (b) how it relates to article (c) software, systems, packages and versions\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nDocuments (a) inventory of artefacts (b) sufficient description for artefacts to be exercised\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n❌\n\n\n\nArtefacts are carefully documented and well-structured to the extent that reuse and repurposing is facilitated, adhering to norms and standards\n❌\n❌\n❌\n✅\n✅\n❌\n❌\n❌\n\n\n\nREADME file with step-by-step instructions to run analysis\n❌\n❌\n❌\n❌\n✅\n✅\n❌\n❌\n\n\n\nDependencies (e.g. package versions) stated\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nClear how output of analysis corresponds to article\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\nCriteria related to running and reproducing results\n\n\n\n\n\n\n\n\n\n\n\nScripts can be successfully executed\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nReproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nArtefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI): Fulfillment doesn’t impact reproduction as I was able to get everything needed from the remote code repository (GitHub or GitLab). However, if these had been deleted from GitHub, it would have become invaluable.\nOpen licence: This had a big impact on our ability to complete reproductions, as we had to ask authors to add an open licence to their work, to enable us to use it. Gladly, all authors we contacted kindly add these on request. However, it’s worth noting that this was a relatively common issue, and one of the most important, since it completely prevents reuse if excluded.\nComplete (all relevant artefacts available): This had a really big impact on the reproduction. The main reason for longer times in reproduction was (a) code for scenarios not provided, and (b) code to process results into figures and tables not provided.\nArtefacts relevant to paper: All met (if not met, this would be a massive hindrance).\nDocuments (a) how code is used (b) how it relates to article (c) software, systems, packages and versions / Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised / Artefacts are carefully documented and well-structured to the extent that reuse and repurposing is facilitated, adhering to norms and standards / README file with step-by-step instructions to run analysis:\n\nAll really handy. In cases where this was not met, this was often related to having quite a busy/cluttered repository which was confusing to navigate, with minimal documentation.\nIn Anagnostou et al. (2022), they include a file CHARM_INFO.md alongside their README which walks through the input parameters for the model. I didn’t need to change any of these for the reproduction, but would imagine this is to be very helpful if someone were to reuse the model.\nOnly three studies had any documentation - READMEs for Kim et al. (2021), Anagnostou et al. (2022) and Johnson et al. (2021). However, focusing on the READMEs, in each of these cases it was great to have these, guiding on how to run the scripts, or on what each folder/file in the repository is - although one didn’t have step-by-step instructions, as requested.\nWhilst Anagnostou et al. (2022) did meet criteria, it should be noted that this was a very simple example, just requiring to run one script which quickly reproduces everything! I had been a bit uncertain on it, since the README doesn’t explicitly say how to make the figure, but it does provide instructions that lead you to regenerate the exact model results from the paper, and so I feel that it does provide instructions to reproduce results sufficiently (although would be more complete to include instructions for figure too - so if it weren’t a yes/no decision for badges, I would’ve said this was partially met). Ideally, studies would clearly outline how to reproduce results in full.\n\nDependencies (e.g. package versions) stated: Important and impacts analysis, takes a while to work out otherwise.\nClear how output of analysis corresponds to article: This is handy - clear link between analysis and items in paper.\nScripts can be successfully executed: This is true, though I did allow troubleshooting. Hence, the importance of e.g. environments and scripts being provided in a runnable format (both covered on the reflections page), since these are the hurdles to successfully executing scripts.\nReproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting): On the reproduction page, I reflected (where possible) on what I thought the primary reasons were, for cases where I didn’t manage to reproduce results despite troubleshooting. It is worth noting however that there were two studies that were quite quick to run, which I reflect about on the reproduction page.\n\n\n\nBadges:\nThe badges are grouped into three categories:\n\n“Open objects” badges: These badges relate to research artefacts being made openly available.\n“Object review” badges: These badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n“Reproduced” badges: These badges relate to an independent party regenerating the reuslts of the article using the author objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\n“Open objects” badges\n\n\n\n\n\n\n\n\n\n\n\nACM “Artifacts Available”• Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nNISO “Open Research Objects (ORO)”• Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)• Open licence\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nNISO “Open Research Objects - All (ORO-A)”• Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)• Open licence• Complete (all relevant artefacts available)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nCOS “Open Code”• Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)• Open licence• Documents (a) how code is used (b) how it relates to article (c) software, systems, packages and versions\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nIEEE “Code Available”• Complete (all relevant artefacts available)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n“Object review” badges\n\n\n\n\n\n\n\n\n\n\n\nACM “Artifacts Evaluated - Functional”• Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised• Artefacts relevant to paper• Complete (all relevant artefacts available)• Scripts can be successfully executed\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nACM “Artifacts Evaluated - Reusable”• Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised• Artefacts relevant to paper• Complete (all relevant artefacts available)• Scripts can be successfully executed• Artefacts are carefully documented and well-structured to the extent that reuse and repurposing is facilitated, adhering to norms and standards\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nIEEE “Code Reviewed”• Complete (all relevant artefacts available)• Scripts can be successfully executed\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n“Reproduced” badges\n\n\n\n\n\n\n\n\n\n\n\nACM “Results Reproduced”• Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\nNISO “Results Reproduced (ROR-R)”• Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\nIEEE “Code Reproducible”• Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\nPsychological Science “Computational Reproducibility”• Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)• README file with step-by-step instructions to run analysis• Dependencies (e.g. package versions) stated• Clear how output of analysis corresponds to article\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\nOnly one study had permanent archive (with persistent identifier), hence one being awarded NISO “Open Research Objects (ORO)”, ACM “Artifacts Available” and COS “Open Code”. However, that study did not receive NISO “Open Research Objects - All (ORO-A)” as artefacts were not complete.\nA complete set of materials was required by IEEE “Code Available” and IEEE “Code Reviewed” - but this was only met by one study, as studies commonly did not include code for scenarios or creation of figures and tables. It was also required by ACM “Artifacts Evaluated - Functional and Reusable” badges, but since that one study didn’t meet their documentation requirements, none were awarded those badges.\nThree badges had one criteria: reproduction of results - but with assumptions - and only one study met this (with 100% reproduction PLUS meeting the assumptions).\n\n\n\n\n\n\n\n\n\nBadges summary tables for the article\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nStudies that met criteria\n\n\nBadge\n\n\n\n\n\n\nACM “Artifacts&lt;br&gt;Available”\n• Artefacts are archived in a repository that ...\n1/8 (12.5%)\n\n\nNISO “Open Research&lt;br&gt;Objects (ORO)”\n• Artefacts are archived in a repository that ...\n1/8 (12.5%)\n\n\nNISO “Open Research&lt;br&gt;Objects - All (ORO-A)”\n• Artefacts are archived in a repository that ...\n0/8 (0.0%)\n\n\nCOS “Open Code”\n• Artefacts are archived in a repository that ...\n1/8 (12.5%)\n\n\nIEEE “Code Available”\n• Complete (all relevant artefacts available)\n1/8 (12.5%)\n\n\nACM “Artifacts&lt;br&gt;Evaluated - Functional”\n• Documents (a) inventory of artefacts (b) suf...\n0/8 (0.0%)\n\n\nACM “Artifacts&lt;br&gt;Evaluated - Reusable”\n• Documents (a) inventory of artefacts (b) suf...\n0/8 (0.0%)\n\n\nIEEE “Code Reviewed”\n• Complete (all relevant artefacts available)&lt;...\n1/8 (12.5%)\n\n\nACM “Results Reproduced”\n• Reproduced results (assuming (a) acceptably ...\n1/8 (12.5%)\n\n\nNISO “Results&lt;br&gt;Reproduced (ROR-R)”\n• Reproduced results (assuming (a) acceptably ...\n1/8 (12.5%)\n\n\nIEEE “Code Reproducible”\n• Reproduced results (assuming (a) acceptably ...\n1/8 (12.5%)\n\n\nPsychological Science&lt;br&gt;“Computational Reproducibility”\n• Reproduced results (assuming (a) acceptably ...\n0/8 (0.0%)",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#stars-framework",
    "href": "pages/repo_evaluation.html#stars-framework",
    "title": "7  Evaluation of the repository",
    "section": "7.3 STARS framework",
    "text": "7.3 STARS framework\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nEssential components\n\n\n\n\n\n\n\n\n\n\n\nOpen licenceFree and open-source software (FOSS) licence (e.g. MIT, GNU Public Licence (GPL))\n❌\n✅\n❌\n✅\n✅\n✅\n❌\n❌\n\n\n\nDependency managementSpecify software libraries, version numbers and sources (e.g. dependency management tools like virtualenv, conda, poetry)\n❌\n❌\n❌\n🟡\n✅\n🟡\n❌\n❌\n\n\n\nFOSS modelCoded in FOSS language (e.g. R, Julia, Python)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nMinimum documentationMinimal instructions (e.g. in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments\n❌\n❌\n❌\n✅\n✅\n🟡\n❌\n❌\n\n\n\nORCIDORCID for each study author\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nCitation informationInstructions on how to cite the research artefact (e.g. CITATION.cff file)\n❌\n❌\n❌\n❌\n✅\n✅\n❌\n❌\n\n\n\nRemote code repositoryCode available in a remote code repository (e.g. GitHub, GitLab, BitBucket)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nOpen science archiveCode stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g. Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net))\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nOptional components\n\n\n\n\n\n\n\n\n\n\n\nEnhanced documentationOpen and high quality documentation on how the model is implemented and works (e.g. via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:• Plain english summary of project and model• Clarifying licence• Citation instructions• Contribution instructions• Model installation instructions• Structured code walk through of model• Documentation of modelling cycle using TRACE• Annotated simulation reporting guidelines• Clear description of model validation including its intended purpose\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nDocumentation hostingHost documentation (e.g. with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nOnline coding environmentProvide an online environment where users can run and change code (e.g. BinderHub, Google Colaboratory, Deepnote)\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n❌\n\n\n\nModel interfaceProvide web application interface to the model so it is accessible to less technical simulation users\n❌\n✅\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\nWeb app hostingHost web app online (e.g. Streamlit Community Cloud, ShinyApps hosting)\n❌\n✅\n❌\n❌\n🟡\n❌\n❌\n❌\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nThese topics were covered in the badge criteria reflections: open licence, minimum documentation, and open science archive.\nDependency management: This was pretty uncommon, and often took some troubleshooting at the start, to figure out which packages were needed, and certain versions.\nFOSS model: All met as requirement of our reproduction.\nORCID and citation information: Doesn’t impact reproduction in this case - but:\n\nWe do go to these from having found an article. I was choosing repositories that I had found from papers, so I already at least knew who the paper authors were.\nIn all cases, I emailed the authors, which requires finding contact information (generally via paper, sometimes from googling them to find new emails).\nAny attempted citation of the repository itself would’ve necessarily been correct, depending on whether the author list would be the same as in the paper, if you relied on the paper without citation information.\n\nRemote code repository: All met, most common way to share code.\nEnhanced documentation: Only three studies had any documentation, and neither met these extensive requirements. I anticipate - if any had met this - it would’ve made the reproduction very quick and easy!\nDocumentation hosting: Not applicable, given only basic documentation.\nOnline coding environment: None provided. I always intended to run on my own machine, so this might not have had much bearing in my case if provided, but would moreso for people who perhaps didn’t have Python or R installed, and hopefully would have bypassed environment troubleshooting issues.\nModel interface: Two studies had applications, although in both cases, these weren’t “outcomes” in scope of reproduction, nor did they produce them.\nWeb app hosting: This was quite important. Both apps had been hosted, but one was hosted with a site that is no longer operational. In both cases, the app wasn’t in “scope” although I did still view it and look into it for one as it was hosted and so could very easily - but for the other, I didn’t view it, as I didn’t go through the steps of running it locally, since it wasn’t the focus.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#timings",
    "href": "pages/repo_evaluation.html#timings",
    "title": "7  Evaluation of the repository",
    "section": "7.4 Timings",
    "text": "7.4 Timings\n\nShoaib and Ramamohan (2021) - 30m\nHuang et al. (2019) - 17m\nLim et al. (2020) - 18m\nKim et al. (2021) - 18m\nAnagnostou et al. (2022) - 19m\nJohnson et al. (2021) - 20m\nHernandez et al. (2015) - 13m\nWood et al. (2021) - 14m\n\nRevisiting and redoing evaluation for badges was only 2-3 minutes per study, and have just stuck with the original evaluation timings above.\n\n\n\n\n\n\nReflections\n\n\n\n\n\nNo particular comments, don’t think we learn much from the timings here.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#badge-sources",
    "href": "pages/repo_evaluation.html#badge-sources",
    "title": "7  Evaluation of the repository",
    "section": "7.5 Badge sources",
    "text": "7.5 Badge sources\nNational Information Standards Organisation (NISO) (NISO Reproducibility Badging and Definitions Working Group (2021))\n\n“Open Research Objects (ORO)”\n“Open Research Objects - All (ORO-A)”\n“Results Reproduced (ROR-R)”\n\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\n\n“Artifacts Available”\n“Artifacts Evaluated - Functional”\n“Artifacts Evaluated - Resuable”\n“Results Reproduced”\n\nCenter for Open Science (COS) (Blohowiak et al. (2023))\n\n“Open Code”\n\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (2024))\n\n“Code Available”\n“Code Reviewed”\n“Code Reproducible”\n\nPsychological Science (Hardwicke and Vazire (2024) and Association for Psychological Science (APS) (2024))\n\n“Computational Reproducibility”",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/repo_evaluation.html#references",
    "href": "pages/repo_evaluation.html#references",
    "title": "7  Evaluation of the repository",
    "section": "7.6 References",
    "text": "7.6 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nAssociation for Computing Machinery (ACM). 2020. “Artifact Review and Badging Version 1.1.” ACM. https://www.acm.org/publications/policies/artifact-review-and-badging-current.\n\n\nAssociation for Psychological Science (APS). 2024. “Psychological Science Submission Guidelines.” APS. https://www.psychologicalscience.org/publications/psychological_science/ps-submissions.\n\n\nBlohowiak, Ben B., Johanna Cohoon, Lee de-Wit, Eric Eich, Frank J. Farach, Fred Hasselman, Alex O. Holcombe, Macartan Humphreys, Melissa Lewis, and Brian A. Nosek. 2023. “Badges to Acknowledge Open Practices.” https://osf.io/tvyxz/.\n\n\nHardwicke, Tom E., and Simine Vazire. 2024. “Transparency Is Now the Default at Psychological Science.” Psychological Science 35 (7): 708–11. https://doi.org/10.1177/09567976231221573.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nInstitute of Electrical and Electronics Engineers (IEEE). 2024. “About Content in IEEE Xplore.” IEEE Explore. https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-content.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882.\n\n\nNISO Reproducibility Badging and Definitions Working Group. 2021. “Reproducibility Badging and Definitions.” https://doi.org/10.3789/niso-rp-31-2021.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluation of the repository</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html",
    "href": "pages/paper_evaluation.html",
    "title": "8  Evaluation of the article",
    "section": "",
    "text": "8.1 Summary\nThis page shares the results from the evaluation of the journal articles against criteria from two discrete-event simulation study reporting guidelines:\nConsider: What criteria are people struggling to meet from the guidelines?\nSTRESS-DES:\nDES checklist derived from ISPOR-SDM:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#summary",
    "href": "pages/paper_evaluation.html#summary",
    "title": "8  Evaluation of the article",
    "section": "",
    "text": "Reflections\n\n\n\n\n\nOf the applicable criteria, all studies fully met at least 60% (and many others still partially met) (score_fully).\nLooking at the quality score suggested (score), we see no relationship between reporting quality and reproduction sucess. This remains the case when using other scoring systems used in other papers (score_schwander and score_zhang).\nWith similar conclusions from score and score_fully, I think it’s sensible to present score_fully as that is the simplest interpretation.\n\n\n\n\n\n\n\n\n\nstudy\nfully\npartially\nnot\nna\nreproduce\nscore\nscore_fully\nscore_schwander\nscore_zhang\n\n\n\n\n0\nShoaib and&lt;br&gt;Ramamohan 2021 (16/17)\n17\n6\n1\n0\n94.1\n83.333333\n70.833333\n83.333333\n70.833333\n\n\n1\nHuang et al. 2019 (3/8)\n14\n5\n4\n1\n37.5\n71.739130\n60.869565\n68.750000\n58.333333\n\n\n2\nLim et al. 2020 (9/9)\n15\n3\n3\n3\n100.0\n78.571429\n71.428571\n68.750000\n62.500000\n\n\n3\nKim et al. 2021 (10/10)\n15\n5\n2\n2\n100.0\n79.545455\n68.181818\n72.916667\n62.500000\n\n\n4\nAnagnostou et al. 2022 (1/1)\n14\n2\n5\n3\n100.0\n71.428571\n66.666667\n62.500000\n58.333333\n\n\n5\nJohnson et al. 2021 (4/5)\n16\n1\n2\n5\n80.0\n86.842105\n84.210526\n68.750000\n66.666667\n\n\n6\nHernandez et al. 2015 (1/8)\n18\n2\n3\n1\n12.5\n82.608696\n78.260870\n79.166667\n75.000000\n\n\n7\nWood et al. 2021 (5/5)\n22\n2\n0\n0\n100.0\n95.833333\n91.666667\n95.833333\n91.666667\n\n\n\n\n\n\n\nMean values...\n\n\nstudy                    NaN\nfully              16.375000\npartially           3.250000\nnot                 2.500000\nna                  1.875000\nreproduce          78.012500\nscore              81.237757\nscore_fully        74.014752\nscore_schwander    75.000000\nscore_zhang        68.229167\ndtype: float64\n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nThe proportion of applicable criteria met was lower for this checklist (score_fully).\nSame conclusions as for STRESS.\n\n\n\n\n\n\n\n\n\nstudy\nfully\npartially\nnot\nna\nreproduce\nscore\nscore_fully\nscore_schwander\nscore_zhang\n\n\n\n\n0\nShoaib and&lt;br&gt;Ramamohan 2021 (16/17)\n11\n2\n2\n3\n94.1\n80.000000\n73.333333\n66.666667\n61.111111\n\n\n1\nHuang et al. 2019 (3/8)\n7\n2\n7\n2\n37.5\n50.000000\n43.750000\n44.444444\n38.888889\n\n\n2\nLim et al. 2020 (9/9)\n12\n0\n4\n2\n100.0\n75.000000\n75.000000\n66.666667\n66.666667\n\n\n3\nKim et al. 2021 (10/10)\n12\n0\n5\n1\n100.0\n70.588235\n70.588235\n66.666667\n66.666667\n\n\n4\nAnagnostou et al. 2022 (1/1)\n8\n3\n4\n3\n100.0\n63.333333\n53.333333\n52.777778\n44.444444\n\n\n5\nJohnson et al. 2021 (4/5)\n15\n0\n2\n1\n80.0\n88.235294\n88.235294\n83.333333\n83.333333\n\n\n6\nHernandez et al. 2015 (1/8)\n10\n0\n7\n1\n12.5\n58.823529\n58.823529\n55.555556\n55.555556\n\n\n7\nWood et al. 2021 (5/5)\n13\n0\n4\n1\n100.0\n76.470588\n76.470588\n72.222222\n72.222222\n\n\n\n\n\n\n\nMean values...\n\n\nstudy                    NaN\nfully              11.000000\npartially           0.875000\nnot                 4.375000\nna                  1.750000\nreproduce          78.012500\nscore              70.306373\nscore_fully        67.441789\nscore_schwander    63.541667\nscore_zhang        61.111111\ndtype: float64\n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\nTable with proportion of applicable reporting criteria that were fully met\n\n\n\n\n\nThis is part of a table used in the journal article:\n\n\n\n\n\n\n\n\n\nreproduction\nstress\ngeneric\n\n\n\n\nKim et al. 2021 (10/10)\n100.0% (10/10)\n68%\n71%\n\n\nLim et al. 2020 (9/9)\n100.0% (9/9)\n71%\n75%\n\n\nWood et al. 2021 (5/5)\n100.0% (5/5)\n92%\n76%\n\n\nAnagnostou et al. 2022 (1/1)\n100.0% (1/1)\n67%\n53%\n\n\nShoaib and&lt;br&gt;Ramamohan 2021 (16/17)\n94.1% (16/17)\n71%\n73%\n\n\nJohnson et al. 2021 (4/5)\n80.0% (4/5)\n84%\n88%\n\n\nHuang et al. 2019 (3/8)\n37.5% (3/8)\n61%\n44%\n\n\nHernandez et al. 2015 (1/8)\n12.5% (1/8)\n78%\n59%",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#stress-des",
    "href": "pages/paper_evaluation.html#stress-des",
    "title": "8  Evaluation of the article",
    "section": "8.2 STRESS-DES",
    "text": "8.2 STRESS-DES\n\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\nIn this section and below, the criteria for each study are marked as either being fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nObjectives\n\n\n\n\n\n\n\n\n\n\n\n1.1 Purpose of the modelExplain the background and objectives for the model\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n1.2 Model outputsDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n🟡\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n1.3 Experimentation aimsIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis – Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments – Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation – (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\nLogic\n\n\n\n\n\n\n\n\n\n\n\n2.1 Base model overview diagramDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.2 Base model logicGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.3 Scenario logicGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\n2.4 AlgorithmsProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e. scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n✅\n🟡\n✅\n🟡\n✅\n✅\n✅\n✅\n\n\n\n2.5.1 Components - entitiesGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.5.2 Components - activitiesDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2.5.3 Components - resourcesList all the resources included within the model and which activities make use of them.\n✅\n✅\nN/A\nN/A\n✅\nN/A\n✅\n✅\n\n\n\n2.5.4 Components - queuesGive details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n✅\n✅\nN/A\nN/A\n✅\nN/A\n✅\n✅\n\n\n\n2.5.5 Components - entry/exit pointsGive details of the model boundaries i.e. all arrival and exit points of entities. Detail the arrival mechanism (e.g. ‘thinning’ to mimic a non-homogenous Poisson process or balking)\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\nData\n\n\n\n\n\n\n\n\n\n\n\n3.1 Data sourcesList and detail all data sources. Sources may include:• Interviews with stakeholders,• Samples of routinely collected data,• Prospectively collected samples for the purpose of the simulation study,• Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n3.2 Pre-processingProvide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers.\n✅\nN/A\nN/A\n✅\nN/A\nN/A\nN/A\n✅\n\n\n\n3.3 Input parametersList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:• Base case data• Data use in experimentation, where different from the base case.• Where optimisation or design of experiments has been used, state the range of values that parameters can take.• Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n🟡\n🟡\n✅\n🟡\n✅\n✅\n✅\n✅\n\n\n\n3.4 AssumptionsWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n✅\n❌\n✅\n✅\n❌\n✅\n✅\n✅\n\n\n\nExperimentation\n\n\n\n\n\n\n\n\n\n\n\n4.1 InitialisationReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n🟡\n❌\n❌\n🟡\n❌\n✅\n❌\n✅\n\n\n\n4.2 Run lengthDetail the run length of the simulation model and time units.\n✅\n✅\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\n4.3 Estimation approachState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n🟡\n🟡\n🟡\n✅\n✅\nN/A\n✅\n✅\n\n\n\nImplementation\n\n\n\n\n\n\n\n\n\n\n\n5.1 Software or programming languageState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g. Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n🟡\n\n\n\n5.2 Random samplingState the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n❌\n❌\n❌\n❌\n❌\nN/A\n❌\n✅\n\n\n\n5.3 Model executionState the event processing mechanism used e.g. three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n🟡\n❌\n❌\n❌\n❌\n❌\n❌\n✅\n\n\n\n5.4 System specificationState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n✅\n❌\n🟡\n🟡\n❌\n❌\n🟡\n🟡\n\n\n\nCode access\n\n\n\n\n\n\n\n\n\n\n\n6.1 Computer model sharing statementDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nI find this chart a really helpful visualisation, to see what is commonly being met, things that are often not applicable, and things often not met.\nBelow, I’ve reflected on the impact of each criteria being fulfilled, on the reproduction. I’ve also identified where I feel I’ve been a bit more lax in my evaluation - indeed, the “subjective” nature of this evaluation is a limitation - although did (a) get consensus on uncertainties and unmet, and (b) revisit alongside other evaluations when writing this section, which helped me identify a few inconsistent decisions between studies within the evaluation, which I then addressed.\n1.1 Purpose of the modelExplain the background and objectives for the model.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n1.2 Model outputsDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n\nOne paper was rated as “partially met” as many of the outcomes used were not defined.\nFor those marked fully met, this was mainly based on them mentioning or being pretty clear about what the outcomes were, but wasn’t super strict - ie. didn’t require equations, or to specify how and when they were calculated.\nWhilst this is important, the raw/basic outcomes themselves were generally pretty straightforward, and issues with outputs more-so related to:\n\nWhich output to use from the results table (if unclear names, or similar names)\nHow to apply any required transformations\n\n\n1.3 Experimentation aimsIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis – Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments – Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation – (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n\nIn all seven papers with scenarios, these were described. Didn’t necessarily provide a “name” for each scenario, nor a “rationale” for the choice of scenarios, but did describe them.\nAlthough the description of the scenario can feel clear from the paper, when the code was not provided, it could be quick tricky and time-consuming to work out how to appropriately change the code, in order to implement the scenario.\n\n2.1 Base model overview diagramDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n\nAll papers included a diagram, which was great.\n\n2.2 Base model logicGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n2.3 Scenario logicGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n\nOverlap with 1.3 (implicit in describing 1.3, that would describe this).\n\n2.4 AlgorithmsProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e. scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n\nThose partially met are for describing some but not all of the algorithms. However, it is worth noting that it could be hard to actually ensure all relevant algorithms were described, if I hadn’t identified them in the more complex models.\n\n2.5.1 Components - entitiesGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n\nPretty basic requirement that all meet (unsurprisingly, as implicit in description of model / logic)\n\n2.5.2 Components - activitiesDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n\nPretty basic requirement that all meet (unsurprisingly, as implicit in description of model / logic)\nDidn’t necessarily require explicit description of routing.\n\n2.5.3 Components - resourcesList all the resources included within the model and which activities make use of them.\n\nGenerally seem to be mentioned when included, particularly as often form part of output (e.g. resource utilisation)\nWhen not mentioned (and based on known structure of model), assume not relevant.\n\n2.5.4 Components - queuesGive details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n\nAs for 2.5.3\n\n2.5.5 Components - entry/exit pointsGive details of the model boundaries i.e. all arrival and exit points of entities. Detail the arrival mechanism (e.g. ‘thinning’ to mimic a non-homogenous Poisson process or balking).\n\nGenerally fairly implicit\nOverlap with 2.4 algorithms, and hence didn’t necessarily mark this down if not full detail of arrival mechanism.\n\n3.1 Data sourcesList and detail all data sources. Sources may include:• Interviews with stakeholders,• Samples of routinely collected data,• Prospectively collected samples for the purpose of the simulation study,• Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n\nAll meet.\n\n3.2 Pre-processingProvide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers.\n\nOften had to assume that none occurred if none described.\n\n3.3 Input parametersList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:• Base case data• Data use in experimentation, where different from the base case.• Where optimisation or design of experiments has been used, state the range of values that parameters can take.• Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n\nThis was very important as it allows to check that the code parameters were correct as - in several cases - the provided code did not include the base case parameters as described. When missing from the paper, it was not possible to check them.\n\n3.4 AssumptionsWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n\nAlthough not relevant for reproduction, would be very relevant for reuse and validity\n\n4.1 InitialisationReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n\nOften not reported, and would be handy as it’s a pretty basic/fundamental aspect to the model that would help readers to have better understanding of how the model is working.\n\n4.2 Run lengthDetail the run length of the simulation model and time units.\n\nImportant to mention for same reasons as 3.3\n\n4.3 Estimation approachState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n\nThe description of this criteria is quite confusing - generally just focussed on identifying whether it was multiple replications or a big run, and then if the numbers used were justified.\nPartially met cases are those with replications that are not justified.\n\n5.1 Software or programming languageState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g. Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n\nWill often mentioned the programming language, but not operating system or version numbers\nThis was pretty handy, when versions were given, for cases where no versions were given in the repository itself - although ideally, versions could just simply be there.\n\n5.2 Random samplingState the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n\nFrequently not described, despite some of the studies having implemented and used seeds.\nInitially incorrectly addressed this in one of my evaluations, accidentally adding notes on the sampling of arrivals, rather than specifically on random samples. Have noted this to be clear that it was possible to misinterpret (although that may be more a mistake of my own! and not one others would necessarily make.)\n\n5.3 Model executionState the event processing mechanism used e.g. three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n\nOften not mentioned event processing machnism\nThis is a very long list of requirements in one category, and I found a bit difficult to evaluate, e.g. to check against all criteria\n\n5.4 System specificationState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n\nModel run time was really important to mention, but often not given. Particularly important for the models with longer run times, to know what to expect - including in light of reuse, and perhaps short times being necessary in certain contexts - or here, when troubleshooting, to know I might need to try lower numbers first while getting it working\nOverlaps with 5.3 parallel and 5.1 operating system.\n\n6.1 Computer model sharing statementDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n\nThis is inevitable/selection bias, given we chose papers that had links to code.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#des-checklist-derived-from-ispor-sdm",
    "href": "pages/paper_evaluation.html#des-checklist-derived-from-ispor-sdm",
    "title": "8  Evaluation of the article",
    "section": "8.3 DES checklist derived from ISPOR-SDM",
    "text": "8.3 DES checklist derived from ISPOR-SDM\nKey:\n\nS: Shoaib and Ramamohan (2021) - link to evaluation\nHu: Huang et al. (2019) - link to evaluation\nL: Lim et al. (2020) - link to evaluation\nK: Kim et al. (2021) - link to evaluation\nA: Anagnostou et al. (2022) - link to evaluation\nJ: Johnson et al. (2021) - link to evaluation\nHe: Hernandez et al. (2015) - link to evaluation\nW: Wood et al. (2021) - link to evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nS\nHu\nL\nK\nA\nJ\nHe\nW\n\n\n\n\n\nModel conceptualisation\n\n\n\n\n\n\n\n\n\n\n\n1 Is the focused health-related decision problem clarified?…the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n2 Is the modeled healthcare setting/health condition clarified?…the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n3 Is the model structure described?…the model’s conceptual structure was described in the form of either graphical or text presentation.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n4 Is the time horizon given?…the time period covered by the simulation was reported.\n✅\n✅\n✅\n✅\n❌\n✅\n✅\n✅\n\n\n\n5 Are all simulated strategies/scenarios specified?…the comparators under test were described in terms of their components, corresponding variations, etc\n✅\n✅\n✅\n✅\nN/A\n✅\n✅\n✅\n\n\n\n6 Is the target population described?…the entities simulated and their main attributes were characterized.\n✅\n❌\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\nParamaterisation and uncertainty assessment\n\n\n\n\n\n\n\n\n\n\n\n7 Are data sources informing parameter estimations provided?…the sources of all data used to inform model inputs were reported.\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n8 Are the parameters used to populate model frameworks specified?…all relevant parameters fed into model frameworks were disclosed.\n🟡\n🟡\n✅\n✅\n✅\n✅\n✅\n✅\n\n\n\n9 Are model uncertainties discussed?…the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n🟡\n❌\n❌\n❌\n✅\nN/A\n✅\n✅\n\n\n\n10 Are sensitivity analyses performed and reported?…the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters’ plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n✅\n❌\n✅\n❌\nN/A\n✅\n❌\n✅\n\n\n\nValidation\n\n\n\n\n\n\n\n\n\n\n\n11 Is face validity evaluated and reported?…it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n❌\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n\n\n\n12 Is cross validation performed and reported…comparison across similar modeling studies which deal with the same decision problem was undertaken.\nN/A\n❌\n❌\n✅\n❌\n✅\n❌\n❌\n\n\n\n13 Is external validation performed and reported?…the modeler(s) examined how well the model’s results match the empirical data of an actual event modeled.\nN/A\nN/A\nN/A\n✅\n❌\n✅\n❌\n❌\n\n\n\n14 Is predictive validation performed or attempted? …the modeler(s) examined the consistency of a model’s predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\nN/A\nN/A\nN/A\nN/A\nN/A\n❌\nN/A\nN/A\n\n\n\nGeneralisability and stakeholder involvement\n\n\n\n\n\n\n\n\n\n\n\n15 Is the model generalizability issue discussed?…the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n✅\n✅\n✅\n❌\n🟡\n✅\n❌\n✅\n\n\n\n16 Are decision makers or other stakeholders involved in modeling?…the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n❌\n❌\n❌\n❌\n✅\n❌\n❌\n❌\n\n\n\n17 Is the source of funding stated?…the sponsorship of the study was indicated.\n✅\n❌\n✅\n✅\n✅\n✅\n❌\n✅\n\n\n\n18 Are model limitations discussed?…limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\n✅\n🟡\n✅\n✅\n🟡\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n                                                \n\n\nThese guidelines have quite a different focus from STRESS-DES. STRESS-DES is very focused on what and how modelling work is performed. These guidelines do cover that, but have quite alot of focus on validity of the work, and good practice (e.g. stating funding, involving stakeholders). This is important, as different checklists can be important - and although practice is typically to use one practice, referring to more than one could also be beneficial.\n1 Is the focused health-related decision problem clarified?…the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n2 Is the modeled healthcare setting/health condition clarified?…the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n3 Is the model structure described?…the model’s conceptual structure was described in the form of either graphical or text presentation.\n\nPretty basic requirement that all meet (unsurprisingly).\n\n4 Is the time horizon given?…the time period covered by the simulation was reported.\n\nThere was only one study where this was not stated (Anagnostou et al. (2022)), though this didn’t impact the reproduction itself, for this study I was simply able to run the code and pretty much get the result required with minimal troubleshooting.\n\n5 Are all simulated strategies/scenarios specified?…the comparators under test were described in terms of their components, corresponding variations, etc\n\nAll papers with scenarios described them. As reflected for STRESS-DES, though the description of the scenario can feel clear from the paper, when the code was not provided, it could be quick tricky and time-consuming to work out how to appropriately change the code, in order to implement the scenario.\n\n6 Is the target population described?…the entities simulated and their main attributes were characterized.\n\nFulfilment didn’t impact the reproduction as this is more about interpretation/validity/etc.\n\n7 Are data sources informing parameter estimations provided?…the sources of all data used to inform model inputs were reported.\n\nAll met, although it’s worth noting that I didn’t check that every single parameter’s data source was stated (and indeed, its likely some were not) - simply whether I could identified that at least some were.\n\n8 Are the parameters used to populate model frameworks specified?…all relevant parameters fed into model frameworks were disclosed.\n\nThis was quite important for the reproduction as it allows us to check the parameters in the code are correct. When I identified that there were some discrepancies between the article and code, then in cases where a parameter was not given in the article, I couldn’t be sure if it was correct or not in the code, as there was nothing to compare against.\n\n9 Are model uncertainties discussed?…the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n\nSome don’t (but worth noting this doesn’t have impact on reproduction - beyond just these being additional values/lines in plot that we are trying to reproduce)\n\n10 Are sensitivity analyses performed and reported?…the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters’ plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n\nSome performed, some mentioned but no results presented, and some didn’t mention at all. One explained it to not be relevant.\n\n11 Is face validity evaluated and reported?…it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n\nRare (only one completed) - but didn’t impact reproduction as this is more related to validity\n\n12 Is cross validation performed and reported…comparison across similar modeling studies which deal with the same decision problem was undertaken.\n\nRare (only two completed) - but didn’t impact reproduction as this is more related to validity\n\n13 Is external validation performed and reported?…the modeler(s) examined how well the model’s results match the empirical data of an actual event modeled.\n\nRare (only two completed) - although some argue not applicable - but didn’t impact reproduction as this is more related to validity\n\n14 Is predictive validation performed or attempted? …the modeler(s) examined the consistency of a model’s predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\n\nGenerally not applicable - but didn’t impact reproduction as this is more related to validity\n\n15 Is the model generalizability issue discussed?…the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n\nMore than half did discuss this - but didn’t impact reproduction as this is more related to reuse\n\n16 Are decision makers or other stakeholders involved in modeling?…the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n\nRare (only one mentioned) - but didn’t impact reproduction as this is more related to validity\n\n17 Is the source of funding stated?…the sponsorship of the study was indicated.\n\nGood practice, usually a journal requirement, although some did not have - but didn’t impact reproduction.\n\n18 Are model limitations discussed?…limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\n\nMost discuss limitations (didn’t really assess how comprehensive these were though, except in two cases where they only had a very general or hint towards limitations) - but didn’t impact reproduction.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#timings",
    "href": "pages/paper_evaluation.html#timings",
    "title": "8  Evaluation of the article",
    "section": "8.4 Timings",
    "text": "8.4 Timings\n\nShoaib and Ramamohan (2021) - 1h 56m\nHuang et al. (2019) - 1h 28m\nLim et al. (2020) - 1h 12m\nKim et al. (2021) - 2h 12m\nAnagnostou et al. (2022) - 53m\nJohnson et al. (2021) - 1h 32m\nHernandez et al. (2015) - 1h 11m\nWood et al. (2021) - 1h 24m\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\nIt sometimes took quite a while to find all this information from the articles - and we acknowledge there’s a chance that it is provided somewhere in the article but I missed it. For both of these reasons, we see the value in actually attaching a completed reporting checklist, clearly laying out key information about the model and study.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#use-of-reporting-guidelines",
    "href": "pages/paper_evaluation.html#use-of-reporting-guidelines",
    "title": "8  Evaluation of the article",
    "section": "8.5 Use of reporting guidelines",
    "text": "8.5 Use of reporting guidelines\nRegarding whether each study mentioned using reporting guidelines:\n\nShoaib and Ramamohan (2021) - ❌\nHuang et al. (2019) - ❌\nLim et al. (2020) - ❌\nKim et al. (2021) - ❌\nAnagnostou et al. (2022) - ❌\nJohnson et al. (2021) - ✅ Consolidated Health Economic Evaluation Reporting Standards (CHEERS) - Husereau et al. (2013)\nHernandez et al. (2015) - ❌\nWood et al. (2021) - ✅ STRESS-DES: Strengthening The Reporting of Empirical Simulation Studies (Discrete-Event Simulation) - Monks et al. (2019)\n\nAlthough this is only a small sample, its interesting to note that the two studies that used reporting guidelines both had the highest proportion of fully met criteria in either reporting guideline.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#uses-a-previously-reported-model",
    "href": "pages/paper_evaluation.html#uses-a-previously-reported-model",
    "title": "8  Evaluation of the article",
    "section": "8.6 Uses a previously reported model",
    "text": "8.6 Uses a previously reported model\nRegarding whether each study was using a previously reported model:\n\nShoaib and Ramamohan (2021) - No\nHuang et al. (2019) - No\nLim et al. (2020) - No\nKim et al. (2021) - Yes - previously described by Glover et al. (2018) and Thompson et al. (2018)\nAnagnostou et al. (2022) - No\nJohnson et al. (2021) - Yes - EPIC model previously described by Sadatsafavi et al. (2019)\nHernandez et al. (2015) - No\nWood et al. (2021) - Yes - previously described by Wood et al. (2020)\n\nAgain, a small sample, but this time a weaker pattern. We note that the two studies that used reporting guidelines are the same that are previously reported models here, alongside one other study which was previously reported but did not use reporting guidelines in this instance, and has a lower proportion of criteria that were fully met.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/paper_evaluation.html#references",
    "href": "pages/paper_evaluation.html#references",
    "title": "8  Evaluation of the article",
    "section": "8.7 References",
    "text": "8.7 References\n\n\n\n\nAnagnostou, Anastasia, Derek Groen, Simon J. E. Taylor, Diana Suleimenova, Nura Abubakar, Arindam Saha, Kate Mintram, et al. 2022. “FACS-CHARM: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level.” In 2022 Winter Simulation Conference (WSC), 1223–34. https://doi.org/10.1109/WSC57314.2022.10015462.\n\n\nGlover, Matthew J., Edmund Jones, Katya L. Masconi, Michael J. Sweeting, Simon G. Thompson, Janet T. Powell, Pinar Ulug, and Matthew J. Bown. 2018. “Discrete Event Simulation for Decision Modeling in Health Care: Lessons from Abdominal Aortic Aneurysm Screening.” Medical Decision Making 38 (4): 439–51. https://doi.org/10.1177/0272989X17753380.\n\n\nHernandez, Ivan, Jose E. Ramirez-Marquez, David Starr, Ryan McKay, Seth Guthartz, Matt Motherwell, and Jessica Barcellona. 2015. “Optimal Staffing Strategies for Points of Dispensing.” Computers & Industrial Engineering 83 (May): 172–83. https://doi.org/10.1016/j.cie.2015.02.015.\n\n\nHuang, Shiwei, Julian Maingard, Hong Kuan Kok, Christen D. Barras, Vincent Thijs, Ronil V. Chandra, Duncan Mark Brooks, and Hamed Asadi. 2019. “Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation.” Frontiers in Neurology 10 (June). https://doi.org/10.3389/fneur.2019.00653.\n\n\nHusereau, Don, Michael Drummond, Stavros Petrou, Chris Carswell, David Moher, Dan Greenberg, Federico Augustovski, Andrew H. Briggs, Josephine Mauskopf, and Elizabeth Loder. 2013. “Consolidated Health Economic Evaluation Reporting Standards (CHEERS) Statement.” Value in Health 16 (2): e1–5. https://doi.org/10.1016/j.jval.2013.02.010.\n\n\nJohnson, Kate M., Mohsen Sadatsafavi, Amin Adibi, Larry Lynd, Mark Harrison, Hamid Tavakoli, Don D. Sin, and Stirling Bryan. 2021. “Cost Effectiveness of Case Detection Strategies for the Early Detection of COPD.” Applied Health Economics and Health Policy 19 (2): 203–15. https://doi.org/10.1007/s40258-020-00616-2.\n\n\nKim, Lois G., Michael J. Sweeting, Morag Armer, Jo Jacomelli, Akhtar Nasim, and Seamus C. Harrison. 2021. “Modelling the Impact of Changes to Abdominal Aortic Aneurysm Screening and Treatment Services in England During the COVID-19 Pandemic.” PLOS ONE 16 (6): e0253327. https://doi.org/10.1371/journal.pone.0253327.\n\n\nLim, Chun Yee, Mary Kathryn Bohn, Giuseppe Lippi, Maurizio Ferrari, Tze Ping Loh, Kwok-Yung Yuen, Khosrow Adeli, and Andrea Rita Horvath. 2020. “Staff Rostering, Split Team Arrangement, Social Distancing (Physical Distancing) and Use of Personal Protective Equipment to Minimize Risk of Workplace Transmission During the COVID-19 Pandemic: A Simulation Study.” Clinical Biochemistry 86 (December): 15–22. https://doi.org/10.1016/j.clinbiochem.2020.09.003.\n\n\nMonks, Thomas, Christine S. M. Currie, Bhakti Stephan Onggo, Stewart Robinson, Martin Kunc, and Simon J. E. Taylor. 2019. “Strengthening the Reporting of Empirical Simulation Studies: Introducing the STRESS Guidelines.” Journal of Simulation 13 (1): 55–67. https://doi.org/10.1080/17477778.2018.1442155.\n\n\nSadatsafavi, Mohsen, Shahzad Ghanbarian, Amin Adibi, Kate Johnson, J. Mark FitzGerald, William Flanagan, Stirling Bryan, and Don Sin. 2019. “Development and Validation of the Evaluation Platform in COPD (EPIC): A Population-Based Outcomes Model of COPD for Canada.” Medical Decision Making 39 (2): 152–67. https://doi.org/10.1177/0272989X18824098.\n\n\nShoaib, Mohd, and Varun Ramamohan. 2021. “Simulation Modelling and Analysis of Primary Health Centre Operations.” arXiv, June. https://doi.org/10.48550/arXiv.2104.12492.\n\n\nThompson, Simon G, Matthew J Bown, Matthew J Glover, Edmund Jones, Katya L Masconi, Jonathan A Michaels, Janet T Powell, Pinar Ulug, and Michael J Sweeting. 2018. “Screening Women Aged 65 Years or over for Abdominal Aortic Aneurysm: A Modelling Study and Health Economic Evaluation.” Health Technology Assessment 22 (43): 1–142. https://doi.org/10.3310/hta22430.\n\n\nWood, Richard M., Christopher J. McWilliams, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2020. “COVID-19 Scenario Modelling for the Mitigation of Capacity-Dependent Deaths in Intensive Care.” Health Care Management Science 23 (3): 315–24. https://doi.org/10.1007/s10729-020-09511-7.\n\n\nWood, Richard M., Adrian C. Pratt, Charlie Kenward, Christopher J. McWilliams, Ross D. Booton, Matthew J. Thomas, Christopher P. Bourdeaux, and Christos Vasilakis. 2021. “The Value of Triage During Periods of Intense COVID-19 Demand: Simulation Modeling Study.” Medical Decision Making 41 (4): 393–407. https://doi.org/10.1177/0272989X21994035.\n\n\nZhang, Xiange, Stefan K. Lhachimi, and Wolf H. Rogowski. 2020. “Reporting Quality of Discrete Event Simulations in Healthcare—Results From a Generic Reporting Checklist.” Value in Health 23 (4): 506–14. https://doi.org/10.1016/j.jval.2020.01.005.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation of the article</span>"
    ]
  },
  {
    "objectID": "pages/compendium.html",
    "href": "pages/compendium.html",
    "title": "9  Reflections from research compendium and test-run",
    "section": "",
    "text": "9.1 Reflections related to compendium\nAfter completing the reproduction and evaluation, the final stage involved organising the reproduction/ folder into a mini “research compendium”, and having a test-run of this by a second STARS team member.\nOrganisation:\nTests:\nGitHub actions:\nDocker:",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reflections from research compendium and test-run</span>"
    ]
  },
  {
    "objectID": "pages/compendium.html#reflections-related-to-compendium",
    "href": "pages/compendium.html#reflections-related-to-compendium",
    "title": "9  Reflections from research compendium and test-run",
    "section": "",
    "text": "No one right way to organise. If it was already pretty good, I left it (e.g. Kim et al. 2021). However, for lots of them, I did change organise to e.g. scripts/, inputs/, outputs/\n\n\n\nWhen creating the tests, since many of the models had long run times, my intention was often just to create a “mini-run” of the model that allows someone to quickly and easily check if they can run some aspect of the model (without needing to try the whole one with long run time).\nThe tests created provide examples of how can set up tests in Python and R, doing something basic like comparing dataframes or CSV files between runs. Indeed, it took a little bit of time to get set up initially with tests working as I want, but once figured for one, I could reuse like a template for the next.\nSometimes had to amend model to be able to run tests - e.g. for Wood et al. 2021, test wouldn’t work if model was run with parallel processing\nAlso, an example of Johnson et al. 2021 where I couldn’t get the model to work correctly within a test (despite no visible errors), and so went with the solution of making a normal .R script that operates like a test but without the testthat package\n\n\n\nWasn’t able to successfully build a quarto book using GitHub actions if it required both Python and R (in which case, had to push from local)\n\n\n\nThis sometimes took rather alot of troubleshooting, and could sometimes be quite tricky. However, once resolved (Python, R, and later a older Python), I could then just reuse previous dockerfiles like a template (although sometimes still some troubleshooting still needed).",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reflections from research compendium and test-run</span>"
    ]
  },
  {
    "objectID": "pages/compendium.html#reflections-related-to-test-run",
    "href": "pages/compendium.html#reflections-related-to-test-run",
    "title": "9  Reflections from research compendium and test-run",
    "section": "9.2 Reflections related to test-run",
    "text": "9.2 Reflections related to test-run\nThe test-run stage after making compendium helped us to spot run issues that I hadn’t noticed, mainly from having not completely tested absolutely everything before pushing. For example:\n\nShoaib and Ramamohan 2022 - there was an authentication error when trying to pull the docker image from GitHub Container Registry, as I was missing some of the steps related to personal access tokens\nHernandez et al. 2015 - though the test worked locally, it did not work on docker. This was related to imports, and resolved by adding __init__.py files, but I hadn’t noticed as hadn’t checked the test on docker.\nLim et al. 2020 - one of the files failed to run as I hadn’t uploaded some of the required data\n\nShows importance of having someone else check things - easy to leave in small mistakes that mean it doesn’t run. In general, having something like this to check your code is a really handy practice, if possible.\nThe test-run stage also helped highlight things that weren’t clear. For example, I had a test that took a few minutes to run, and whilst running your console is blank. He was unsure if this was an error - so I then add a section to the README that explained what to expect to see when running those tests.\nAlso, likely errors people will encounter - for example, by not having a particular environment active or not being in the right folder when trying to run something - and so I then made sure to clarify this in the README.\nWhen running test-runs on a fresh machine, Tom often found there were operating system dependencies he had to install for R. Also, examples like Huang et al. 2019 were Tom looked to run on his virtual machine but that only allocates 4GB RAM and the model used 8GB RAM so he had to use a different machine - hence, also helping highlight memory requirements.",
    "crumbs": [
      "Results",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reflections from research compendium and test-run</span>"
    ]
  },
  {
    "objectID": "pages/acknowledgements.html",
    "href": "pages/acknowledgements.html",
    "title": "10  Acknowledgements",
    "section": "",
    "text": "We would like to thank the authors who made their code available under open licences, facilitating our research. We are especially grateful to the following authors for their helpful communication and support throughout the project:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Acknowledgements</span>"
    ]
  }
]