---
title: "Comparison to other studies"
bibliography: ../references.bib
---

## Studies that assessed reproducibility

### Summary of studies

| Study | Focus | Articles that tried running code from | Brief summary |
| - | - | - | - |
| @chang_is_2022 | Macroeconomics articles | 59 | Of 67 articles, 6 use confidential data and 2 use software they don't have, leaving 59. Of those, replicate 22 without author assistance and 29 with. (Note: no access to article - based on abstract). |
| @eubank_lessons_2016 | Articles in the Quarterly Journal of Political Science which underwent in-house replication review | 24 | Of the 24, 4 didn't require modifications, 13 had code that would run without error, 8 were missing code for results in paper, 7 had no installation directions for dependencies, 14 (58%) had results in paper that differed from code-generated |
| @galiani_incentives_2017 | Economics articles | Unclear | Of 203 included articles, 76% published any data or code. For those that had data and code, 54% of data manipulation code and 61% of "estimation code" could run without major modifications. They found 14% articles fully replicable (raw data to results) and 37% partially (estimated data to results) |
| @konkol_computational_2019 | Geoscientific articles | 41 | Of 41 (R), 2 ran without issue, 33 had resolvable issues, 2 were partially executable, and 4 could not resolve issues. Author assistance for several (e.g. pointing out solutions, sending additional code and data). |
| @krafczyk_learning_2021 | Articles from Journal of Computational Physics | 7 | Attempt to reproduce 7, allow troubleshooting up to 40 hours, success only described via viewing in a figure |
| @henderson_reproducibility_2024 | Infectious disease models | 67 | 200 (100 random, 100 most highly cited). Not reproducible if properietary software/data inaccessible. For random, 19 had code and data. Of those, 4 completedly reproduced and 8 partially. For highly cited, 48 had code and data. Of those, 11 completely reproduced and 22 partially. |
| @mccullough_lessons_2006 | Articles from Journal of Money, Credit and Banking | 58 | Of 266 articles, 193 should have archive entries, but only 69 did. Of those 69, should have code and data, but found only data for 11 of them (i.e. no code). Attempted to use supplied data and code of remaining 58 to reproduce published results. Allowed minor troubleshooting. |
| @obels_analysis_2020 | Registered Reports in psychology | 36 | Of 62 articles, 36 had data and code. Could run 31 and reproduced main results for 21. |
| @samuel_computational_2024 | Jupyter notebooks associated with publications | 15,817 | 3467 publications, from which 27,271 jupyter notebooks were downloaded for further analysis. Of those, 22,578 were Python and 15,817 had dependencies declared in standard file. Attempted to re-run those automatically. 10,388 installed dependencies successfully. Of those, 1203 ran without error - and then 879 had results identical to original, 324 differed |
| @stagge_assessing_2019 | Hydrology and water resources articles | 20 | Looked at data, model and code availability in 360 articles. Of those, 20 had required data/code/directions available for reproduction. Fully reproduce 4, partially reproduced 2, couldn't reproduce 2, and found 10 actually didn't have all the required artefacts. Allowed minor troubleshooting (e.g. folder/file locations) |
| @stockemer_data_2018 | Political science articles | 71 | Looked for data/code in 145 articles, then emailed authors to ask for it. Received data and code for 71. For those, ran code, and success in reproducing apx. 70%. |
| @stodden_enabling_2018 | Articles from Journal of Computational Physics | 55 | Precursor to @krafczyk_learning_2021. Of 306 articles, 55 had code. Spent max 4 hours on them, managed to reproduce some struff for 18, but didn't complete any. |
| @wood_push_2018 | Impact evaluations in low- and middle-income countries | 50 | Of 109, 27 ran to produced comparable results. 59 refused to provide replications files. Of remaining 23, 3 had propreitrary data, 15 had incomplete replication files, and 5 had minor differences in results. | 
: {tbl-colwidths="[20, 25, 15, 40]"}

Examples that use code and/or description in article (i.e. dont' *require* code):

* @laurinavichyute_share_2022 - Articles in the Journal of Memory and Language - Look at data sharing in 59 articles before and 59 articles after a policy is in place. For the 59 after policy, attempted to reproduced. 8 had inaccessible data. Under strict criteria, reproduced 20 (34%). Under lenient criteria, 33 (56%).
* @hardwicke_data_2018 - 35 articles, two team members attempted for each article, used available data and information in article or any other additional documentation
* @hardwicke_analytic_2021 - Articles in Psychological Science - 25 evaluated - initially 16 had at least one "major numerical discprenancy", after troubleshooting with/without authors, 37 discrepancnies remained from 789 checked values
* @ostermann_advancing_2017 - reproducibility required full data set + executable tools or precise step-by-step instructions

Examples that reuses data and repeats analysis "exactly as described in report" with clarification if needed:

* @schwander_replication_2021 - 4 health economic obesity models
* @naudet_data_2018
* @raff_step_2019
* @artner_reproducibility_2021
* @maassen_reproducibility_2020

### Which studies are most relevant? How do they compare?

Note: It is difficult to make comparisons between all these studies due to the large variation in methods (such as amount of troubleshooting allowed, or thresholds for deciding something is reproduced). As such, the actual percentage reproduced is difficult to compare.

However, this section does try to pull out the more relevant studies (in terms of their focus and/or methods), and consider the **similarities and differences** to our work (e.g. broadly considering reproduction findings, but focusing particularly on **issues encountered** and **recommendations made**).

<!--TODO: Write this section -->

**Similar focus**: @henderson_reproducibility_2024

* Infectious disease models (i.e. models, health)

::: {.callout-note collapse="true"}

## Some old rough notes

* @krafczyk_learning_2021 - results are recommendations based on experiences, which were:
    * can see completion in figure 1, but focus is very much on the learnings rather than % reproduced and so on
    * being clear on links between article, code and data (e.g. which code uses which data, and which parts of code made each bit of article)
    * include scripts for each aspect of article, with it easy to locate the scripts needed, and the scripts including the parameters needed and clearly label
    * be clear about hardware needed, e.g. if large amount of computing resources would be required. at least report hardware used. ideally include "small test case that can be run by users with conventional hardware"
    * list software dependencies and versions
    * use seeds, and report seed you used
    * make all code and data available with an appropriate licence
    * include master script that runs in computations in publication
    * use same terminology in code and article
    * use version control and specify the e.g. commit hash that identifies the version used
    * avoid hard coding parameters
    * design scripts in a way that allows people to easily change parameters and run again
    * avoid hard coding file paths
    * provide script that checks whether users results match original (within expected deviation)
    * if compare against competing methods, include info on how those were implemented and tested
    * use build system for C/C++ code
    * provide scripts to make the figures and tables
* @wood_push_2018
    * complete data: 27 comparable results, 5 minor differences, 0 major differences
    * incomplete data: 10 comparable, 4 minor differences, 1 major differences
    * main issue was the code and data not being shared
* @schwander_replication_2021
    * reproduction success for 3 out of 4 models
    * facilitators:
        * "Model structure and possible state transitions were presented in a state transition diagram"
        * "Overview of input parameters was provided in table format"
    * hurdles:
        * "PSAs were performed" (probablistic sensitivity analysis)
        * "Relevant PSA values for PSA result reproduction were provided (type of distribution and either mean and standard deviation or distribution parameters were provided)"
        * "Clinical event simulation results were provided (which are very helpful to guide potential assumptions to be made for rebuilding the model and which provide an additional means of testing the fit of the replication)"
        * "Relevant details on the underlying life tables were provided (including year of data)"
        * "Several self-created regression equations were introduced but without details on how to apply/solve the provided regressions correctly"
    * "We **investigated whether the CHEERS score might be predictive for the success of model replication**, but this was not the case, with scores ranging from 18 to 20. The non-successful replication case study 1 rated a score of 19 (the maximum possible CHEERS score is 24). A **comparable finding was observed by another research team that investigated the Phillips checklist** [24] in the context of model replication; they found that the Phillips checklist was not reliable for ensuring that studies are replicable [2]." They then **propose changes** to CHEERS that might help with replicability
* @henderson_reproducibility_2024
* @samuel_computational_2024
    * Reproduction success:
        * 27,271 jupyter notebooks (2660 GitHub repositories, 3467 publications, majority python)
        * 15,817 included dependencies in standard requirement files
        * Of those, 10,388 could be installed successfully
        * Of those, 1203 ran without error
        * Of those, 879 produced results identical to those reported in the original notebook
    * References several other studies that have attempted to re-run jupyter notebooks
    * ModuleNotFoundError, ImportError and FileNotFounderError were the top 3 common exceptions
    * Also used `flakenb` to look for code styling errors
    * Implications:
        * Low reproducibility as in prior studies
        * Review processes don't pay attention to journal reproducibiltiy
        * Common errors around dependencies... Use exiting approaches like requirements, conda and poetry
        * Some journals, article types, levels of documentation and research fields had more reproduced notebooks than others. Worth considering procedures at those journals with most reproduced (**iScience**). Also, notebooks combining computation and narrative.
        * Things go out of data... importance of pre-prints...
        * Reproducibility badges, reproducibility platforms like REScience, nanopublications

:::

## Studies that evaluated research artefacts and/or articles

<!--TODO: Look into this -->

e.g. @schwander_replication_2021 and @zhang_reporting_2020 for reporting... @laurinavichyute_share_2022 for code...

## Other relevant articles for introduction and/or discussion

* @monks_computer_2023 - see summary in **introduction**
* @hrynaszkiewicz_survey_2021
    * Survey in Autumn 2020 of previous authors and other registered users with PLOS Computational Biology
    * 214 complete responses
    * Lots had papers where code was not shared - reasons inc (1) takes too much time to prepare code for sharing, (2) software and systems dependencies (3) concerns with my ability to prepare the code for sharing (4) I needed to protect intellectual property, etc...
    * More likely to submit to journal if mandatory code sharing policy (moreso for ECR)
    * Low mean satisfication score for accessing others code (44.0)
* @cadwallader_survey_2022
    * Survey in Spring 2021 of previous authors and other registers used with PLOS computational biology related disciplines
    * 188 complete responses
    * Questions on
        * How often look at code associated with research articles
        * Whether have encountered different methods of code sharing
        * How useful found different methods of accessing code - **link to code repository was rated most useful - with code "available upon request" least useful**. When explaining why link was most useful, common reasons were: (**a) ability to see new versions of code (b) quick access to code (c) good documentation/README (d) practicality (e) reproduction of results (f) established**. For sharing via notebooks, like ability to explore code and reproduction of results. For archived code, like that it gives long term access.
        * Features of code notebooks that are useful when accessing/reading code, highest ranked: (1) having all code, data and figures in one place, (2) knowing code is running in right environment (3) ability to interact inline with code in browser (4) ability to uncover data point by hovering over point in graph ...etc
        * Satisifcation and importance scores of factors associated with sharing code
        * Time spent preparing and sharing code
        * Extra time willing to spend to make code easier to read and run
* @noauthor_revisiting_2022
    * Initiative from Nature Machine Intelligence to have new article type **"Reusability Report"** where researchers try code from article in journal and try running it, applying to new data or tweaking or extending it
* @mejba_evolution_2023
    * Lists **benefits of code reuse**: increase **productivity**, improved code quality, reduced errors, cost efficiency, faster time-to-market, **knowledge sharing**, scalability, sustainability, innovation
    * Lists challenges of code reuse: **understanding reused code**, integration issues, incompatability, maintaining and updating reused code, **licensing and ownership**, security risks, quality control, overhead, dependency management
    * Gives solutions to changes e.g. understanding reused code --> **comprehensive documentation**
    * Strategies for effective code reuse... modular design, use of libraries and frameworks, design patterns, object oriented programming, apis and microservices, code documentation, automated testing, refactoring, code reviews, ci/cd
    * Lists security implications of code reuse, and best practices for security
    * Impacts of code reuse on software development lifecycle:
        * Planning and analysis - **reuse code can reduce time and resources required, although crucial to check code compatability, quality and security to meet needs**
        * Design - design of code can facilitate reuse
        * Implementation - reuse snippets, lirbaries, etc.
        * Testing - reused code can have benefit of being previously tests so less bugs, more stable software, but will need rigorous re test
        * Deployment - if code has depenedencies, need to manage properly eg docker
        * Maintenance - if reused code well understood and well documentation can make maintence easier
* @dupre_beyond_2022
    * <https://theplosblog.plos.org/2022/05/uphold-the-code/> describes the article, saying " DuPree and colleagues draw a distinction between the scholarly article, which they argue functions as a kind of preview or entrypoint to the research, and the real scholarly work. That is, the methods researchers develop (in this case software and code), and the data they gather and analyze in order to produce a result. It’s this data and code which forms the core of the research, the fulcrum on which everything else turns. For that reason, code not only deserves a place in the literature, it is required in order to fully understand and appreciate the scholarship."
    * Article is about "**hybrid research objects**" - i.e. multiple content types, so narrative text plus at least one of: code, data, and/or computation. Can link in data/code availability statements. Publisher may require those linked objects to meet specific standards and include as part of review process. "Hope to see more hybrid research objects where each linked object is formatted with domain-relevant standards... and bi-directionally linked using persistent identifiers"
    * Article then talks about interactive research objects, with everything all in one place, like "executable research articles", RMarkdown, Jupyter Book, Binder, Pangeo, NeuroLibre
* @benureau_re-run_2018
    * They use terms from **Goble 2016**. They then show a simple python example which they progressively modify to meet these criteria. **Really nice paper! Could be good in introduction and elsewhere?**
        * **R1: Re-runnable** (executable)
            * "it is impossible to write future-proof code... modernizing an unmaintained ten-year-old code can reveal itself to be an arduous and expensive undertaking—and precarious... more straightforward solution is to recreate the old execution environment. For this to happen however, the dependencies in terms of systems, software, and libraries must be made clear enough."
        * **R2: Repeatable** (produce the same result more than once)
            * "it often comes down to controlling the initialization of the pseudo-random number generators (RNG)." --> **importance of seeds**
        * **R3: Reproducible** (allow an investigator to reobtain published results)
            * Explicit dependencies and operating system
            * "includes pre-processing, post-processing, and plotting code"
            * good practice to include short unit tests that verify reproduction
            * code and input data and result data should all be made available
        * **R4: Reusable** (easy to use, understand and modify)
            * comments and documentation
            * "reusability can represent an expensive endeavor if undertaken as an afterthought"
            * simple things that improve reusability:
                * avoid hardcoded values or magic numbers
                * changes to code behaviour should be via modifiable parameters rather than commenting/uncommenting - so (a) conditions are recorded as parameters, and (b) can have seperate scripts to run and produce results
        * **R5: Replicable** ("act as an available reference for any ambiguity in the algorithmic descriptions of the article")
            * "replicating implies writing a new code matching the conceptual description of the article, in order to reobtain the same result"
            * whilst this relies on the article, you will inveitably have ambiguities in the article, and so "Replication efforts use the paper first, and then the reproducible code that comes along with it whenever the paper falls short of being precise enough."
    * "In conclusion, the R3 **(reproducible) form should be accepted as the minimum scientific standard** (Wilson et al., 2017). "
* @wilson_good_2017
    * Supplements the "**Best Practices for Scientific Computing" article** by giving a minimum set of "**good enough**" things to do
    * Box 1 summarises the practices which are grouped into:
        * Data management (e.g. saving data, documenting steps, tidy data)
        * Software (eg. writing organising sharing scripts)
        * Collaboration (eg. make it easier for collaborators to understand and contribute)
        * Project organisation (eg. organise artefacts to ease discvoery and understanding)
        * Tracking changes (i.e. record how components of project change over time)
        * Manuscripts (eg. write manuscript with audit trail - so either history/change tracking, or version control)
* @gomes_why_2022
    * **Figure 1** is a nice 
    * **Barriers**
        * Knowledge barriers - unsure about process, complex or manual workflows, large data files, insecurity, do not see value
        * Reuse concerns - inappropriate use, rights, sensitive content, transient storage
        * Disincentives - scooping, lack of time, lack of incentives
* @connolly_software_2023
    * 3Rs of software enginerring: "readability (human understandable codes), resilience (fails rarely/gracefully), and reuse (can easily be used by others and can be embedded in other software)"
    * Walks through some practices... version control, software design, computer programming or coding, quality assurance, packaging and deployment, user documentation, team management, user engagement
* @eglen_toward_2017
    * Simple guidelines for code sharing: version control, persistent URLs, license, etiquette, documentation, tools, case studies, data, standards, tests, user supoprt
    * Tests - "We recommend including **test suites demonstrating that the code is producing the correct results**37. These tests can be at a low level (testing each individual function, called unit testing) or at a higher level (for example, testing that the program yields correct answers on simulated data)38." - and can link with **continuous integration** so automatically run and fail if change made to code that fails it
* @gil_toward_2016
    * Proposed checklits for things to include in paper, grouped into categories of:
        * Data accessibility
        * Data documentation
        * Software accessibility
        * Software documentation
        * Provenance documentation
        * Methods documentation
        * Authors identification
    * Suggest papers enriched with metadata/artefacts

## Journal procedures

<!--TODO: Finish this -->

*Psychological Science*

*Quarterly Journal of Political Science* in-house replication review - @eubank_lessons_2016

*Cortex* Verification Reports - @chambers_verification_2020

@galiani_incentives_2017 surveys journals about policies on sharing data and code

Journal of Memory and Language  - open data and code policy, requiring code and data to be released

Vadose Zone Journal - "Reproducible Research" programme where code and data is published alongside article - @skaggs_reproducible_2015