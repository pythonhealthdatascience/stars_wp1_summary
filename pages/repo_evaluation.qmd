---
title: "Evaluation of the repository"
echo: False
bibliography: ../references.bib
---

The code and related research artefacts in the original code repositories were evaluated against:

* The criteria of badges related to reproducibility from various organisations and journals.
* Recommendations from the STARS framework for the sharing of code and associated materials from discrete-event simulation models (@monks_towards_2024).

Between each journal badge, there was often alot of overlap in criteria. Hence, a list of unique criteria was produced. The repositories are evaluated against this criteria, and then depending on which criteria they met, against the badges themselves.

*Caveat: Please note that these criteria are based on available information about each badge online. Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals.*

Consider: **What criteria are people struggling to meet from the guidelines?**

<!-- TODO: Once complete, carefully go over again and check everything is correct -->

```{python}
# Imports and function
import plotly.express as px
import plotly.io as pio
import pandas as pd
from functions import eval_chart, plot_scatter
```

## Summary

**Unique badge criteria:**

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
criteria_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [2, 0, 10, 0],
  'Huang et al. 2019 (3/8)': [3, 0, 9, 0],
  'Lim et al. 2020 (9/9)': [2, 0, 10, 0],
  'Kim et al. 2021 (10/10)': [5, 0, 7, 0],
  'Anagnostou et al. 2022 (1/1)': [8, 0, 4, 0],
  'Johnson et al. 2021 (4/5)': [4, 0, 8, 0],
  'Hernandez et al. 2015 (1/8)': [2, 0, 10, 0],
  'Wood et al. 2021 (5/5)': [4, 0, 8, 0]
}
criteria_wide = pd.DataFrame(criteria_dict, index=col).T

eval_chart(criteria_wide).show(config={'displayModeBar': False})
```

::: {.callout icon=false collapse=false}

## Reflections

No clear relationship. I think it is more meaningful to actually look at what criteria were and were not met.

```{python}
plot_scatter('Unique badge criteria met',
             sho = 16.7,
             hua = 25,
             lim = 16.7,
             kim = 41.7,
             ana = 66.7,
             joh = 33.3,
             her = 16.7,
             woo = 33.3)
```

:::

**Badges:**

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
badge_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [0, 0, 12, 0],
  'Huang et al. 2019 (3/8)': [0, 0, 12, 0],
  'Lim et al. 2020 (9/9)': [0, 0, 12, 0],
  'Kim et al. 2021 (10/10)': [0, 0, 12, 0],
  'Anagnostou et al. 2022 (1/1)': [3, 0, 9, 0],
  'Johnson et al. 2021 (4/5)': [0, 0, 12, 0],
  'Hernandez et al. 2015 (1/8)': [0, 0, 12, 0],
  'Wood et al. 2021 (5/5)': [5, 0, 7, 0]
}
badge_wide = pd.DataFrame(badge_dict, index=col).T

eval_chart(badge_wide).show(config={'displayModeBar': False})
```

::: {.callout icon=false collapse=false}

## Reflections

Not certain how meaningful these numbers are, as we have imbalanced numbers of different types of badge, and meeting certain popular criteria will weight what is met v.s. not.

Feel that looking at the criteria met is a bit more meaningful? And then specific examples of how that translates into badges - e.g.

* None meeting ACM "Artifacts Evaluated - Functional" as requires xyz and these are commonly not met.
* For several, they meet three badges, but those three badges have one criteria: reproducing results.

```{python}
plot_scatter('Badges awarded',
             sho = 0,
             hua = 0,
             lim = 0,
             kim = 0,
             ana = 25,
             joh = 0,
             her = 0,
             woo = 41.7)
```

:::

**Essential components of STARS framework:**

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
essential_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [2, 0, 6, 0],
  'Huang et al. 2019 (3/8)': [2, 0, 6, 0],
  'Lim et al. 2020 (9/9)': [2, 0, 6, 0],
  'Kim et al. 2021 (10/10)': [4, 1, 3, 0],
  'Anagnostou et al. 2022 (1/1)': [7, 0, 1, 0],
  'Johnson et al. 2021 (4/5)': [4, 2, 2, 0],
  'Hernandez et al. 2015 (1/8)': [2, 0, 6, 0],
  'Wood et al. 2021 (5/5)': [2, 0, 6, 0]
}
essential_wide = pd.DataFrame(essential_dict, index=col).T

eval_chart(essential_wide).show(config={'displayModeBar': False})
```

::: {.callout icon=false collapse=false}

## Reflections

Similar to journal criteria (unsurprisingly, as looking at similar things) - most studies meet very few and have wide range of reproduction success, from 12.5% to 100%. Three met more, and these were 80% to 100% reproduced.

I think, if we were to draw anything from this, it would be to reflect on exactly what criteria were and were not met, and why/how that impacted reproduction, in any way (either success or time).

*Note: Just considers those fully met, in plot*

```{python}
plot_scatter('Essential STARS components met',
             sho = 25,
             hua = 25,
             lim = 25,
             kim = 50,
             ana = 87.5,
             joh = 50,
             her = 25,
             woo = 25)
```

:::

**Optional components of STARS framework:**

```{python}
# Create dataframe of results
optional_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [0, 0, 5, 0],
  'Huang et al. 2019 (3/8)': [2, 0, 3, 0],
  'Lim et al. 2020 (9/9)': [0, 0, 5, 0],
  'Kim et al. 2021 (10/10)': [0, 0, 5, 0],
  'Anagnostou et al. 2022 (1/1)': [1, 1, 3, 0],
  'Johnson et al. 2021 (4/5)': [0, 0, 5, 0],
  'Hernandez et al. 2015 (1/8)': [0, 0, 5, 0],
  'Wood et al. 2021 (5/5)': [0, 0, 5, 0]
}
optional_wide = pd.DataFrame(optional_dict, index=col).T

eval_chart(optional_wide).show(config={'displayModeBar': False})
```

::: {.callout icon=false collapse=false}

## Reflections

This highlights how Huang meets the most criteria, but is only partially reproduced - but I think it is most interesting to consider why this is.

*Note: Just considers those fully met, in plot*

```{python}
plot_scatter('Optional STARS components met',
             sho = 0,
             hua = 40,
             lim = 0,
             kim = 0,
             ana = 20,
             joh = 0,
             her = 0,
             woo = 0)
```

:::

## Journal badges

Key:

* **S:** @shoaib_simulation_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-shoaib-2022/evaluation/badges.html" target="_blank">link to evaluation</a>
* **Hu:** @huang_optimizing_2019  - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/badges.html" target="_blank">link to evaluation</a>
* **L:** @lim_staff_2020 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-lim-2020/evaluation/badges.html" target="_blank">link to evaluation</a>
* **K:** @kim_modelling_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-kim-2021/evaluation/badges.html" target="_blank">link to evaluation</a>
* **A:** @anagnostou_facs-charm_2022 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-anagnostou-2022/evaluation/badges.html" target="_blank">link to evaluation</a>
* **J:** @johnson_cost_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-johnson-2021/evaluation/badges.html" target="_blank">link to evaluation</a>
* **He:** @hernandez_optimal_2015 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-hernandez-2015/evaluation/badges.html" target="_blank">link to evaluation</a>
* **W:** @wood_value_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-wood-2021/evaluation/badges.html" target="_blank">link to evaluation</a>

In this section and below, the criteria for each study are marked as either being fully met (âœ…), partially met (ğŸŸ¡), not met (âŒ) or not applicable (N/A).

**Unique criteria:**

| Item | S | Hu | L | K | A | J | He | W |
| - | - | - | - | - | - | - | - | - | - |
| **Criteria related to how artefacts are shared** |
| Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI) | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| Open licence | âŒ | âœ… | âŒ | âœ… | âœ… | âœ… | âŒ | âŒ |
| **Criteria related to what artefacts are shared** |
| Complete (all relevant artefacts available) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| Artefacts relevant to paper | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Criteria related to the structure and documentation of the artefacts** |
| Documents (a) how code is used (b) how it relates to article (c) software, systems, packages and versions | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ |
| Artefacts are carefully documented and well-structured to the extent that reuse and repurposing is facilitated, adhering to norms and standards | âŒ | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ | âŒ |
| README file with step-by-step instructions to run analysis | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ |
| Dependencies (e.g. package versions) stated | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| Clear how output of analysis corresponds to article | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| **Criteria related to running and reproducing results** |
| Scripts can be successfully executed | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
: {tbl-colwidths="[60, 5, 5, 5, 5, 5, 5, 5, 5]"}

::: {.callout icon=false collapse=false}

## Reflections

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
criteria_dict = {
  'Archive': [1, 0, 7, 0],
  'Licence': [4, 0, 4, 0],
  'Complete': [1, 0, 7, 0],
  'Relevant': [8, 0, 0, 0],
  'Docs use/relate/software': [1, 0, 7, 0],
  'Docs inventory': [1, 0, 7, 0],
  'README step-by-step': [2, 0, 6, 0],
  'Dependencies': [1, 0, 7, 0],
  'Output ~ Article': [1, 0, 7, 0],
  'Execute': [8, 0, 0, 0],
  'Reproduced': [1, 0, 7, 0],
}
criteria_wide = pd.DataFrame(criteria_dict, index=col).T

eval_chart(criteria_wide).show(config={'displayModeBar': False})
```

**Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)**: Fulfillment doesn't impact reproduction as I was able to get everything needed from the remote code repository (GitHub or GitLab). However, if these had been deleted from GitHub, it would have become invaluable.

**Open licence**: This had a big impact on our ability to complete reproductions, as we had to ask authors to add an open licence to their work, to enable us to use it. Gladly, all authors we contacted kindly add these on request. However, it's worth noting that this was a relatively common issue, and one of the most important, since it completely prevents reuse if excluded.

**Complete (all relevant artefacts available)**: This had a really big impact on the reproduction. The main reason for longer times in reproduction was (a) code for scenarios not provided, and (b) code to process results into figures and tables not provided.

**Artefacts relevant to paper**: All met (if not met, this would be a massive hindrance).

**Documents (a) how code is used (b) how it relates to article (c) software, systems, packages and versions** / **Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised** / **Artefacts are carefully documented and well-structured to the extent that reuse and repurposing is facilitated, adhering to norms and standards** / **README file with step-by-step instructions to run analysis**:

* All really handy. In cases where this was not met, this was often related to having quite a busy/cluttered repository which was confusing to navigate, with minimal documentation.
* In @anagnostou_facs-charm_2022, they include a file `CHARM_INFO.md` alongside their README which walks through the input parameters for the model. I didn't need to change any of these for the reproduction, but would imagine this is to be very helpful if someone were to reuse the model.
* Only three studies had any documentation - READMEs for @kim_modelling_2021, @anagnostou_facs-charm_2022 and @johnson_cost_2021. However, focusing on the READMEs, in each of these cases it was great to have these, guiding on how to run the scripts, or on what each folder/file in the repository is - although one didn't have step-by-step instructions, as requested.
* Whilst @anagnostou_facs-charm_2022 did meet criteria, it should be noted that this was a very simple example, just requiring to run one script which quickly reproduces everything! I had been a bit uncertain on it, since the README doesnâ€™t explicitly say how to make the figure, but it does provide instructions that lead you to regenerate the exact model results from the paper, and so I feel that it does provide instructions to reproduce results sufficiently (although would be more complete to include instructions for figure too - so if it werenâ€™t a yes/no decision for badges, I wouldâ€™ve said this was partially met). Ideally, studies would clearly outline how to reproduce results in full.

**Dependencies (e.g. package versions) stated**: Important and impacts analysis, takes a while to work out otherwise.

**Clear how output of analysis corresponds to article**: This is handy - clear link between analysis and items in paper.

**Scripts can be successfully executed**: This is true, though I did allow troubleshooting. Hence, the importance of e.g. environments and scripts being provided in a runnable format (both covered on the [reflections page](reflections.qmd)), since these are the hurdles to successfully executing scripts.

**Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)**: On the [reproduction page](reproduction.qmd), I reflected (where possible) on what I thought the primary reasons were, for cases where I didn't manage to reproduce results despite troubleshooting. It is worth noting however that there were two studies that were quite quick to run, which I reflect about on the [reproduction page](reproduction.qmd).

:::

**Badges:**

The badges are grouped into three categories:

* "Open objects" badges: These badges relate to research artefacts being made openly available.
* "Object review" badges: These badges relate to the research artefacts being reviewed against criteria of the badge issuer.
* "Reproduced" badges: These badges relate to an independent party regenerating the reuslts of the article using the author objects.

| Item | S | Hu | L | K | A | J | He | W |
| - | - | - | - | - | - | - | - | - | - |
| **â€œOpen objectsâ€ badges** |
| **ACM "Artifacts Available"**<br>â€¢ Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI) | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| **NISO "Open Research Objects (ORO)"**<br>â€¢ Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)<br>â€¢ Open licence | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| **NISO "Open Research Objects - All (ORO-A)"**<br>â€¢ Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)<br>â€¢ Open licence<br>â€¢ Complete (all relevant artefacts available) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| **COS "Open Code"**<br>â€¢ Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)<br>â€¢ Open licence<br>â€¢ Documents (a) how code is used (b) how it relates to article (c) software, systems, packages and versions | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| **IEEE "Code Available"**<br>â€¢ Complete (all relevant artefacts available) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| **"Object review" badges** |
| **ACM "Artifacts Evaluated - Functional"**<br>â€¢ Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised<br>â€¢ Artefacts relevant to paper<br>â€¢ Complete (all relevant artefacts available)<br>â€¢ Scripts can be successfully executed | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| **ACM "Artifacts Evaluated - Reusable"**<br>â€¢ Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised<br>â€¢ Artefacts relevant to paper<br>â€¢ Complete (all relevant artefacts available)<br>â€¢ Scripts can be successfully executed<br>â€¢ Artefacts are carefully documented and well-structured to the extent that reuse and repurposing is facilitated, adhering to norms and standards | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| **IEEE "Code Reviewed"**<br>â€¢ Complete (all relevant artefacts available)<br>â€¢ Scripts can be successfully executed | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| **"Reproduced" badges** |
| **ACM "Results Reproduced"**<br>â€¢ Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| **NISO "Results Reproduced (ROR-R)"**<br>â€¢ Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| **IEEE "Code Reproducible"**<br>â€¢ Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| **Psychological Science "Computational Reproducibility"**<br>â€¢ Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)<br>â€¢ README file with step-by-step instructions to run analysis<br>â€¢ Dependencies (e.g. package versions) stated<br>â€¢ Clear how output of analysis corresponds to article | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
: {tbl-colwidths="[60, 5, 5, 5, 5, 5, 5, 5, 5]"}

::: {.callout icon=false collapse=false}

## Reflections

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
badges_dict = {
  'ACM â€œArtifacts<br>Availableâ€': [1, 0, 7, 0],
  'NISO â€œOpen Research<br>Objects (ORO)â€': [1, 0, 7, 0],
  'NISO â€œOpen Research<br>Objects - All (ORO-A)â€': [0, 0, 8, 0],
  'COS â€œOpen Codeâ€': [1, 0, 7, 0],
  'IEEE â€œCode Availableâ€': [1, 0, 7, 0],
  'ACM â€œArtifacts<br>Evaluated - Functionalâ€': [0, 0, 8, 0],
  'ACM â€œArtifacts<br>Evaluated - Reusableâ€': [0, 0, 8, 0],
  'IEEE â€œCode Reviewedâ€': [1, 0, 7, 0],
  'ACM â€œResults Reproducedâ€': [1, 0, 7, 0],
  'NISO â€œResults<br>Reproduced (ROR-R)â€': [1, 0, 7, 0],
  'IEEE â€œCode Reproducibleâ€': [1, 0, 7, 0],
  'Psychological Science<br>â€œComputational Reproducibilityâ€': [0, 0, 8, 0],
}
badges_wide = pd.DataFrame(badges_dict, index=col).T

eval_chart(badges_wide).show(config={'displayModeBar': False})
```

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
badges_dict = {
  'open_acm': [1, 0, 7, 0],
  'open_niso': [1, 0, 7, 0],
  'open_niso_all': [0, 0, 8, 0],
  'open_cos': [1, 0, 7, 0],
  'open_ieee': [1, 0, 7, 0],
  'review_acm_functional': [0, 0, 8, 0],
  'review_acm_reusable': [0, 0, 8, 0],
  'review_ieee': [1, 0, 7, 0],
  'reproduce_acm': [1, 0, 7, 0],
  'reproduce_niso': [1, 0, 7, 0],
  'reproduce_ieee': [1, 0, 7, 0],
  'reproduce_psy': [0, 0, 8, 0],
}
badges_wide = pd.DataFrame(badges_dict, index=col).T

# Full badge names
badge_names = {
    # Open objects
    'open_acm': 'ACM â€œArtifacts<br>Availableâ€',
    'open_niso': 'NISO â€œOpen Research<br>Objects (ORO)â€',
    'open_niso_all': 'NISO â€œOpen Research<br>Objects - All (ORO-A)â€',
    'open_cos': 'COS â€œOpen Codeâ€',
    'open_ieee': 'IEEE â€œCode Availableâ€',
    # Object review
    'review_acm_functional': 'ACM â€œArtifacts<br>Evaluated - Functionalâ€',
    'review_acm_reusable': 'ACM â€œArtifacts<br>Evaluated - Reusableâ€',
    'review_ieee': 'IEEE â€œCode Reviewedâ€',
    # Results reproduced
    'reproduce_acm': 'ACM â€œResults Reproducedâ€',
    'reproduce_niso': 'NISO â€œResults<br>Reproduced (ROR-R)â€',
    'reproduce_ieee': 'IEEE â€œCode Reproducibleâ€',
    'reproduce_psy': 'Psychological Science<br>â€œComputational Reproducibilityâ€'
}

# Replace with full badge names
badges_fig = badges_wide.copy()
badges_fig.index = badges_fig.index.map(badge_names)

# Create figure
eval_chart(badges_fig).show(config={'displayModeBar': False})
```

Only one study had **permanent archive (with persistent identifier)**, hence one being awarded *NISO â€œOpen Research Objects (ORO)â€*, *ACM â€œArtifacts Availableâ€* and *COS â€œOpen Codeâ€*. However, that study did not receive *NISO â€œOpen Research Objects - All (ORO-A)â€* as artefacts were not complete.

A **complete set of materials** was required by *IEEE â€œCode Availableâ€* and *IEEE â€œCode Reviewedâ€* - but this was only met by one study, as studies commonly did not include code for scenarios or creation of figures and tables. It was also required by *ACM â€œArtifacts Evaluated - Functional and Reusableâ€* badges, but since that one study didn't meet their **documentation** requirements, none were awarded those badges.

Three badges had one criteria: **reproduction of results** - but with assumptions - and only one study met this (with 100% reproduction PLUS meeting the assumptions).
:::

::: {.callout icon=false collapse=false}

## Badges summary tables for the article

```{python}
# Criteria and their definitions
criteria = {
    'archive': 'Artefacts are archived in a repository that is: (a) public (b) guarantees persistence (c) gives a unique identifier (e.g. DOI)',
    'licence': 'Open licence',
    'complete': 'Complete (all relevant artefacts available)',
    'docs1': 'Documents (a) how code is used (b) how it relates to article (c) software, systems, packages and versions',
    'docs2': 'Documents (a) inventory of artefacts (b) sufficient description for artefacts to be exercised',
    'relevant': 'Artefacts relevant to paper',
    'execute': 'Scripts can be successfully executed',
    'careful': 'Artefacts are carefully documented and well-structured to the extent that reuse and repurposing is facilitated, adhering to norms and standards',
    'reproduce': 'Reproduced results (assuming (a) acceptably similar (b) reasonable time frame (c) only minor troubleshooting)',
    'readme': 'README file with step-by-step instructions to run analysis',
    'dependencies': 'Dependencies (e.g. package versions) stated',
    'correspond': 'Clear how output of analysis corresponds to article'
}

# Criteria required by each badge
badges = {
    # Open objects
    'open_acm': ['archive'],
    'open_niso': ['archive', 'licence'],
    'open_niso_all': ['archive', 'licence', 'complete'],
    'open_cos': ['archive', 'licence', 'docs1'],
    'open_ieee': ['complete'],
    # Object review
    'review_acm_functional': ['docs2', 'relevant', 'complete', 'execute'],
    'review_acm_reusable': ['docs2', 'relevant', 'complete', 'execute', 'careful'],
    'review_ieee': ['complete', 'execute'],
    # Results reproduced
    'reproduce_acm': ['reproduce'],
    'reproduce_niso': ['reproduce'],
    'reproduce_ieee': ['reproduce'],
    'reproduce_psy': ['reproduce', 'readme', 'dependencies', 'correspond']
}

# Find proportion fully met, then use to make "met criteria" column
badges_table = badges_wide.copy()
badges_table['percent'] = round((badges_table['fully'] / 8)*100, 2)
badges_table['met_criteria'] = badges_table['fully'].astype(str) + '/8 (' + badges_table['percent'].astype(str) + '%)'

# Create a column with criteria for each badge
badges_table['criteria'] = badges_table.index.map(
    lambda badge: '<br>'.join(
        f"â€¢ {criteria[crit]}" for crit in badges[badge]
    )
)

# Replace with full badge names, then filter and relabel columns
badges_table.index = badges_table.index.map(badge_names)
badges_table.index.name = 'Badge'
badges_table = badges_table[['criteria', 'met_criteria']].rename(
  columns={'criteria': 'Criteria', 'met_criteria': 'Studies that met criteria'}
)

# Save to CSV and show
badges_table.to_csv('../data/badges_table.csv')
badges_table
```

:::

## STARS framework

Key:

* **S:** @shoaib_simulation_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-shoaib-2022/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **Hu:** @huang_optimizing_2019  - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **L:** @lim_staff_2020 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-lim-2020/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **K:** @kim_modelling_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-kim-2021/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **A:** @anagnostou_facs-charm_2022 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-anagnostou-2022/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **J:** @johnson_cost_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-johnson-2021/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **He:** @hernandez_optimal_2015 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-hernandez-2015/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **W:** @wood_value_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-wood-2021/evaluation/artefacts.html" target="_blank">link to evaluation</a>

| Item | S | Hu | L | K | A | J | He | W |
| - | - | - | - | - | - | - | - | - | - |
| **Essential components** |
| **Open licence**<br>Free and open-source software (FOSS) licence (e.g. MIT, GNU Public Licence (GPL)) | âŒ | âœ… | âŒ | âœ… | âœ… | âœ… | âŒ | âŒ |
| **Dependency management**<br>Specify software libraries, version numbers and sources (e.g. dependency management tools like virtualenv, conda, poetry) | âŒ | âŒ | âŒ | ğŸŸ¡ | âœ… | ğŸŸ¡ | âŒ | âŒ |
| **FOSS model**<br>Coded in FOSS language (e.g. R, Julia, Python) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Minimum documentation**<br>Minimal instructions (e.g. in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments | âŒ | âŒ | âŒ | âœ… | âœ… | ğŸŸ¡ | âŒ | âŒ |
| **ORCID**<br>ORCID for each study author | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| **Citation information**<br>Instructions on how to cite the research artefact (e.g. CITATION.cff file) | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ |
| **Remote code repository**<br>Code available in a remote code repository (e.g. GitHub, GitLab, BitBucket) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Open science archive**<br>Code stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g. Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net)) | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| **Optional components** |
| **Enhanced documentation**<br>Open and high quality documentation on how the model is implemented and works  (e.g. via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:<br>â€¢ Plain english summary of project and model<br>â€¢ Clarifying licence<br>â€¢ Citation instructions<br>â€¢ Contribution instructions<br>â€¢ Model installation instructions<br>â€¢ Structured code walk through of model<br>â€¢ Documentation of modelling cycle using TRACE<br>â€¢ Annotated simulation reporting guidelines<br>â€¢ Clear description of model validation including its intended purpose | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| **Documentation hosting**<br>Host documentation (e.g. with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| **Online coding environment**<br>Provide an online environment where users can run and change code (e.g. BinderHub, Google Colaboratory, Deepnote) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| **Model interface**<br>Provide web application interface to the model so it is accessible to less technical simulation users | âŒ | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| **Web app hosting**<br>Host web app online (e.g. Streamlit Community Cloud, ShinyApps hosting) | âŒ | âœ… | âŒ | âŒ | ğŸŸ¡ | âŒ | âŒ | âŒ |
: {tbl-colwidths="[60, 5, 5, 5, 5, 5, 5, 5, 5]"}

::: {.callout icon=false collapse=false}

## Reflections

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
stars_dict = {
  'Open licence': [4, 0, 4, 0],
  'Dependency management': [1, 2, 5, 0],
  'FOSS model': [8, 0, 0, 0],
  'Minimum documentation': [2, 1, 5, 0],
  'ORCID': [0, 0, 8, 0],
  'Citation information': [2, 0, 6, 0],
  'Remote code repository': [8, 0, 0, 0],
  'Open science archive': [1, 0, 7, 0],
  'Enhanced documentation': [0, 0, 8, 0],
  'Documentation hosting': [0, 0, 8, 0],
  'Online coding environment': [0, 0, 8, 0],
  'Model interface': [2, 0, 6, 0],
  'Web app hosting': [1, 1, 6, 0],
}
stars_wide = pd.DataFrame(stars_dict, index=col).T

# Create figure
stars_chart = eval_chart(stars_wide)

# Display figure
stars_chart.show(config={'displayModeBar': False})

# Save figure as 300DPI static image for use in article
pio.write_image(stars_chart, '../images/stars_criteria.png',
                width=2.5*300, height=1.8*300, scale=1)
```

These topics were covered in the badge criteria reflections: **open licence**, **minimum documentation**, and **open science archive**.

**Dependency management**: This was pretty uncommon, and often took some troubleshooting at the start, to figure out which packages were needed, and certain versions.

**FOSS model**: All met as requirement of our reproduction.

**ORCID** and **citation information**: Doesn't impact reproduction in this case - but:

* We do go to these from having found an article. I was choosing repositories that I had found from papers, so I already at least knew who the paper authors were.
* In all cases, I emailed the authors, which requires finding contact information (generally via paper, sometimes from googling them to find new emails).
* Any attempted citation of the repository itself would've necessarily been correct, depending on whether the author list would be the same as in the paper, if you relied on the paper without citation information.

**Remote code repository**: All met, most common way to share code.

**Enhanced documentation**: Only three studies had any documentation, and neither met these extensive requirements. I anticipate - if any had met this - it would've made the reproduction very quick and easy!

**Documentation hosting**: Not applicable, given only basic documentation.

**Online coding environment**: None provided. I always intended to run on my own machine, so this might not have had much bearing in my case if provided, but would moreso for people who perhaps didn't have Python or R installed, and hopefully would have bypassed environment troubleshooting issues.

**Model interface**: Two studies had applications, although in both cases, these weren't "outcomes" in scope of reproduction, nor did they produce them.

**Web app hosting**: This was quite important. Both apps had been hosted, but one was hosted with a site that is no longer operational. In both cases, the app wasn't in "scope" although I did still view it and look into it for one as it was hosted and so could very easily - but for the other, I didn't view it, as I didn't go through the steps of running it locally, since it wasn't the focus.

:::

## Timings

* @shoaib_simulation_2021 - 30m
* @huang_optimizing_2019 - 17m
* @lim_staff_2020 - 18m
* @kim_modelling_2021 - 18m
* @anagnostou_facs-charm_2022 - 19m
* @johnson_cost_2021 - 20m
* @hernandez_optimal_2015 - 13m
* @wood_value_2021 - 14m

Revisiting and redoing evaluation for badges was only 2-3 minutes per study, and have just stuck with the original evaluation timings above.

::: {.callout icon=false collapse=false}

## Reflections

No particular comments, don't think we learn much from the timings here.

:::

## Badge sources

**National Information Standards Organisation (NISO)** (@niso_reproducibility_badging_and_definitions_working_group_reproducibility_2021)

* "Open Research Objects (ORO)"
* "Open Research Objects - All (ORO-A)"
* "Results Reproduced (ROR-R)"

**Association for Computing Machinery (ACM)** (@association_for_computing_machinery_acm_artifact_2020)

* "Artifacts Available"
* "Artifacts Evaluated - Functional"
* "Artifacts Evaluated - Resuable"
* "Results Reproduced"

**Center for Open Science (COS)** (@blohowiak_badges_2023)

* "Open Code"

**Institute of Electrical and Electronics Engineers (IEEE)** (@institute_of_electrical_and_electronics_engineers_ieee_about_2024)

* "Code Available"
* "Code Reviewed"
* "Code Reproducible"

**Psychological Science** (@hardwicke_transparency_2024 and @association_for_psychological_science_aps_psychological_2024)

* "Computational Reproducibility"

## References