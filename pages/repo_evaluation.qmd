---
title: "Evaluation of the repository"
echo: False
bibliography: ../references.bib
---

The code and related research artefacts in the original code repositories were evaluated against:

* The criteria of badges related to reproducibility from various organisations and journals.
* Recommendations from the STARS framework for the sharing of code and associated materials from discrete-event simulation models (@monks_towards_2024).

Between each journal badge, there was often alot of overlap in criteria. Hence, a list of unique criteria was produced. The repositories are evaluated against this criteria, and then depending on which criteria they met, against the badges themselves.

*Caveat: Please note that these criteria are based on available information about each badge online, and that we have likely differences in our procedure (e.g. allowed troubleshooting for execution and reproduction, not under tight time pressure to complete). Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals.*

Consider: **What criteria are people struggling to meet from the guidelines?**

<!-- TODO: Once complete, carefully go over again and check everything is correct -->

## Summary

<!-- TODO: Add Johnson et al. 2021 -->

**Unique badge criteria:**

```{python}
import pandas as pd
from functions import eval_chart

# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
criteria_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [3, 0, 9, 0],
  'Huang et al. 2019 (3/8)': [3, 0, 9, 0],
  'Lim et al. 2020 (9/9)': [3, 0, 9, 0],
  'Kim et al. 2021 (10/10)': [5, 0, 7, 0],
  'Anagnostou et al. 2022 (1/1)': [10, 0, 2, 0],
  'Johnson et al. 2021 (X/X)': [0, 0, 0, 1],
  'Hernandez et al. 2015 (1/8)': [3, 0, 9, 0],
  'Wood et al. 2021 (5/5)': [4, 0, 8, 0]
}
criteria_wide = pd.DataFrame(criteria_dict, index=col).T

eval_chart(criteria_wide)
```

::: {.callout icon=false collapse=true}

## Reflections

The studies meeting 33.3%, 41.7% and 83.3% of criteria were all fully reproduced.

The remaining four studies met 25% of criteria, and ranged from 12.5% to 100% reproduced.

However, I think it is more meaningful to actually look at what criteria were and were not met.

```{python}
import plotly.express as px

# Percentage of items reproduced, and percentage of badge criteria met
df = pd.DataFrame({
  'Shoaib and Ramamohan 2021': [94, 25],
  'Huang et al. 2019': [37.5, 25],
  'Lim et al. 2020': [100, 25],
  'Kim et al. 2021': [100, 41.7],
  'Anagnostou et al. 2022': [100, 83.3],
  #'Johnson et al. 2021': [],
  'Hernandez et al. 2015': [12.5, 25],
  'Wood et al. 2021': [100, 33.3]
}).T.reset_index()
df.columns = ['study', 'reproduce', 'criteria']

px.scatter(df, x='criteria', y='reproduce',
           hover_data={'study': True})
```

:::

**Badges:**

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
badge_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [3, 0, 9, 0],
  'Huang et al. 2019 (3/8)': [0, 0, 12, 0],
  'Lim et al. 2020 (9/9)': [3, 0, 9, 0],
  'Kim et al. 2021 (10/10)': [3, 0, 9, 0],
  'Anagnostou et al. 2022 (1/1)': [5, 0, 7, 0],
  'Johnson et al. 2021 (X/X)': [0, 0, 0, 1],
  'Hernandez et al. 2015 (1/8)': [0, 0, 12, 0],
  'Wood et al. 2021 (5/5)': [5, 0, 7, 0]
}
badge_wide = pd.DataFrame(badge_dict, index=col).T

eval_chart(badge_wide)
```

::: {.callout icon=false collapse=true}

## Reflections

Not certain how meaningful these numbers are, as we have imbalanced numbers of different types of badge, and meeting certain popular criteria will quite weight what is met v.s. not.

Feel that looking at the criteria met is a bit more meaningful? And then specific examples of how that translates into badges - e.g. none meeting ACM "Artifacts Evaluated - Functional" as requires xyz and these are commonly not met.

```{python}
import plotly.express as px

# Percentage of items reproduced, and percentage of badges awarded
df = pd.DataFrame({
  'Shoaib and Ramamohan 2021': [94, 25],
  'Huang et al. 2019': [37.5, 0],
  'Lim et al. 2020': [100, 25],
  'Kim et al. 2021': [100, 25],
  'Anagnostou et al. 2022': [100, 41.7],
  #'Johnson et al. 2021': [],
  'Hernandez et al. 2015': [12.5, 0],
  'Wood et al. 2021': [100, 41.7]
}).T.reset_index()
df.columns = ['study', 'reproduce', 'badges']

px.scatter(df, x='badges', y='reproduce',
           hover_data={'study': True})
```

:::

**Essential components of STARS framework:**

```{python}
# Create dataframe of results
col = ['fully', 'partially', 'not', 'na']
essential_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [2, 0, 6, 0],
  'Huang et al. 2019 (3/8)': [2, 0, 6, 0],
  'Lim et al. 2020 (9/9)': [2, 0, 6, 0],
  'Kim et al. 2021 (10/10)': [4, 1, 3, 0],
  'Anagnostou et al. 2022 (1/1)': [7, 0, 1, 0],
  'Johnson et al. 2021 (X/X)': [0, 0, 0, 1],
  'Hernandez et al. 2015 (1/8)': [2, 0, 6, 0],
  'Wood et al. 2021 (5/5)': [2, 0, 6, 0]
}
essential_wide = pd.DataFrame(essential_dict, index=col).T

eval_chart(essential_wide)
```

**Optional components of STARS framework:**

```{python}
# Create dataframe of results
optional_dict = {
  'Shoaib and Ramamohan<br>2021 (16/17)': [0, 0, 5, 0],
  'Huang et al. 2019 (3/8)': [2, 0, 3, 0],
  'Lim et al. 2020 (9/9)': [0, 0, 5, 0],
  'Kim et al. 2021 (10/10)': [0, 0, 5, 0],
  'Anagnostou et al. 2022 (1/1)': [1, 1, 3, 0],
  'Johnson et al. 2021 (X/X)': [0, 0, 0, 1],
  'Hernandez et al. 2015 (1/8)': [0, 0, 5, 0],
  'Wood et al. 2021 (5/5)': [0, 0, 5, 0]
}
optional_wide = pd.DataFrame(optional_dict, index=col).T

eval_chart(optional_wide)
```

## Journal badges

Key:

* **S:** @shoaib_simulation_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-shoaib-2022/evaluation/badges.html" target="_blank">link to evaluation</a>
* **Hu:** @huang_optimizing_2019  - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/badges.html" target="_blank">link to evaluation</a>
* **L:** @lim_staff_2020 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-lim-2020/evaluation/badges.html" target="_blank">link to evaluation</a>
* **K:** @kim_modelling_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-kim-2021/evaluation/badges.html" target="_blank">link to evaluation</a>
* **A:** @anagnostou_facs-charm_2022 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-anagnostou-2022/evaluation/badges.html" target="_blank">link to evaluation</a>
* **J:** @johnson_cost_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-johnson-2021/evaluation/badges.html" target="_blank">link to evaluation</a>
* **He:** @hernandez_optimal_2015 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-hernandez-2015/evaluation/badges.html" target="_blank">link to evaluation</a>
* **W:** @wood_value_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-wood-2021/evaluation/badges.html" target="_blank">link to evaluation</a>

<!-- TODO: Add Johnson et al. 2021 -->

In this section and below, the criteria for each study are marked as either being fully met (✅), partially met (🟡), not met (❌) or not applicable (N/A).

**Unique criteria:**

| Item | S | Hu | L | K | A | J | He | W |
| - | - | - | - | - | - | - | - | - | - |
| **Criteria related to how artefacts are shared** |
| Stored in a permanent archive that is publicly and openly accessible | ❌ | ❌ | ❌ | ❌ | ✅ | <!--TODO--> | ❌ | ❌ |
| Has a persistent identifier | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| Includes an open license | ❌ | ✅ | ❌ | ✅ | ✅ |<!--TODO--> | ❌ | ❌ |
| **Criteria related to what artefacts are shared** |
| Artefacts are relevant to and contribute to the article’s results | ✅ | ✅ | ✅ | ✅ | ✅ |<!--TODO--> | ✅ | ✅ |
| Complete set of materials shared (as would be needed to fully reproduce article) | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ✅ |
| **Criteria related to the structure and documentation of the artefacts** |
| Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community) | ❌ | ❌ | ❌ | ✅ | ✅ |<!--TODO--> | ✅ | ❌ |
| Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions) | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose) | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| **Criteria related to running and reproducing results** |
| Scripts can be successfully executed | ✅ | ✅ | ✅ | ✅ | ✅ |<!--TODO--> | ✅ | ✅ |
| Independent party regenerated results using the authors research artefacts | ✅ | ❌ | ✅ | ✅ | ✅ |<!--TODO--> | ❌ | ✅ |
| Reproduced within approximately one hour (excluding compute time) | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ |
: {tbl-colwidths="[60, 5, 5, 5, 5, 5, 5, 5, 5]"}

\ 

**Badges:**

The badges are grouped into three categories:

* "Open objects" badges: These badges relate to research artefacts being made openly available.
* "Object review" badges: These badges relate to the research artefacts being reviewed against criteria of the badge issuer.
* "Reproduced" badges: These badges relate to an independent party regenerating the reuslts of the article using the author objects.

<!-- TODO: Add Johnson et al. 2021, Hernandez et al. 2015 and Wood et al. 2021 -->

| Item | S | Hu | L | K | A | J | He | W |
| - | - | - | - | - | - | - | - | - | - |
| **“Open objects” badges** |
| **NISO "Open Research Objects (ORO)"**<br>• Stored in a permanent archive that is publicly and openly accessible<br>• Has a persistent identifier<br>• Includes an open license | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| **NISO "Open Research Objects - All (ORO-A)"**<br>• Stored in a permanent archive that is publicly and openly accessible<br>• Has a persistent identifier<br>• Includes an open license<br>• Complete set of materials shared (as would be needed to fully reproduce article) | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **ACM "Artifacts Available"**<br>• Stored in a permanent archive that is publicly and openly accessible<br>• Has a persistent identifier | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| **COS "Open Code"**<br>• Stored in a permanent archive that is publicly and openly accessible<br>• Has a persistent identifier<br>• Includes an open license<br>• Complete set of materials shared (as would be needed to fully reproduce article)<br>• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions) | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **IEEE "Code Available"**<br>• Complete set of materials shared (as would be needed to fully reproduce article) | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ✅ |
| **"Object review" badges** |
| **ACM "Artifacts Evaluated - Functional"**<br>• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)<br>• Artefacts are relevant to and contribute to the article’s results<br>• Complete set of materials shared (as would be needed to fully reproduce article)<br>• Scripts can be successfully executed | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **ACM "Artifacts Evaluated - Reusable"**<br>• Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)<br>• Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)<br>• Artefacts are relevant to and contribute to the article's results<br>• Complete set of materials shared (as would be needed to fully reproduce article)<br>• Scripts can be successfully executed<br>• Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community) | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **IEEE "Code Reviewed"**<br>• Complete set of materials shared (as would be needed to fully reproduce article)<br>• Scripts can be successfully executed | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ✅ |
| **"Reproduced" badges** |
| **NISO "Results Reproduced (ROR-R)"**<br>• Independent party regenerated results using the authors research artefacts | ✅ | ❌ | ✅ | ✅ | ✅ |<!--TODO--> | ❌ | ✅ |
| **ACM "Results Reproduced"**<br>• Independent party regenerated results using the authors research artefacts | ✅ | ❌ | ✅ | ✅ | ✅ |<!--TODO--> | ❌ | ✅ |
| **IEEE "Code Reproducible"**<br>• Independent party regenerated results using the authors research artefacts | ✅ | ❌ | ✅ | ✅ | ✅ |<!--TODO--> | ❌ | ✅ |
| **Psychological Science "Computational Reproducibility"**<br>• Independent party regenerated results using the authors research artefacts<br>• Reproduced within approximately one hour (excluding compute time)<br>• Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)<br>• Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
: {tbl-colwidths="[60, 5, 5, 5, 5, 5, 5, 5, 5]"}

## STARS framework

Key:

* **S:** @shoaib_simulation_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-shoaib-2022/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **Hu:** @huang_optimizing_2019  - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **L:** @lim_staff_2020 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-lim-2020/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **K:** @kim_modelling_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-kim-2021/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **A:** @anagnostou_facs-charm_2022 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-anagnostou-2022/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **J:** @johnson_cost_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-johnson-2021/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **He:** @hernandez_optimal_2015 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-hernandez-2015/evaluation/artefacts.html" target="_blank">link to evaluation</a>
* **W:** @wood_value_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-wood-2021/evaluation/artefacts.html" target="_blank">link to evaluation</a>

<!-- TODO: Add Johnson et al. 2021 -->

| Item | S | Hu | L | K | A | J | He | W |
| - | - | - | - | - | - | - | - | - | - |
| **Essential components** |
| **Open license**<br>Free and open-source software (FOSS) license (e.g. MIT, GNU Public License (GPL)) | ❌ | ✅ | ❌ | ✅ | ✅ |<!--TODO--> | ❌ | ❌ |
| **Dependency management**<br>Specify software libraries, version numbers and sources (e.g. dependency management tools like virtualenv, conda, poetry) | ❌ | ❌ | ❌ | 🟡 | ✅ |<!--TODO--> | ❌ | ❌ |
| **FOSS model**<br>Coded in FOSS language (e.g. R, Julia, Python) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| **Minimum documentation**<br>Minimal instructions (e.g. in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments | ❌ | ❌ | ❌ | ✅ | ✅ |<!--TODO--> | ❌ | ❌ |
| **ORCID**<br>ORCID for each study author | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **Citation information**<br>Instructions on how to cite the research artefact (e.g. CITATION.cff file) | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| **Remote code repository**<br>Code available in a remote code repository (e.g. GitHub, GitLab, BitBucket) | ✅ | ✅ | ✅ | ✅ | ✅ |<!--TODO--> | ✅ | ✅ |
| **Open science archive**<br>Code stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g. Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net)) | ❌ | ❌ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| **Optional components** |
| **Enhanced documentation**<br>Open and high quality documentation on how the model is implemented and works  (e.g. via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:<br>• Plain english summary of project and model<br>• Clarifying license<br>• Citation instructions<br>• Contribution instructions<br>• Model installation instructions<br>• Structured code walk through of model<br>• Documentation of modelling cycle using TRACE<br>• Annotated simulation reporting guidelines<br>• Clear description of model validation including its intended purpose | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **Documentation hosting**<br>Host documentation (e.g. with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub) | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **Online coding environment**<br>Provide an online environment where users can run and change code (e.g. BinderHub, Google Colaboratory, Deepnote) | ❌ | ❌ | ❌ | ❌ | ❌ |<!--TODO--> | ❌ | ❌ |
| **Model interface**<br>Provide web application interface to the model so it is accessible to less technical simulation users | ❌ | ✅ | ❌ | ❌ | ✅ |<!--TODO--> | ❌ | ❌ |
| **Web app hosting**<br>Host web app online (e.g. Streamlit Community Cloud, ShinyApps hosting) | ❌ | ✅ | ❌ | ❌ | 🟡 |<!--TODO--> | ❌ | ❌ |
: {tbl-colwidths="[60, 5, 5, 5, 5, 5, 5, 5, 5]"}

## Timings

<!-- TODO: Add Johnson et al. 2021, Hernandez et al. 2015 and Wood et al. 2021 -->

* @shoaib_simulation_2021 - 30m
* @huang_optimizing_2019 - 17m
* @lim_staff_2020 - 18m
* @kim_modelling_2021 - 18m
* @anagnostou_facs-charm_2022 - 19m
* @johnson_cost_2021 - <!--TODO-->
* @hernandez_optimal_2015 - 13m
* @wood_value_2021 - 14m

## Badge sources

**National Information Standards Organisation (NISO)** (@niso_reproducibility_badging_and_definitions_working_group_reproducibility_2021)

* "Open Research Objects (ORO)"
* "Open Research Objects - All (ORO-A)"
* "Results Reproduced (ROR-R)"

**Association for Computing Machinery (ACM)** (@association_for_computing_machinery_acm_artifact_2020)

* "Artifacts Available"
* "Artifacts Evaluated - Functional"
* "Artifacts Evaluated - Resuable"
* "Results Reproduced"

**Center for Open Science (COS)** (@blohowiak_badges_2023)

* "Open Code"

**Institute of Electrical and Electronics Engineers (IEEE)** (@institute_of_electrical_and_electronics_engineers_ieee_about_nodate)

* "Code Available"
* "Code Reviewed"
* "Code Reproducible"

**Psychological Science** (@hardwicke_transparency_2023 and @association_for_psychological_science_aps_psychological_2023)

* "Computational Reproducibility"

## References