---
title: "Results discussion"
bibliography: ../references.bib
---

## Reflections

Reflections from the reproduction and evaluation are broadly grouped into two categories:

* **Analysis pipeline**
* **Model structure and functionality**

A few additional general reflections are under **other**.

### Analysis pipeline

![Analysis pipeline diagram](../images/pipeline.png){.lightbox}

These are important for reproduction. If need to troubleshoot, then model structure and functionality are particularly useful (e.g. writing code for results is impacted by understandable output)...

Links to evaluation:

* Packages - STARS: dependency management
* Versions - Badges: documentation. STRESS: software (handy when mentioned in article but would ideally be in repository)
* Scenario - ISPOR and STRESS: scenario (may describe in paper but if code not provided can still be tricky and time consuming to implement. Implementation of scenarios impacted by model structure e.g. programmatic, don't hard code, sufficient comments)
* Required parameters - ISPOR and STRESS: input parameters (this was VERY important to be reported as it allows us to check code parameters are correct. When missing from paper, not possible to check)

### Model structure and functionality

TODO: TURN INTO DIAGRAM (like have done for analysis pipeline)

* Runnable
* Programmatic
    * E.g. classes... but WITH inputs... as had example using classes but all hard coded so not programmatic so using classes isn't magic fix, just a way of doing, so maybe don't mention
* Don't hard code
* Relative file paths
* Avoid duplication
* Sufficient comments
* Save outputs to file
* Understandable output
    * Data dictionaries (relevant to inputs and outputs).
* Avoid large file sizes
* Excessive file number
* Instructions on how to run
* Quicker model
    * State memory use
    * State run time

These are important for reuse - but also pipeline stuff! As they (a) make reproduction easier and (b) are important when troubleshooting.

Links to evaluation:

* Data dictionaries - Badges: careful documentation
* Instructions on how to run - STARS and Badges: documentation (sufficient, minimal and step by step)
* Run time - STRESS: run time (found really important for this to be mentioned (particularly relevant when considering reuse too) and should be in article AND repository)

### Other

These didn't fit into the two categories above:

* **Open license**
    * On ALL repositories required
* **Unsupported versions**

## Headline findings

Whilst above captures the reflections, I've also pulled out a few **headline** findings.

The four most important things:

1. **Using correct parameters in code**
2. **Including code for all scenarios**
3. **Include code to create tables, figures and in-text results**
4. **Open license**

Why were these the most fundamental?

**Using the code**. Open license  (4) is fundamentally important - can't reuse code without it. In several studies, had to ask authors to add license (all kindly agreed).

**Timing**. Timings varied alot, from 2h11 to 28h14

* Six slower studies (>12h)
    * I think main reasons for time were: troubleshooting wrong parameters (1), writing code for scenarios (2), and creating tables/figures/in-text results (3) (including appropriate transformations, which columns to use, and so on).
    * Scope could be larger (5 to 17 items)
* Two quicker studies (<4h)
    * Both required very little troubleshooting (e.g. only small amount of coding)
    * Scope was also simple (1 or 5 items)

**Reproduction success**. I think main reasons were having wrong parameters (1), not having code for scenarios (2) and not having code for results (3).

Links to evaluation:

* Badges and STARS: open license
* Badges: complete set of materials

## Relationship between reproduction and evaluation

Regarding relationship between reproduction and evaluation, in terms of article reporting, these feel like the key things that impacted reproduction...

### Article

Really important to be in article:

* STRESS: Run time
* ISPOR and STRESS: Input parameters

Handy to be in article but should ideally be in code:

* STRESS: software
* Scenario code ISPOR and STRESS

### Repository

* STARS and badges: documentation (sufficient, minimal, step by step, careful)
    * Instructions on how to run
    * Understandable output e.g. data dictionaries
    * Versions
* STARS: dependency management

## Other things I wanted to mention

Different checklists have different focuses... STRESS-DES focuses on what and how modelling work is performed... derived from ISPOR-SDM checklist is more related to validity type stuff...

Whether or not a study was fully reproduced or not doesn't necessarily mean it was "good" or "bad". Did lots of troubleshooting for some before fully reproduced. And sometimes, couldn't reproduce for simple reasons, like likely parameter being wrong impacting lal results.

Badges not awarded for not having combination of things required e.g. archive + docs, or complete + docs. Many got basic one of reproducing results, but I allowed troubleshooting - yet they may not - particularly HOURS of troubleshooting.

Article evaluation sometimes took quite a while... limitation that may not be perfect... those that used guidelines did better... emphasis importance of (a) using guidelines and (b) attaching completed guidelines

Meeting STARS optional wasn't helpful for reproduction (but would be for reuse)
