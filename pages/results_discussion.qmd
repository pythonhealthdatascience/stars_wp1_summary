---
title: "Results discussion"
bibliography: ../references.bib
---

::: {.callout-note}

## What is on this page?

* Reflections from the reproduction and evaluation, broadly grouped into two categories
* Headline findings
* Relationship between reproduction and evaluation
* Some other things I want to mention

:::

## Reflections

### Analysis pipeline

These are important for reproduction. If need to troubleshoot, then model structure and functionality are particularly useful - e.g.:

* Writing code for scenarios is impacted by: programmatic, don't hard code, sufficient comments
* Writing code for results is impacted by: understandable output

"Reflections" in this group:

!["Reflections" in the group "Analysis Pipeline"](../images/pipeline.png){.lightbox}

::: {.callout-note collapse="true"}

## Links to evaluation

Hence, these are included in relationship between reproduction and evaluation below

* Packages - STARS: dependency management
* Versions - Badges: documentation, and STRESS: software
* Scenario - ISPOR and STRESS: scenario, and Badges: complete set of materials
* Required parameters - ISPOR and STRESS: input parameters

:::

### Model structure and functionality

These are important for reuse - but also pipeline stuff! As they (a) make reproduction easier and (b) are important when troubleshooting.

Note: Data dictionaries are relevant to inputs and outputs.

!["Reflections" in the group "Model structure and functionality"](../images/structure_function.png){.lightbox}

::: {.callout-note collapse="true"}

## Links to evaluation

Hence, these are included in relationship between reproduction and evaluation below

* Data dictionaries - Badges: careful documentation
* Instructions on how to run - STARS and Badges: documentation (sufficient, minimal and step by step)
* Run time - STRESS: run time

:::

### Other

These didn't fit into the two categories above:

* **Open license**
    * On ALL repositories required
* **Unsupported versions**

::: {.callout-note collapse="true"}

## Links to evaluation

Hence, these are included in relationship between reproduction and evaluation below

* Badges and STARS: open license

:::

## Headline findings

Whilst above captures the reflections, I've also pulled out a few **headline** findings.

The four most important things:

1. **Using correct parameters in code**
2. **Including code for all scenarios**
3. **Include code to create tables, figures and in-text results**
4. **Open license**

Why were these the most fundamental?

> **Using the code**. Open license  (4) is fundamentally important - can't reuse code without it. In several studies, had to ask authors to add license (all kindly agreed).

> **Timing**. Timings varied alot, from 2h11 to 28h14
> 
> * Six slower studies (>12h)
>     * I think main reasons for time were: troubleshooting wrong parameters (1), writing code for scenarios (2), and creating tables/figures/in-text results (3) (including appropriate transformations, which columns to use, and so on).
>     * Scope could be larger (5 to 17 items)
> * Two quicker studies (<4h)
>     * Both required very little troubleshooting (e.g. only small amount of coding)
>     * Scope was also simple (1 or 5 items)

> **Reproduction success**. I think main reasons were having wrong parameters (1), not having code for scenarios (2) and not having code for results (3).

## Relationship between reproduction and evaluation

Regarding relationship between reproduction and evaluation, in terms of article reporting, these feel like the key things that impacted reproduction...

### Article

**Really important to be in article:**

* STRESS: Run time
    * Found this very useful when mentioned, and a bit frustrating when not and later realise its a long run time. Want in both article and repository as whether someone finds repository from article or vice versa, want them to have right expectations up front of run requirements for this model
* ISPOR and STRESS: Input parameters
    * This was VERY important to be reported as it allows us to check code parameters are correct. When missing from paper, not possible to check

**Handy to be in article but should ideally be in repository:**

* STRESS: software
* Scenario code ISPOR and STRESS
    * May describe in paper but if code not provided can still be tricky and time consuming to implement

### Repository

* STARS and badges: documentation (sufficient, minimal, step by step, careful)
    * Instructions on how to run
    * Understandable output e.g. data dictionaries
    * Versions
* STARS: dependency management

## Other things I wanted to mention

Different checklists have different focuses... STRESS-DES focuses on what and how modelling work is performed... derived from ISPOR-SDM checklist is more related to validity type stuff...

Whether or not a study was fully reproduced or not doesn't necessarily mean it was "good" or "bad". Did lots of troubleshooting for some before fully reproduced. And sometimes, couldn't reproduce for simple reasons, like likely parameter being wrong impacting lal results.

Badges not awarded for not having combination of things required e.g. archive + docs, or complete + docs. Many got basic one of reproducing results, but I allowed troubleshooting - yet they may not - particularly HOURS of troubleshooting.

Article evaluation sometimes took quite a while... limitation that may not be perfect... those that used guidelines did better... emphasis importance of (a) using guidelines and (b) attaching completed guidelines

Meeting STARS optional wasn't helpful for reproduction (but would be for reuse)
