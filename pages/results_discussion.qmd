---
title: "Results discussion"
bibliography: ../references.bib
---

::: {.callout-note}

## What is on this page?

* Reflections from the reproduction and evaluation, broadly grouped into two categories
* Headline findings
* Relationship between reproduction and evaluation
* Some other things I want to mention

:::

## Reflections

(Addressing Tom's suggestion - "Maybe there are some high level groupings of your findings? e.g. Study reporting; Model structuring and functionality; Analysis pipeline")

### Analysis pipeline

These are important for reproduction. If need to troubleshoot, then model structure and functionality are particularly useful - e.g.:

* Writing code for scenarios is impacted by: programmatic, don't hard code, sufficient comments
* Writing code for results is impacted by: understandable output

Notes:

* Tom: Key for STARS 2.0: seeds

!["Reflections" in the group "Analysis Pipeline"](../images/pipeline.png){.lightbox}

::: {.callout-note collapse="true"}

## Links to evaluation

Hence, these are included in relationship between reproduction and evaluation below

* Packages - STARS: dependency management
* Versions - Badges: documentation, and STRESS: software
* Scenario - ISPOR and STRESS: scenario, and Badges: complete set of materials
* Required parameters - ISPOR and STRESS: input parameters

:::

### Model structure and functionality

These are important for reuse - but also pipeline stuff! As they (a) make reproduction easier and (b) are important when troubleshooting.

Notes:

* Data dictionaries are relevant to inputs and outputs.
* Some of these can become "less of an issue" for reproduction if there is a reproducible analytical pipeline, as don't need to delve into it and troubleshoot (but are still relevant for reuse).

!["Reflections" in the group "Model structure and functionality"](../images/structure_function.png){.lightbox}

::: {.callout-note collapse="true"}

## Links to evaluation

Hence, these are included in relationship between reproduction and evaluation below

* Data dictionaries - Badges: careful documentation
* Instructions on how to run - STARS and Badges: documentation (sufficient, minimal and step by step)
* Run time - STRESS: run time

:::

### Other

These didn't fit into the two categories above:

* **Open license**
    * On ALL repositories required
* **Unsupported versions**

::: {.callout-note collapse="true"}

## Links to evaluation

Hence, these are included in relationship between reproduction and evaluation below

* Badges and STARS: open license

:::

## Headline findings

Whilst above captures the reflections, I've also pulled out a few **headline** findings.

The four most important things:

1. **Using correct parameters in code**
2. **Including code for all scenarios**
3. **Include code to create tables, figures and in-text results**
4. **Open license**

Many of these headlines link to the importance of reproducible analytical pipelines (RAP) for simulation.

Why were these the most fundamental?

> **Using the code**. Open license  (4) is fundamentally important - can't reuse code without it. In several studies, had to ask authors to add license (all kindly agreed).

> **Timing**. Timings varied alot, from 2h11 to 28h14
> 
> * Six slower studies (>12h)
>     * I think main reasons for time were: troubleshooting wrong parameters (1), writing code for scenarios (2), and creating tables/figures/in-text results (3) (including appropriate transformations, which columns to use, and so on).
>     * Scope could be larger (5 to 17 items)
> * Two quicker studies (<4h)
>     * Both required very little troubleshooting (e.g. only small amount of coding)
>     * Scope was also simple (1 or 5 items)

> **Reproduction success**. I think main reasons were having wrong parameters (1), not having code for scenarios (2) and not having code for results (3).

## Relationship between reproduction and evaluation

Regarding relationship between reproduction and evaluation, in terms of article reporting, these feel like the key things that impacted reproduction...

(Also addressing Tom's suggestion: "Evidence supporting the use of reporting guidelines... as in - reporting guidelines state this should info be included and you found it v.difficult or impossible to reproduce without it")

### Article

**Really important to be in article:**

* STRESS: Run time
    * Found this very useful when mentioned, and a bit frustrating when not and later realise its a long run time. Want in both article and repository as whether someone finds repository from article or vice versa, want them to have right expectations up front of run requirements for this model
* ISPOR and STRESS: Input parameters
    * This was VERY important to be reported as it allows us to check code parameters are correct. When missing from paper, not possible to check

**Handy to be in article but should ideally be in repository:**

* STRESS: software
* Scenario code ISPOR and STRESS
    * May describe in paper but if code not provided can still be tricky and time consuming to implement

### Repository

* STARS and badges: documentation (sufficient, minimal, step by step, careful)
    * Instructions on how to run
    * Understandable output e.g. data dictionaries
    * Versions
* STARS: dependency management

## Evidence for framework components

Reflections:

* "Include instructions on how to run the model/script" --> Evidence for STARS essential component minimum documentation
* "For slow models, state the expected run time" --> Evidence for STRESS-DES 5.4
* "For computationally expensive models, state memory usage and provide alternatives for lower spec machines" --> Evidence for STRESS-DES
* "Provide all the required parameters" and "If not provided in the script, then *clearly* present all parameters in the paper" --> Evidence for STRESS-DES 3.3

## Other things I wanted to mention

Different checklists have different focuses... STRESS-DES focuses on what and how modelling work is performed... derived from ISPOR-SDM checklist is more related to validity type stuff...

Whether or not a study was fully reproduced or not doesn't necessarily mean it was "good" or "bad". Did lots of troubleshooting for some before fully reproduced. And sometimes, couldn't reproduce for simple reasons, like likely parameter being wrong impacting lal results.

Badges not awarded for not having combination of things required e.g. archive + docs, or complete + docs. Many got basic one of reproducing results, but I allowed troubleshooting - yet they may not - particularly HOURS of troubleshooting.

Article evaluation sometimes took quite a while... limitation that may not be perfect... those that used guidelines did better... emphasis importance of (a) using guidelines and (b) attaching completed guidelines

Meeting STARS optional wasn't helpful for reproduction (but would be for reuse)
