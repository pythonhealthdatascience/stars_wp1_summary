---
title: "Limitations"
bibliography: ../references.bib
---

## Subjective reproduction success

As noted in the research protocol, the assessment of **reproduction success** involves **subjective judgment**.

We sometimes looked at **absolute and percentage difference** betweeen our reproduction and the original results. However, these could be **significantly influenced by the scale** used. For instance, comparing values like 0.1, 0.2, and 0.3 results in larger percentage differences than comparing values such as 10, 15, and 20, even though the latter comparison might hold more practical significance depending on the context.

The **nature of the metrics being analyzed** also affected outcomes. For example, in the health economic study, costs and Quality-Adjusted Life Years (QALYs) appeared similar in value. However, when using them to calculating the Incremental Cost-Effectiveness Ratio (ICER) and the Incremental Net Monetary Benefit (INMB), even **minor variations in the costs and QALYs resulted in substantial differences in the ICER and INMB**.

## Reuse v.s. reproduction

This study **did not attempt to reuse** the model/s in new contexts. Despite this, for some studies, I did have to examine the underlying code extensively to understand how to implement the scenarios or process the results. This troubleshooting process was **similar to reuse**, as I was attempting to **adapt the model code to run a new scenario**. However, for other studies, I could quite easily run the model and didn't need to delve into the code.

As such, I would not have been able to consistently assess whether the repository provided sufficient information for a comprehensive understanding of the model, necessary for reuse. As such, the focus is moreso on **reproduction** and what was necessary/helpful/unnecessary for that.

It's important to note that features I didn't as important for reproduction were indeed specific to reproduction, not to reuse. As such, certain **elements that weren't essential for reproduction might still be highly valuable for reuse** (and, we know, often would be), such as:

* Validity information from the article
* Web applications

## Human error in evaluation

The evaluations (in particular, of the articles) are likely to be **imperfect**. It can be difficult to locate information in the papers and errors may have occurred. To help mitigate this we:

* Had a **second person** look at uncertain or unmet criteria
* Revisited all the evaluations **side-by-side** once complete to check for any inconsistent decisions between them

However, this potential for error - as well as the time taken to complete these evaluations - does highlight how it can be so helpful to actually **provide a completed reporting guidelines framework**, which highlights key reporting points for each study and indicates where to find specific information. This can become especially helpful as studies become more complex with **longer articles**, or details often spread across **several appendices** or **earlier papers**.