---
title: "Reflections from reproductions"
bibliography: ../references.bib
---

This page describes the facilitators for each reproduction (and, conversely, the barriers to the reproduction, in their absence). With each section, I have created a table which evaluates whether the facilitators were fully met (‚úÖ), partially met (üü°), not met (‚ùå) or not applicable (N/A) for each study.

<!--TODO: Add Johnson et al. 2021, Hernandez et al. 2015 and Wood et al. 2021 reflections once have finished reproduction -->

Links to each study:

* @shoaib_simulation_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-shoaib-2022/evaluation/reflections.html" target="_blank">link to reflections</a>
* @huang_optimizing_2019  - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/reflections.html" target="_blank">link to reflections</a>
* @lim_staff_2020 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-lim-2020/evaluation/reflections.html" target="_blank">link to reflections</a>
* @kim_modelling_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-kim-2021/evaluation/reflections.html" target="_blank">link to reflections</a>
* @anagnostou_facs-charm_2022 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-anagnostou-2022/evaluation/reflections.html" target="_blank">link to reflections</a>
* @johnson_cost_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-johnson-2021/evaluation/reflections.html" target="_blank">link to reflections</a>
* @hernandez_optimal_2015 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-hernandez-2015/evaluation/reflections.html" target="_blank">link to reflections</a>
* @wood_value_2021 - <a href="https://pythonhealthdatascience.github.io/stars-reproduce-wood-2021/evaluation/reflections.html" target="_blank">link to reflections</a>

<!-- TODO: Consider whether to make further attempts at managing to backdate R. Didn't manage before. E.g. `containerit` package. Although - is it necessary, if I succeeded without? Note, I always backdated the python environments. So, relatedly, TODO: Is it worth checking/trying with newest Python and seeing if they would work?</mark -->

<!-- TODO: Add reflections related specifically to the reproduction package and second reproduction test parts of the work (i.e. after the reproduction) -->

<!-- TODO: Once complete, carefully go over again and check everything is correct -->

## Environment

::: {.callout-note icon=false collapse=true}

## List required packages

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | üü° | ‚úÖ | ‚úÖ | <!--TODO--> | üü° | ‚ùå |

@shoaib_simulation_2021: Not met. This became a time-consuming issue as it took a while to identify a dependency that was needed for the code to work (`greenlet`) (based on reading the documentation for `salabim`), and a while longer to realise I had installed another package when the package I needed was in base (`statistics`).

@huang_optimizing_2019: Not met. However, this was fairly easily resolved based on imports to `.R` script, and then on extra imports suggested by RStudio when I tried and failed to run the script.

@lim_staff_2020: Partially met. The only packages needed (`numpy` and `pandas`) are mentioned in the paper (although only listed as imports in the script).

@kim_modelling_2021: Fully met. Provides commands to install packages required at the start of scripts, which I could then easily base renv on automatically (as it detects them).

@anagnostou_facs-charm_2022: Fully met. Provides `requirements.txt`

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Partially met. Some (but not all) of the required packages were listed in the paper. Of particular note, this depended on having a local package `myutils/`, which was another GitHub repository from the author. This was not mentioned anywhere, and so required to notice this was needed.

@wood_value_2021: Not met. However, easily resolved based on imports to `.R` script.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* The `import` statements can be sufficient in indicating all the packages required but this is not always the case if there are "hidden"/unmentioned dependencies that don't get imported
* There are various options for listing the packages (e.g. comprehensive import statements, installation lines in the script, environment files).
* Ideally mention this in repository, not just the paper.
* If there are local dependencies (e.g. other GitHub repositories), make sure to (a) mention and link to these repositories, so it is clear they are also required, and (b) include license/s in those repositories also, so they can be used.
* This was a common issue.

:::

::: {.callout-note icon=false collapse=true}

## Provide versions

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | üü° | üü° | üü° | <!--TODO--> | üü° | üü° |

@shoaib_simulation_2021: Not met. I had to backdate the package versions as the model didn't work with the latest.

@huang_optimizing_2019: Not met. I initially tried to create an environment with R and package versions that were prior to the article publication date. However, I had great difficulties implementing this with R, and never managed to successfully do this. This was related to:

* The difficulty of switching between R versions
* Problems in finding available/a source for specific package versions for specific versions or R

@lim_staff_2020: Partially met. Provides major Python version, but chose minor and the package versions based on article publication date.

@kim_modelling_2021: Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.

@anagnostou_facs-charm_2022: Partially met (depending on how strict you are being). The Python version was stated in the paper, and the SimPy version was stated in the complementary app repository (although neither were mentioned in the model repository itself).

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Partially. Versions of Python, R and some (but not all) packages given in the paper. Some versions weren't very specific (e.g. Python 2.7 v.s. something specific like 2.7.12)

@wood_value_2021: Partially met. States version of R but not package. Due to prior issues with backdating R, used latest versions. There were no issues using the latest versions of R and packages, but if there had been, it would be important to know what versions had previously been used and worked.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Models will sometimes work with the latest versions of packages, but likewise, you will sometimes need to backdate as it no longer works with the latest
* For Python, it was very easy to "backdate" the python and package versions. However, I found this very difficult to in R, and ended up always using the latest versions.
* Versions are sometimes provided elsewhere (e.g. in paper, in other repositories), but would be handy to be in model repository itself.
* Handy to provide specific versions too, particularly when there can be reasonably large changes between minor versions.
* This was a very common issue.

:::

## Structure of code and scripts

::: {.callout-warning icon=false collapse=true}

## Model is provided in a "runnable" format

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: Fully met. Provided as a single `.py` file which ran model with function `main()`.

@huang_optimizing_2019: Not met. The model code was provided within the code for a web application, but the paper was not focused on this application, and instead on specific model scenarios. I had to extract the model code and transform it into a format that was "runnable" as an R script/notebook.

@lim_staff_2020: Fully met. Provided as a single `.py` file which ran the model with a for loop.

@kim_modelling_2021: Fully met. Has seperate `.R` scripts for each scenario which ran the model by calling functions from elsewhere in repository.

@anagnostou_facs-charm_2022: Fully met. Can run model from command line.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met. The model (python code) can be run from `main.py`.

@wood_value_2021: Fully met. Provided as a single `.R` file which ran the model with a for loop.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* If you are presenting the results of a model, then provide the code for that model in a "runnable" format.
* This was an uncommon issue.

:::

<!--TODO: Consider overlap between some of these and how clear it is. I seperated to seperate out the script structure v.s. hard coding v.s. providing parameters and scenarios later. Consider whether evaluation for Shoaib below feels correct or not. -->

::: {.callout-warning icon=false collapse=true}

## Model is designed to be run programmatically (i.e. can run model with different parameters without needing to change the model code)

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: Not met. The model is set up as classes and run using a function. However, it is not designed to allow any variation in inputs. Everything uses default inputs, and it designed in such a way that - if you wish to vary model parameters - you need to directly change these in the script itself.

@huang_optimizing_2019: Fully met. Model was set up as a function, with many of the required parameters already set as "changeable" inputs to that function.

@lim_staff_2020: Fully met. The model is created from a series of functions and run with a for loop that iterates through different parameters. As such, the model is able to be run programmatically (within that for loop, which varied e.g. staff per shift and so on and re-ran the model).

@kim_modelling_2021: Fully met. Each scenario is an R script which states different parameters and then calls functions to run model.

@anagnostou_facs-charm_2022: Fully met. Change inputs in input `.csv` files.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met. Model created from classes, which accept some inputs and can run the model.

@wood_value_2021: Fully met. Changes inputs to run all scenarios from a single `.R` file.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Design model so that you can re-run it with different parameters without needing to make changes to the model code itself.
    * This allows you to run multiple versions of the model with the same script.
    * It also reduces the likelihood of missing errors (e.g. if miss changing an input parameter somewhere, or input the wrong parameters and don't realise).
* This was an uncommon issue.

:::

::: {.callout-warning icon=false collapse=true}

## Don't hard code parameters that you will want to change for scenario analyses

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | üü° | ‚ùå | ‚úÖ | N/A | <!--TODO--> | üü° | ‚úÖ |

@shoaib_simulation_2021: Not met. Although some parameters sat "outside" of the model within the `main()` function (and hence were more "changeable", even if not "changeable" inputs to that function, but changed directly in script). However, many of the other parameters were hard-coded within the model itself. It took time to spot where these were and correctly adjust them to be modifiable inputs.

@huang_optimizing_2019: Partially met. Pretty much all of the parameters that we wanted to change were not hard coded and were instead inputs to the model function `simulate_nav()`. However, I did need to add an `exclusive_use` scenario which conditionally changed `ir_resources`, but that is the only exception. I also add `ed_triage` as a changeable input but didn't end up needing that to reproduce any results (was just part of troubleshooting). I also

@lim_staff_2020: Not met. Some parameters were not hard coded within the model, but lots of them were not.

@kim_modelling_2021: Fully met. All model parameters could be varied from "outside" the model code itself, as they were provided as changeable inputs to the model.

@anagnostou_facs-charm_2022: N/A as no scenarios.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Partially met. Did not hard code, runs, population, generations, and percent pre-screened. However, did hard code other parameters like bi-objective v.s tri-objective model and bounding. Also, it was pretty tricky to change percent pre-screened, as it assumed you provided a `.txt` file for each %.

@wood_value_2021: Fully met. All model parameters for the scenarios/sensitivity analysis could be varied from "outside" the model code itself.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* It can be quite difficult to change parameters that are hard coded into the model. Ideally, all the parameters that a user might want to change should be easily changeable and not hard coded.
* This is a somewhat common issue.
* There is overlap between this and whether the code for scenarios is provided (as typically, the code for scenario is conditionally changing parameter values, although this can be facilitated by not hard coding the parameters, so you call need to change the values from "outside" the model code, rather than making changes to the model functions themselves). Hence, have included as two seperate reflections.
* Important to note that we evaluate this in the context of reproduction - and have not checked for hard-coded parameters outside the specified scenario analyses, but that someone may wish to alter if reusing the model for a different analysis/context/purpose.

:::

::: {.callout-warning icon=false collapse=true}

## Use relative file paths

<!--TODO: Consider whether this is something to keep? Did any have an issue with relative file paths? Or was it more that I saw an issue with relative file paths that were tricky to change? Have mentioned this is a bit under "Saves output to a file"-->

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| <!--TODO--> | <!--TODO--> | <!--TODO--> | <!--TODO--> | <!--TODO--> | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: <!--TODO-->

@huang_optimizing_2019: <!--TODO-->

@lim_staff_2020: <!--TODO-->

@kim_modelling_2021: <!--TODO-->

@anagnostou_facs-charm_2022: <!--TODO-->

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met. Creates folder in current working directory based on date/time to store results.

@wood_value_2021: Fully met. Although I then changed things a bit as reorganised repository and prefer not to work with `setwd()`, these were set up in such a way that it would be really easy to correct file path, just by setting working directory at start of script.

**Reflections:** <!--TODO: Update reflections (when add Shoaib, Huang, Lim, Kim and Wood) and then (since have added Johnson, Hernandez and Wood) -->

:::

::: {.callout-warning icon=false collapse=true}

## Avoid large amounts of code duplication

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: Not met. The model often contained very similar blocks of code before or after warm-up. This has the potential to introduce mistakes - with a suspected (although unconfirmed) mistake being that the lower boundary for the doctor consultation times in configuration 1 differed before and after warm-up.

@huang_optimizing_2019: Fully met.

@lim_staff_2020: Fully met.

@kim_modelling_2021: Not met. There was alot of duplication when running each scenario (e.g. repeated calls to `Eventsandcosts`, and repeatedly defining the same parameters). This meant, if changing a parameter that you want to be consistent between all the scripts (e.g. number of persons), you had to change each of the scripts one by one.

@anagnostou_facs-charm_2022: Fully met.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met.

@wood_value_2021: Fully met.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) --> Large amounts of code duplication are non-ideal as they can:

* Make code less readable
* Make it trickier to change universal parameters
* Increase the likelihood of introducing mistakes

:::

::: {.callout-warning icon=false collapse=true}

## Include sufficient comments in the code

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | ‚úÖ | üü° | üü° | <!--TODO--> | üü° | ‚ùå |

@shoaib_simulation_2021 and @huang_optimizing_2019: Not met. Would have benefitted from more comments, as it took some time to ensure I have correctly understood code, particularly if they used lots of abbreviations.

@lim_staff_2020: Fully met. There were lots of comments in the code (including doc-string-style comments at the start of functions) that aided understanding of how it worked.

@kim_modelling_2021: Partially met. Didn't have any particular issues in working out the code. There are sufficient comments in the scenario scripts and at the start of the model scripts, although within the model scripts, there were sometimes quite dense sections of code that would likely benefit from some additional comments.

@anagnostou_facs-charm_2022: Partially met. Didn't have to delve into the code much, so can't speak from experience as to whether the comments were sufficient. From looking through the model code, several scripts have lots of comments and docstrings for each function, but some do not.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Partially met. There are some comments and doc-strings, but not comprehensively.

@wood_value_2021: Not met. Very few comments, so for the small bit of the code that I did delve into, took a bit of working out what different variables referred to.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* With increasing code complexity, the inclusion of sufficient comments becomes increasingly important, as it can otherwise be quite time consuming to figure out how to fix and change sections of code
* Define abbreviations used within the code
* Good to have consistent comments and docstrings throughout (i.e. on all scripts, on not just some of them)

:::

## Run time and memory usage

::: {.callout-important icon=false collapse=true}

## Quicker models are easier to work with

I have not evaluated like as a criteria, as a long run time is not inherently a bad thing. However, I definitely found that the run time of models had a big impact on how easy it was to reproduce results as longer run times meant it was tricky (or even impossible) to run in the first place, or tricky to re-run.

The studies where I made adjustments were:

* @shoaib_simulation_2021: Add parallel processing and ran fewer replications
* @huang_optimizing_2019: No changes made.
* @lim_staff_2020: Add parallel processing
* @kim_modelling_2021: Reduced number of people in simulation, and switched from serial to the provided parallel option.
* @anagnostou_facs-charm_2022: Model was super quick which made it really easy to run and re-run each time
* @johnson_cost_2021: <!--TODO-->
* @hernandez_optimal_2015: Add parallel processing, did not run one of the scenarios (it was very long, and hadn't managed to reproduce other parts of same figure regardless), and experimented with reducing parameters for evolutionary algorithm (but, in the end, ran with full parameters, though lower were helpful while working through and troubleshooting).
* @wood_value_2021: No changes made, but unlike other reproduction, didn't try to run at smaller amounts - just set it to run as-is over the weekend.

For @kim_modelling_2021, an error appears to have been introduced with the aoorta diameter thresholds by switching between nested and unnested lists, which I‚Äôm anticipating was unresolved due to the long run times of the model meaning they weren‚Äôt all run in sequence at the end.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Reduce model run time if possible as it makes it easier to work with, and facilitates doing full re-runs of all scenarios (which can be important with code changes, etc).
    * Relatedly, it is good practice to re-run all scripts before finishing up, as then you can spot any errors like the one mentioned for @kim_modelling_2021
* Common issue (to varying degrees - i.e. taking 20 minutes, up to taking several hours).

:::

::: {.callout-important icon=false collapse=true}

## For slow models, state the expected run time

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚úÖ | ‚ùå | ‚ùå | ‚ùå | N/A | <!--TODO--> | üü° | üü° |

@shoaib_simulation_2021: Fully met. Run time stated in paper (but not repository).

@huang_optimizing_2019: Not met.

@lim_staff_2020: Not met.

@kim_modelling_2021: Not met. A prior paper describing the model development mentions the run time, but not the current paper or repository, so this is easily missed.

@anagnostou_facs-charm_2022: Not applicable. Very quick! Seconds! So not particularly relevant - although, you could argue, potentially still important if there were some error that made it look like the model were running continuously (e.g. stuck in a loop) - although this is relatively unlikely.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Partially met. Some of the run times are mentioned in the paper, but not all, although this did help indicate that we would anticipate other s scenarios to similarly take hours to run.

@wood_value_2021: Partially met. In the paper, they state that it takes less than five minutes for each scenario, but this feels like half the picture, given the total run time was 48 hours.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* For long models with no statement, it can take a while to realise that it's not an error in the code or anything, but actually just a long run time! And hard to know how long to expect, and whether it is without the capacities of your machine and so on.
* Ideally include statement of run time in repository as well as paper.
* Ideally include run time of all components of analysis (e.g. all scenarios).
* Common issue.

:::

::: {.callout-important icon=false collapse=true}

## For computationally expensive models, state memory usage and provide alternatives for lower spec machines

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| N/A | N/A | N/A | ‚ùå | N/A | <!--TODO--> | <!--TODO--> | N/A |

@shoaib_simulation_2021, @huang_optimizing_2019, and @lim_staff_2020: Not applicable. Didn't find it to be too computationally expensive for my machine.

@kim_modelling_2021: Not met. Unable to run on my machine (`serial` took too long to run (would have to leave laptop on for many many hours which isn't feasible), and `parallel` was too computationally expensive and crashed the machine (with the original number of people)). This is not mentioned in the repository or paper, but only referred to in a prior publication. Would've been handy if it included suggestions like reducing number of people and so on (which is what I had to do to feasibly run it).

@anagnostou_facs-charm_2022: Not applicable. Runs in seconds.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: <!--TODO: This had long run times but I don't know if it was computationally expensive or not. May need to run again for a while and see what memory usage is-->

@wood_value_2021: Not applicable. As stated in their prior paper, the model is constrained by processing time, not computer memory.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Some models are so computationally expensive that it may be simply impossible to run it a feasible length of time without a high powered machine.
* If a model is computationally expensive, it would be good to provide suggested alternatives that allow it to be run on lower spec machines
* Not a common problem - only relevant to computationally expensive models

:::

## Parameters, scenarios and outputs

::: {.callout-tip icon=false collapse=true}

## Provide code for all scenarios

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | ‚ùå | ‚ùå | N/A | <!--TODO--> | ‚ùå | ‚úÖ |

@shoaib_simulation_2021: Not met. There were several instances where it took quite a while to understand how and where to modify the code in order to run scenarios (e.g. no arrivals, transferring admin work, reducing doctor intervention in deliveries).

@huang_optimizing_2019: Not met. Set up a notebook to programmatically run the model scenarios. It took alot of work to modify and write code that could run the scenarios, and I often made mistakes in my interpretation for the implementation of scenarios, which could be avoided if code for those scenarios was provided.

@lim_staff_2020: Not met. Several parameters or scenarios were not incorporated in the code, and had to be added (e.g. with conditional logic to skip or change code run, removing hard-coding, adding parameters to existing).

@kim_modelling_2021: Not met. Took alot of work to change model from for loop to function, to set all parameters as inputs (some were hard coded), and add conditional logic of scenarios when required.

@anagnostou_facs-charm_2022: Not applicable. No scenarios.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Not met. Took a while to figure out how to implement scenarios.

@wood_value_2021: Fully met.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Common issue
* Time consuming and tricky to resolve

:::

::: {.callout-tip icon=false collapse=true}

## All the required outputs are calculated/provided

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: Not met. Had to add some outputs and calculations (e.g. proportion of childbirth cases referred, standard deviation)

@huang_optimizing_2019: Not met. It has a complicated output (standardised density of patient in queue) that I was never certain on whether I correctly calculated. Although it outputs the columns required to calculate it, due its complexity, I feel this was not met, as it feels like a whole new output in its own right (and not just something simple like a mean).

@lim_staff_2020: Not met. The model script provided was only set up to provide results from days 7, 14 and 21. The figures require daily results, so I needed to modify the code to output that. 

@kim_modelling_2021: Not met. Had to write code to find aorta sizes of people with AAA-related deaths.

@anagnostou_facs-charm_2022: Fully met. Although worth noting this only had one scenario/version of model and one output to reproduce.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met.

@wood_value_2021: Fully met.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Calculate and provide all the outputs required
* Appreicate this can be a bit "ambiguous" (e.g. if its just plotting a mean or simple calculation, then didn't consider that here) (however, combined with other criteria, we do want them to provide code to calculate outputs, so we would want them to provide that anyway)

:::

::: {.callout-tip icon=false collapse=true}

## Include correct parameters in the script (even if just for one scenario)

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| üü° | ‚ùå | üü° | ‚úÖ | ‚úÖ | <!--TODO--> | ‚ùå | ‚úÖ |

@shoaib_simulation_2021: Partially met. Script is set with parameters for base configuration 1, with the exception of number of replications.

@huang_optimizing_2019: Not met. The baseline model in the script did not match the baseline model (or any scenario) in the paper, so had to modify parameters.

@lim_staff_2020: Partially met. The included parameters were corrected, but the baseline scenario included varying staff strength to 2, and the provided code only varied 4 and 6. I had to add some code that enabled it to run with staff strength 2 (as there were an error that occured if you tried to set that).

@kim_modelling_2021: Fully met.

@anagnostou_facs-charm_2022: Fully met.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Not met. As agreed with the author, this is likely the primary reason for the discrepancy in these results - they are very close, and we see similar patterns, but not reproduced. Unfortunately, several parameters were wrong, and although we changed those we spotted, we anticipate there could be others we hadn't spotted that might explain the remaining discrepancies.

@wood_value_2021: Fully met.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* At least provide a script that can run the baseline model as in the paper (even if not providing the scenarios)
* This can introduce difficulties - when some parameters are wrong, you rely on the paper to check which parameters are correct or not, but if the paper doesn't mention every single parameter (which is reasonably likely, as this includes those not varied by scenarios), then you aren't able to be sure that the model you are running is correct.
* This can make a really big difference, and be likely cause of managing to reproduce everything v.s. nothing, if it impacts all aspects of the results.

:::

::: {.callout-tip icon=false collapse=true}

## Provide all the required parameters

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | <!--TODO--> | ‚ùå | ‚úÖ |

@shoaib_simulation_2021: Some parameters that could not be calculated were not provided - ie. what consultation boundaries to use when mean length of doctor consultation was 2.5 minutes

@huang_optimizing_2019: Not met. In this case, patient arrivals and resource numbers were listed in the paper, and there were several discprepancies between this and the provided code. However, for many of the model parameters like length of appointment, these were not mentioned in the paper, and so it was not possible to confirm whether or not those were correct. Hence, marked as not met, as the presence of discrepenancies for several other parameters puts these into doubt.

@lim_staff_2020: Not met. For Figure 5, had to guess the value for `staff_per_shift`.

@kim_modelling_2021: Fully met.

@anagnostou_facs-charm_2022: Fully met.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Not met. The results have a large impact by the bounding set, but this was not mentioned in the paper or repository, and required me looking at the numbers in results and GitHub commit history to estimate the appropriate bounds to use.

@wood_value_2021: Fully met.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Provide all required parameters

:::

::: {.callout-tip icon=false collapse=true}

## If not provided in the script, then *clearly* present all parameters in the paper

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | üü° | N/A | N/A | <!--TODO--> | üü° | N/A |

@shoaib_simulation_2021: Not met. Although there was a scenario table, this did not include all the parameters I would need to change. It was more challenging to identify parameters that were only described in the body of the article. There were also some discrepancies in parameters between the main text of the article, and the tables and figures. Some scenarios were quite ambiguous/unclear from their description in the text, and I initially misunderstood the required parameters for the scenarios.

@huang_optimizing_2019: Not met. As described above, paper didn't adequately describe all parameters.

@lim_staff_2020: Partially met. Nearly all parameters are in the paper table, and others are described in the article. However, didn't provide information for the `staff_per_shift` for Figure 5.

@kim_modelling_2021 and @anagnostou_facs-charm_2022: Not applicable. All provided.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Most parameters are relatively easily identified from the text or figure legends (though would be easier if provided in a table or similar). Parameter for bounding was not provided in paper.

@wood_value_2021: Not applicable. All provided.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Provide parameters in a table (including for each scenario), as it can be difficult/ambiguous to interpret them from the text, and hard to spot them too.
* Be sure to mention every parameter that gets changed (e.g. for @lim_staff_2020, as there wasn't a default `staff_per_shift` across all scenarios, but not stated for the scenario, had to guess it).

:::

::: {.callout-tip icon=false collapse=true}

## If will need to process parameters, provide required calculations

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚úÖ | N/A | N/A | N/A | <!--TODO--> | N/A | N/A |

@shoaib_simulation_2021: Not met. It was unclear how to estimate inter-arrival time.

@huang_optimizing_2019: Fully met. The calculations for inter-arrival times were provided in the code, and the inputs to the code were the number of arrivals, as reported in the paper, and so making it easy to compare those parameters and check if numbers were correct or not.

@lim_staff_2020: Not applicable. The parameter not provided is not one that you would calculate.

@kim_modelling_2021 and @anagnostou_facs-charm_2022: Not applicable. All provided.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Not applicable.

@wood_value_2021: Not applicable. All provided.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* If you are going to be mentioning the "pre-processed" values at all, then its important to include the calculation (ideally in the code, as that is the clearest demonstration of exactly what you did)

:::

## Output format

::: {.callout-caution icon=false collapse=true}

## Saves output to a file

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: Fully met. Outputs to `.xlsx` files

@huang_optimizing_2019, @lim_staff_2020, and @kim_modelling_2021: Not met. Outputs to dataframe/s.

@anagnostou_facs-charm_2022: Outputs to `OUT_STATS.csv`. Note: Although not needed for the reproduction itself, when I tried to amend the name and location of the csv file output the model for use in tests, this was very tricky to do as it was hard coded into the scripts and I found difficult to amend due to how the model is run and set up.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met. Outputs to `.txt` files.

@wood_value_2021: Fully met. Outputs to `.csv` files.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Common issue
* Particularly important if model run time is even slightly long (even just minutes long, but even more so as becomes many minutes / hours), so don't always have to re-run it each time to get results
* Set up this in such a way that it is easy to change the name and location of the output file.

:::

::: {.callout-caution icon=false collapse=true}

## Understandable output tables

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: Not met. There were two alternative results spreadsheets with some duplicate metrics but sometimes differing results between them, which made it a bit confusing to work out what to use.

@huang_optimizing_2019, @lim_staff_2020, and @anagnostou_facs-charm_2022: Fully met. Didn't experience issues interpreting the contents of the output table/s.

@kim_modelling_2021: Not met. It took me a little while to work out what surgery columns I needed, and to realise I needed to combine two of them. This required looking at what inputs genreated this, and referring to a input data dictionary.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met. Straightforward with key information provided.

@wood_value_2021: Fully met. I didn't need to work with the output tables, but from looking at them now, they make sense.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Don't provide alternative results for the same metrics
* Make it clear what each colum/category in the results table means, if it might not be immediately clear.

:::

::: {.callout-caution icon=false collapse=true}

## Avoid large file sizes if possible

I have not evaluated like as a criteria, as a large file size is not inherently a bad thing, and might be difficult to avoid. However, when files are very large, this can make things trickier, such as with requiring compression and use of GitHub Large File Storage (LFS) for tracking, which has limits on the free tier.

Regarding file sizes in each study:

* @shoaib_simulation_2021: Not relevant (results files **<35 kB**)
* @huang_optimizing_2019: Provided code didn't save results to file. When I saved to file, these were large, so I compressed to `.csv.gz`, which made them small enough that GitHub was still happy (**26 MB**).
* @lim_staff_2020: Provided code didn't save results to file. When I saved to file, these were small, so not relevant (results files **<60 kB**)
* @kim_modelling_2021: Provided code didn't save results to file. When I saved to file, these were small, so not relevant (results files **<1 kB**)
* @anagnostou_facs-charm_2022: Not relevant (results file **34 kB**)
* @johnson_cost_2021: <!--TODO-->
* @hernandez_optimal_2015: Not relevant (aggregate results files **<10 kB**).
* @wood_value_2021: Aggregate results files are small (**327 kB**), but raw results files are very large (**2.38 GB**), and even when compressed to `.csv.gz` (**128 MB**) require using of GitHub LFS

**Reflections:** <!--TODO: Update reflections once add Johnson -->

* For most studies, this was not relevant, with outputs relatively small
* I only really found this to be an issue when files exceeded GitHub threshold. [GitHub](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github) give warning over 50 MB, blocks files over 100 MB, requiring you to use GitHub LFS. Recommends repositories ideally <1 GB and for sure < 5GB. [GitHub LFS](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-storage-and-bandwidth-usage) has limits on storage and bandwith use (1GB of each).
* **Reducing file size isn't the only solution**. In cases where you have large files, a good option can be storing it elsewhere and then pulling from there into your workflow - for example, storing in **zenodo** and fetching using [pooch](https://github.com/fatiando/pooch).

:::

::: {.callout-caution icon=false collapse=true}

## Do not output excessive numbers of files

I have not evaluated like as a criteria, as a large number of files is not inherently a bad thing, and might be difficult to avoid.

Regarding number of files:

* @shoaib_simulation_2021: Not relevant.
* @huang_optimizing_2019: Not relevant.
* @lim_staff_2020: Not relevant.
* @kim_modelling_2021: Not relevant.
* @anagnostou_facs-charm_2022: Not relevant.
* @johnson_cost_2021: <!--TODO-->
* @hernandez_optimal_2015: The default behaviour of the script was to output **lots of files** from each round (so you could easily have 90, 100, 200+ files), which were then **not** used in analysis (as it just depended on an aggregate results file). Although these individual files might be useful during quality control, as a default behaviour of the script, it could easily make the repository quite busy/littered with files.
* @wood_value_2021: Not relevant.

**Reflections:** <!--TODO: Update reflections once add Johnson -->

* Outputting hundreds of files - particularly if they are not then used normally in analysis - can easily make repository cluttered/busy

:::

## Seeds

::: {.callout-note icon=false collapse=true}

## Use seeds to control stochasticity

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | <!--TODO--> | ‚úÖ | ‚úÖ |

@shoaib_simulation_2021: Not met. The lack of seeds wasn't actually a barrier to the reproduction though due to the replication number. I later add seeds so my results could be reproduced, and found that the ease of setting seeds with salabim was a greater facilitator to the work. I only had to change one or two lines of code to then get consistent results between runs (unlike other simulation software like SimPy where you have to consider the use of seeds by different sampling functions). Moreover, by default, salabim would have set a seed (although overridden by original authors to enable them to run replications).

@huang_optimizing_2019: Not met. It would have been beneficial to include seeds, as there was a fair amount of variability, so with seeds I could then I could be sure that my results do not differ from the original simply due to randomness.

@lim_staff_2020: Not met. The results obtained looked very similar to the original article, with minimal differences that I felt to be within the expected variation from the model stochasticity. However, if seeds had been present, we would have been able to say with certainty. I did not feel I needed to add seeds during the reproduction to get the same results.

@kim_modelling_2021: Fully met. Included a seed, although I don't get identical results as I had to reduce number of people in simulation.

@anagnostou_facs-charm_2022: Fully met. The authors included a random seed so the results I got were identical to the original (so no need for any subjectivity in deciding whether its similar enough, as I could perfectly reproduce).

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Fully met. This ensured consistent results between runs of the script, which was really helpful.

@wood_value_2021: Fully met. Sets seed based on replication number.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Depending on your model and the outputs/type of output you are looking at, the lack of seeds can have varying impacts on the appearance of your results, and can make the subjective judgement of whether results are consistent harder (if discrepancies could be attributed to not having consistent seeds or not).
* It can be really quite simple to include seeds.

:::

## Code to produce article results

::: {.callout-warning icon=false collapse=true}

## Provide code to process results into tables

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | N/A | üü° | ‚ùå | N/A | <!--TODO--> | ‚ùå | ‚úÖ |

<!--TODO: Lim 2020... I think from memory that this output tables  similar to the paper, but I then changed them as that format was difficult to compare/work with -->

@shoaib_simulation_2021: Not met.

@huang_optimizing_2019: Not applicable. No tables in scope.

@lim_staff_2020: Partially met. It outputs the results in a similar structure to the paper (like a section of a table). However, it doesn't have the full code to produce a table outright, for any of the tables, so additional processing still required.

@kim_modelling_2021: Not met. Had to write code to generate tables, which included correctly implementing calculation of excess e.g. deaths, scaling to population size, and identify which columns provide the operation outcomes.

@anagnostou_facs-charm_2022: Not applicable. No tables in scope.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Not met.

@wood_value_2021: Fully met.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* It can take a bit of time to do this processing, so very handy for it to be provided.
* Common issue.

:::

::: {.callout-warning icon=false collapse=true}

## Provide code to process results into figures

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | <!--TODO--> | üü° | ‚úÖ |

@shoaib_simulation_2021: Not met.

@huang_optimizing_2019: Not met. Had to write code from scratch. For one of the figures, it would have been handy if informed that plot was produced by a simmer function (as didn‚Äôt initially realise this). It also took a bit of time for me to work out how to transform the figure axes as this was not mentioned in the paper (and no code was provided for these). It was also unclear and a bit tricky to work out how to standardise the density in the figures (since it is only described in the text and no formula/calculations are provided there or in the code). <!-- TODO: Consider whether this should be split into another box (i.e. about providing the processing?) -->

@lim_staff_2020, @kim_modelling_2021 and @anagnostou_facs-charm_2022: Not met. However, the simplicity and repetition of the figures was handy.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Partially met. Provides a few example `ggplot`s, but these are not all the plots, nor exactly matching article, nor including any of the pre-processing required before the plots, and so could only serve as a starting point (though that was still really handy).

@wood_value_2021: Fully met. Figures match article, with one minor exception that I had to add smoothing to the lines on one of the figures.

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* It can take a bit of time to do this processing, particularly if the figure involves any transformations (and less so if the figure is simple), so very handy for it to be provided.
* Common issue.

:::

::: {.callout-warning icon=false collapse=true}

## Provide code to calculate in-text results

By "in-text results", I am referred to results that are mentioned in the text but not included in/cannot be deduced from any of the tables or figures.

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | N/A | ‚ùå | N/A | N/A | N/A | N/A |

@shoaib_simulation_2021, @huang_optimizing_2019, @kim_modelling_2021: Not met.

@lim_staff_2020, @anagnostou_facs-charm_2022, @johnson_cost_2021, @hernandez_optimal_2015, @wood_value_2021: Not applicable (no in-text results).

**Reflections:**

* Provide code to calculate in-text results
* Common issue

:::

## Documentation

::: {.callout-important icon=false collapse=true}

## Include instructions on how to run the model/script

| @shoaib_simulation_2021 | @huang_optimizing_2019 | @lim_staff_2020 | @kim_modelling_2021 | @anagnostou_facs-charm_2022 | @johnson_cost_2021 | @hernandez_optimal_2015 | @wood_value_2021 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| ‚ùå | ‚ùå | ‚ùå | üü° | ‚úÖ | <!--TODO--> | ‚ùå | ‚ùå |

@shoaib_simulation_2021: Not met. No instructions, although is just a single script that you run.

@huang_optimizing_2019: Not met. Not provided in runnable form but, regardless, no instructions for running it as it is provided (as a web application - i.e. no info on how to get that running).

@lim_staff_2020: Not met. No instructions, although is just a single script that you run.

@kim_modelling_2021: Partially met. README tells you which folder has the scripts you need, although nothing further. Although all you need to do is run them.

@anagnostou_facs-charm_2022: Fully met. Clear README with instructions on how to run the model was really helpful.

@johnson_cost_2021: <!--TODO-->

@hernandez_optimal_2015: Not met.

@wood_value_2021: Not met. No instructions, although it was fairly self explanatory (single script `master.R` to run, then processing scripts named after items in article e.g. `fig7.R`).

**Reflections:** <!--TODO: Update reflections (since have added Johnson, Hernandez and Wood) -->

* Even if as simple as running a script, include instructions on how to do so
* Common issue

:::

## Other

<!--TODO: Reflect whether any of these should be boxes above-->

**Grid lines**. Include tick marks/grid lines on figures, so it is easier to read across and judge whether a result is above or below a certain Y value.

**Data dictionaries**. @anagnostou_facs-charm_2022: Included data dictionary for input parameters. Although I didn‚Äôt need this, this would have been great if I needed to change the input parameters at all.

**Unsupported versions**. @hernandez_optimal_2015: Due to the age of the work:

* Some packages were no longer available on Conda and had to be installed from PyPI
* The version of python was no longer supported which meant:
    * Not supported by Jupyter Lab and Jupyter Notebook (so no `.ipynb` files, or Jupyter Lab on Docker)
    * Not supported by VSCode (so had to use "tricks" to run it, involving using a pre-release version of Python on VSCode)
    * Had to create Docker image from scratch (i.e. couldn't start from e.g. `miniconda3`)

However, this is a slightly unavoidable problem unless you continue to maintain your code (which is ideal but not always feasible in a research environment). Realistically, if reusing this code for a new purpose, you would upgrade it to supported versions.

**Original results files**. @hernandez_optimal_2015: Included some original results files, which was invaluable in identifying some of the parameters in the code that needed to be fixed.

**Classes**. @hernandez_optimal_2015: Structured code into classes, which was nice to work with/amend.

**Reflections:**

* Include grid lines
* Include data dictionaries
* If feasible, maintain code to avoid dependency on unsupported old versions of packages and languages
* Ideally, include copies of your results, so can (a) compare against, (b) use to run analysis on, and (c) use in troubleshooting

## References