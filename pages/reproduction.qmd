---
title: "Reproduction"
echo: False
bibliography: ../references.bib
---

<!-- TODO: Once complete, carefully go over again and check everything is correct -->

## Studies

**@shoaib_simulation_2021:** Uses **python (salabim)** to model primary health centres (PHCs) in India. The model has four patient types: outpatients, inpatients, childbirth cases and antenatal care patients. Four model configurations are developed based on observed PHC practices or government-mandated operational guidelines. The paper explores different operational patterns for scenarios where very high utilisation was observed, to explore what might help reduce utilisation of these resources. Note: The article was as @shoaib_simulation_2022, but we used the green open access pre-print @shoaib_simulation_2021. <a href="https://pythonhealthdatascience.github.io/stars-reproduce-shoaib-2022/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

**@huang_optimizing_2019:** Uses **R (simmer)** to model an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway. The paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR. <a href="https://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

**@lim_staff_2020:** Uses **python (NumPy and pandas)** to model the transmission of COVID-19 in a laboratory. It examines the proportion of staff infected in scenarios varying the: number of shifts per day; number of staff per shift; overall staff pool; shift patterns; secondary attack rate of the virus; introduction of protective measures (social distancing and personal protective equipment). <a href="https://pythonhealthdatascience.github.io/stars-reproduce-lim-2020/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

**@kim_modelling_2021:** Adapts a previously developed **R (Rcpp, expm, msm, foreach, iterators, doParallel)** model for abdominal aortic aneurysm (AAA) screening of men in England. The model is adapted/used to explore different approaches to resuming screening and surgical repair for AAA, as these services were paused or substantially reduced during COVID-19 due to concerns about virus transmission. <a href="https://pythonhealthdatascience.github.io/stars-reproduce-kim-2021/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

**@anagnostou_facs-charm_2022:** This paper includes two models - we have focussed just on the dynamiC Hospital wARd Management (CHARM) model. CHARM uses **python (SimPy)** to model intensive care units (ICU) in the COVID-19 pandemic (as well as subsequent stays in a recovery bed). It includes three types of admission to the ICU (emergency, elective or COVID-19). COVID-19 patients are kept seperate, and if they run out of capacity due to a surge in COVID-19 admissions, additional capacity can be pooled from the elective and emergency capacity. <a href="https://pythonhealthdatascience.github.io/stars-reproduce-anagnostou-2022/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

**@johnson_cost_2021:** This study uses a previously validated discrete-event simulation model, EPIC: Evaluation Platform in chronic obstructive pulmonary disease (COPD). The model is written in **C++** with an **R** interface, using R scripts for execution. The model is adapted to evaluate the cost-effectiveness of 16 COPD case detection strategies in primary care, comparing costs, quality-adjusted life years (QALYs), incremental cost-effectiveness ratios (ICER), and incremental net monetary benefits (INMB) across scenarios. Sensitivity analyses are also conducted. <a href="https://pythonhealthdatascience.github.io/stars-reproduce-johnson-2021/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

**@hernandez_optimal_2015:** This study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense counter-measures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput. They use **python** for most of the analysis (with **SimPy** for the simulation component), but **R** to produce the plots and tables for the paper. <a href="https://pythonhealthdatascience.github.io/stars-reproduce-hernandez-2015/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

**@wood_value_2021:** This study uses discrete-event simulation (**R (base R, dplyr, tidyr)**) to explore the deaths and life years lost under different triage strategies for an intensive care unit, relative to a baseline strategy. The unit is modelled with 20 beds (varied from 10 to 200 in sensitivity analyses). Three different triage strategies are explored, under three different projected demand trajectories. <a href="https://pythonhealthdatascience.github.io/stars-reproduce-wood-2021/evaluation/reproduction_success.html" target="_blank">Link to reproduction.</a>

## Scope and reproduction

| Study | Scope | Success | Time |
| --- | --- | --- | --- |
| Shoaib and Ramamohan 2022 | 17 items:<br>• 1 table<br>• 9 figures<br>• 7 in-text results | 16 out of 17 (94%) | 28h 14m |
| Huang et al. 2019 | 8 items:<br>• 5 figures<br>• 3 in-text results | 3 out of 8 (37.5%) | 24h 10m |
| Lim et al. 2020 | 9 items:<br>• 5 tables<br>• 4 figures | 9 out of 9 (100%) | 12h 27m |
| Kim et al. 2021 | 10 items:<br>• 3 tables<br>• 6 figures<br>• 1 in-text result | 10 out of 10 (100%) | 14h 42m |
| Anagnostou et al. 2022 | 1 item:<br>• 1 figure | 1 out of 1 (100%) | 2h 11m |
| Johnson et al. 2021 | 5 items:<br>• 1 table<br>• 4 figures | 4 out of 5 (80%) | 19h 49m |
| Hernandez et al. 2015 | 8 items:<br>• 6 figures<br>• 2 tables | 1 out of 8 (12.5%) | 17h 56m |
| Wood et al. 2021 | 5 items:<br>• 4 figures<br>• 1 table | 5 out of 5 (100%) | 3h 50m |

::: {.callout icon=false collapse=false}

## Reproduction reflections

For the studies where I didn't manage to fully reproduce results despite troubleshooting, my reflections on what I think to be the primary reason for not managing to reproduce results in each case were:

**@shoaib_simulation_2021:** No specific suggestions - had to write code to run scenarios and process results, so it could be that I hadn't done this all exactly the same as in the original study.

**@huang_optimizing_2019:** No specific suggestions - had to write code to run scenarios and process results, so it could be that I hadn't done this all exactly the same as in the original study.

**@johnson_cost_2021:** No specific suggestions - although note that their results on GitHub appeared to likewise have the same discrepancy compared with the article, so it appears there might have been a minor change to the code/parameters made from that used to produce the article figure that might explain this.

**@hernandez_optimal_2015:** This appears likely to be due to a parameter somewhere being not quite right. Ivan Hernandez suggested that it could be a random seed or that the optimisation doesn't have enough runs - although was using the same seed and played around with run number. The other suggestion was that the version on GitHub might not have exactly the right version of inputs - and interestingly, he had an earlier version of the paper that had more similar results (our discrepancy was in range on Y axis, and he had an earlier version with range of 0 to 15000). This reaffirms that a minor parameter difference is the likely reason for the discrepancy, with that having impacted across nearly all the results.

:::

## Time to complete reproductions

<!--Note: In this figure, Hernandez and Wood overlap. Although we would typically have stepped lines like this for time-to-event data, I did explore switching to diagonals to avoid the overlap, but then realised we just get the same issue except with Johnson and Hernandez almost exactly overlapping. -->

```{python}
# Import required libraries and functions
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import numpy as np
import pandas as pd
from plotly import graph_objects as go
from functions import process_times, success_static, success_interactive

# Create a dataframe of times from each of the reproductions

# Shoaib and Ramamohan 2022 (16/17)
time_sho = [
    [856, 'Table 6'],
    [971, 'Figure 2A'],
    [889, 'Figure 2B'],
    [910, 'Figure 2C'],
    [981, 'Figure 2D'],
    [1002, 'Figure 3A'],
    [1008, 'Figure 3B'],
    [1009, 'Figure 3C'],
    [1012, 'Figure 3D'],
    [1165, 'Figure 4'],
    [1196, 'In-text result 1'],
    [1274, 'In-text result 2'],
    [1383, 'In-text result 3'],
    [1396, 'In-text result 4'],
    [1557, 'In-text result 5'],
    [1490, 'In-text result 6'],
    [1694, 'In-text result 7']]
df_sho = process_times(time_sho, adjust='In-text result 7')

# Huang et al. 2019 (3/8)
time_hua = [
    [516, 'In-text result 1'],
    [1193, 'In-text result 2'],
    [1450, 'Figure 2'], # Finished work, not completed, adjust to set to NaN
    [np.nan, 'Figure 3'],
    [np.nan, 'Figure 4'],
    [1228, 'Figure 5'],
    [np.nan, 'Supplementary figure'],
    [np.nan, 'In-text result 3']]
df_hua = process_times(time_hua, adjust='Figure 2')

# Lim et al. 2020 (9/9)
time_lim = [
    [607, 'Figure 2'],
    [647, 'Figure 3'],
    [655, 'Figure 4'],
    [747, 'Figure 5'],
    [486, 'Supplemental table 2'],
    [513, 'Supplemental table 3'],
    [513, 'Supplemental table 4'],
    [714, 'Supplemental table 5'],
    [690, 'Supplemental table 6']]
df_lim = process_times(time_lim)

# Kim et al. 2021 (10/10)
time_kim = [
    [445, 'Figure 1'],
    [511, 'Table 2'],
    [581, 'Figure 2'],
    [619, 'Figure 3'],
    [774, 'In-text result 1'],
    [672, 'Figure 4'],
    [785, 'Figure 5'],
    [793, 'Table 3'],
    [875, 'Supplementary figure 3'],
    [839, 'Supplementary table 2']]
df_kim = process_times(time_kim)

# Anagnostou et al. 2022 (1/1)
time_ana = [
    [130, 'Figure 3']]
df_ana = process_times(time_ana)

# Johnson et al. 2021
time_joh = [
    [1124, 'Table 3'],
    [1124, 'Figure 3'],
    [1189, 'Figure 4'],
    [np.nan, 'Appendix 6'],
    [1157, 'Appendix 7']]
df_joh = process_times(time_joh)

# Hernandez et al. 2015 (1/8)
time_her = [
    [1076, 'Figure 5'], # Finished work, not completed, adjust to set to NaN
    [696, 'Figure 6'],
    [np.nan, 'Figure 7'],
    [np.nan, 'Figure 8'],
    [np.nan, 'Figure 9'],
    [np.nan, 'Figure 10'],
    [np.nan, 'Table 3'],
    [np.nan, 'Table 4']]
df_her = process_times(time_her, adjust='Figure 5')

# Wood et al. 2021
time_woo = [
    [170, 'Figure 4'],
    [170, 'Table 4'],
    [170, 'Figure 5'],
    [170, 'Figure 6'],
    [230, 'Figure 7']]
df_woo = process_times(time_woo)

# Create list with the dataframes, labels and colours, to use for each plot
to_plot = [
    (df_sho, 'Shoaib and<br>Ramamohan<br>2022', '#ee6677'),
    (df_hua, 'Huang et<br>al. 2019', '#228833'),
    (df_lim, 'Lim et<br>al. 2020', '#4477aa'),
    (df_kim, 'Kim et<br>al. 2021', '#ccbb44'),
    (df_ana, 'Anagnostou<br>et al. 2022', '#66ccee'),
    (df_joh, 'Johnson et<br>al. 2021', '#aa3377'),
    (df_her, 'Hernandez<br>et al. 2015', '#882255'),
    (df_woo, 'Wood et al.<br>2021', '#117733')
]
```

Non-interactive figure:

```{python}
# Create figure object
fig, ax = plt.subplots()

# Loop through and add lines for each reproduction
for x in to_plot:
    success_static(x[0], ax, x[1], x[2])

# Amend appearance
plt.xlim(0, 30)
plt.ylim(0, 100)
plt.xlabel('Time elapsed (hours)')
plt.ylabel('Percentage of items reproduced')
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# Save figure (to use in article)
plt.tight_layout()
plt.savefig('../images/article_times.png', dpi=300)

# Show figure
plt.show()
```

Interactive figure:

```{python}
# Create figure object
fig = go.Figure()

# Loop through and add lines for each reproduction
for x in to_plot:
    success_interactive(x[0], fig, x[1], x[2])

# Amend appearance
fig.update_layout(
    xaxis=dict(
        title='Time elapsed (hours)',
        range=[0, 30]
    ),
    yaxis=dict(
        title='Percentage of items reproduced',
        range=[0, 100],
        ticksuffix='%'
    ),
    showlegend=True)

# Show plot, hiding toolbar and not allowing zoom
fig.layout.xaxis.fixedrange = True
fig.layout.yaxis.fixedrange = True
fig.show(config={'displayModeBar': False})
```

::: {.callout icon=false collapse=false}

## Timing reflections

There was a large amount of variation in the time each reproduction took. For each study, I have also reflected on what I think were the primary reasons behind things being quicker or slower:

**@shoaib_simulation_2021** - 28h 14m:

* This had lots of items to reproduce (17)
* Took me longer than usual to set up environment as needed specific package versions and had some confusion around package `statistics` which is base but I had mixed up with one that can be imported from conda/pypi
* Most of the time was dedicated to identifying parameters for each scenario, writing code to run the scenarios (and run these programmatically), and writing code to process the results into figures and tables.

**@huang_optimizing_2019** - 24h 10m:

* Time-consuming aspects were modifying the code from the app so I could run it, writing code for each scenario, and writing code for figures (it took me quite a whiole to work out transformations for axes and how to standardise density)

**@lim_staff_2020** - 12h 27m:

* Lots of time was spent setting up model so could run programmatically, writing code for scenarios, and writing code to produce figures.

**@kim_modelling_2021** - 14h 42m:

* Time-consuming aspect was largely writing code to produce tables and figures (including figuring out how to process).

**@anagnostou_facs-charm_2022** - 2h 11m:

* This only had one item to reproduce.
* It required very little troubleshooting - just had to write code to produce figure, although it was relatively simple.

**@johnson_cost_2021** - 19h 49m:

* Most time was spent on writing code for sensitivity analysis, and to produce tables and figures (which included working out which results tables / columns / scenarios to use, how to transform columns, and how to calculate features like efficiency frontier)

**@hernandez_optimal_2015** - 17h 56m:

* Time-consuming aspects were writing code for scenarios and for figures and tables, and troubleshooting parameter discrepancies.

**@wood_value_2021** - 3h 50m:

* Ran quick as code for model didn't require troubleshooting (included all correct parameters and scenarios), and as they provided code to produce the figures and tables.

Regarding run time of the models themselves, for longer models, I sometimes experimented with parameters to run it quicker, in order to more easily troubleshoot, else I would run for a long time and then discover XYZ is wrong. Although the model run time itself is not included in the times, it is important to note, as some models took several days, and so in practice, this would impact on someone if they were trying to run a model within a short amount of time.

Regarding the reproduction run times, I think the main reflection from above is that including code with correct parameters and scenarios, and code to make the figures and tables, has a really big impact.

:::

## Model run times

For reference, the run times for the models are detailed below. It's worth being aware that these are not compared on a "level playing field" as they were run on different machines, and with/without parallel processing or parallel terminals.

| Study | Machine specs | Model run time | Additional details |
| --- | --- | --- | --- |
| @shoaib_simulation_2021 | Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux | **22 minutes 26 seconds** | This is based on the combined runtime of the notebooks which use 10 replications (rather than 100) and parallel processing, but with each notebook run seperately. |
| @huang_optimizing_2019 | Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux | **29 minutes 10 seconds** | Combined time from notebooks run seperately. |
| @lim_staff_2020 | Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux | **49 minutes and 17 seconds** |
| @kim_modelling_2021 | Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux | **6 hours 53 minutes** |We reduced the number of patients in the simulation from 10 million to 1 million, to improve run times. Note, you can expect the runtime to be **notably longer** on machines with lower specs than this. For example, I ran surveillance scenario 0 on an Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux. The runtime increased from **4 minutes 28 seconds** up to **21 minutes 59 seconds**. |
| @anagnostou_facs-charm_2022 | Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux | **Only a few seconds**. |
| @johnson_cost_2021 | Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux | **At least 1 day 11 hours** | It was run from the command line via multiple terminals simultaneously - hence, the exact times are impacted by the number being ran at once. The base case scenarios were ran at the same time, and took an overall total time of **20 hours 40 minutes**.  The sensitivity analysis scenario were all ran at the same time, and took an overall total of **1 day 10 hours 57 minutes**. Four files (scenarios 2 and 3) had to be re-run seperately after fixing a mistake, and when just those were run, the total was **21 hours 26 minutes** (quicker than when they were run as part of the full set of scenarios, when they took up to 1.3 days). Based on this, if you were to run all scenarios at once in parallel in seperate terminals, you could expect a run time of **at least 1 day 11 hours**, but would be higher than that (since the more run at once, the lower it takes, as you can see from the scenario 2 and 3 times above). |
| @hernandez_optimal_2015| Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux | **9 hours 16 minutes** | This involved running the models within each experiment in parallel, but each of the experiment files seperately. If these are run at the same time (which I could do without issue), then you will be able to run them all within **4 hours 28 minutes** (the longest experiment) (or a little longer, due to slowing speeds from running at once). Also, this has excluded one of the variants for Experiment 3, which I did not run as it had a very long run time (quoted to be 27 hours in the article) and as, regardless, I had not managed to reproduce the other sub-plots in the figure for that experiment. |
| @wood_value_2021 | Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux | **48 hours 25 minutes** | It ran in a single loop, so required the computer to remain on for that time. This time includes all scenarios - but, for just the base scenario, the run time was **2 hours 3 minutes**. |
: {tbl-colwidths="[20, 20, 20, 40]"}

## References